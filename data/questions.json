[
  {
    "id": "q001",
    "concept_ids": ["c-network-003"],
    "scenario": "Sarah is building two applications: a banking portal where every transaction must be reliably delivered, and a live sports score ticker where speed matters more than perfection. She's choosing between TCP and UDP for each.",
    "question_text": "Explain the key differences between TCP and UDP that would help Sarah make her decision. Which protocol should she choose for each application, and why?",
    "model_answer": "TCP is connection-oriented, provides reliable ordered delivery with error checking and retransmission (3-way handshake: SYN, SYN-ACK, ACK). UDP is connectionless, faster but no delivery/ordering guarantees.\n\nBanking portal → TCP: financial transactions require guaranteed delivery, data integrity, and ordered processing.\nSports ticker → UDP: real-time updates prioritize speed; lost updates are replaced by the next one. Lower overhead enables faster updates.",
    "difficulty": "medium",
    "topic_area": "Networking Fundamentals"
  },
  {
    "id": "q002",
    "concept_ids": ["c-network-004"],
    "scenario": "Priya's company is planning a migration where their main website www.example.com will move from an old server (IP: 10.0.0.5) to a new server (IP: 10.0.0.99) over the weekend. She's worried that even after updating the DNS record, some users will still be directed to the old server for hours.",
    "question_text": "Explain what the TTL setting on a DNS record is and how it contributes to this problem. What would you advise Priya to do before the migration weekend to minimize downtime?",
    "model_answer": "TTL (Time To Live) specifies how long DNS resolvers cache a record before re-querying. High TTL (e.g., 86400s = 24h) means users keep using cached old IP for hours after DNS change.\n\nAdvice: 1) Lower TTL to ~300 seconds several days before migration, 2) Wait for old TTL to expire so all caches refresh, 3) Perform migration and update DNS, 4) After confirming success, gradually increase TTL back to normal.",
    "difficulty": "medium",
    "topic_area": "Networking Fundamentals"
  },
  {
    "id": "q003",
    "concept_ids": ["c-cloud-003"],
    "scenario": "An e-commerce company experiences a 10x traffic spike every year during Black Friday. Their current setup runs on a single powerful server. The CTO is debating two strategies: upgrading to an even more powerful server, or distributing the load across multiple smaller servers.",
    "question_text": "Explain the difference between these two approaches — vertical and horizontal scaling. Discuss the trade-offs of each, and recommend which strategy is better suited for handling unpredictable traffic spikes like Black Friday.",
    "model_answer": "Vertical scaling (Scale Up): increase single server's CPU/RAM/storage. Simpler architecture, no app changes needed, but has hardware limits, single point of failure, expensive, requires downtime.\n\nHorizontal scaling (Scale Out): add more servers to distribute load. No theoretical limit, better fault tolerance, cost-effective with commodity hardware, but requires distributed architecture.\n\nFor Black Friday: horizontal scaling is better — can dynamically add servers for unpredictable demand, provides redundancy, and cloud platforms enable auto-scaling based on demand.",
    "difficulty": "medium",
    "topic_area": "Fundamentals of Cloud Computing"
  },
  {
    "id": "q004",
    "concept_ids": ["c-cloud-002"],
    "scenario": "Your manager claims that because the company recently moved its application to a set of virtual machines hosted in a co-location data center, they are now 'in the cloud.' You're not so sure.",
    "question_text": "According to NIST, what are the five essential characteristics an IT environment must exhibit to be considered cloud computing? Based on these characteristics, explain whether your manager's claim is justified or not.",
    "model_answer": "NIST five essential characteristics:\n1. On-demand self-service: provision resources automatically without human interaction\n2. Broad network access: available over network via standard mechanisms\n3. Resource pooling: provider's resources pooled for multiple customers\n4. Rapid elasticity: quickly scale up/down, appearing unlimited\n5. Measured service: usage monitored, controlled, reported (pay-per-use)\n\nManager's claim is likely NOT justified. Co-location VMs typically lack: self-service provisioning, multi-tenant resource pooling, rapid elasticity, and usage-based billing. This is traditional virtualization, not cloud computing.",
    "difficulty": "medium",
    "topic_area": "Fundamentals of Cloud Computing"
  },
  {
    "id": "q005",
    "concept_ids": ["c-vm-003"],
    "scenario": "A data analytics team runs nightly batch jobs that process terabytes of raw log files. The jobs are compute-intensive, generate massive amounts of temporary intermediate data, and the results are written to S3 when complete. If a job fails, it simply restarts from scratch — no intermediate data needs to be preserved.",
    "question_text": "Explain the difference between an Instance Store-backed EC2 instance and an EBS-backed EC2 instance. Given the analytics team's workload described above, which backing store would you recommend and why? What is the key risk the team should be aware of with your recommendation?",
    "model_answer": "Instance Store: physically attached temporary storage, data lost when instance stops/terminates/fails, high I/O performance, no network latency.\nEBS: persistent network-attached storage, data persists independently, can be backed up via snapshots, slightly higher latency.\n\nRecommend Instance Store because: temporary data doesn't need persistence, high I/O benefits compute-intensive processing, lower cost (included with certain instance types), final results saved to S3.\n\nKey risk: if EC2 instance fails or is stopped, ALL Instance Store data is permanently lost. Team must handle instance failures and restart from S3 source data.",
    "difficulty": "hard",
    "topic_area": "Virtual Machines"
  },
  {
    "id": "q006",
    "concept_ids": ["c-devops-001", "c-devops-002"],
    "scenario": "A traditional software company has separate development and operations teams. Developers write code and 'throw it over the wall' to operations, who then struggle to deploy and maintain it.",
    "question_text": "Explain what DevOps is and describe the Three Ways of DevOps. How would adopting these principles help this company improve their software delivery?",
    "model_answer": "DevOps integrates Development and Operations to shorten delivery cycles and deliver high-quality software continuously. It's a combination of culture, practices, and tools.\n\nThree Ways:\n1. Flow/Systems Thinking: focus on entire system performance, not individual silos. Optimize the whole pipeline from dev to production.\n2. Feedback Loops: create fast feedback to detect and fix problems early. Monitoring, alerting, and quick iteration.\n3. Continuous Learning: foster culture of experimentation and learning from failures.\n\nFor this company: breaks down silos (shared ownership), automates handoffs (CI/CD), enables faster feedback (monitoring), and creates culture of collaboration instead of finger-pointing.",
    "difficulty": "medium",
    "topic_area": "Introduction to DevOps"
  },
  {
    "id": "q007",
    "concept_ids": ["c-12factor-001", "c-12factor-002"],
    "scenario": "A startup is building a new SaaS application and wants to ensure it's cloud-native and scalable from day one. The technical lead mentions following the Twelve-Factor App methodology.",
    "question_text": "What is the Twelve-Factor App methodology? Describe at least four of the twelve factors and explain how they contribute to building scalable, maintainable cloud-native applications.",
    "model_answer": "Twelve-Factor App (Heroku, 2011) provides best practices for cloud-native SaaS apps: declarative automation, clean OS contracts, cloud platform suitability, dev/prod parity.\n\nFour key factors:\n1. Codebase: one codebase in version control, many deploys. Ensures consistency.\n2. Dependencies: explicitly declare and isolate. Never rely on system-wide packages. Use manifests (package.json, requirements.txt).\n3. Config: store in environment variables, not code. Enables same build across environments.\n4. Backing Services: treat databases, queues, caches as attached resources via URLs. Enables swapping without code changes.\n\nThese enable: horizontal scaling, portability across clouds, maintainability through separation of concerns.",
    "difficulty": "medium",
    "topic_area": "The Twelve-Factor App"
  },
  {
    "id": "q008",
    "concept_ids": ["c-regions-001", "c-regions-002", "c-regions-003"],
    "scenario": "A global e-commerce company needs to serve customers in North America, Europe, and Asia with low latency. They also want to ensure their application remains available even if an entire data center fails.",
    "question_text": "Explain the concepts of cloud Regions and Availability Zones. How should the company architect their deployment to meet both the low-latency and high-availability requirements?",
    "model_answer": "Regions: geographically separated locations containing multiple zones. Resources don't auto-replicate across regions. Data sovereignty may require specific regions (GDPR).\n\nAvailability Zones: deployment areas within a region. Zones ≠ data centers — they're logical isolation boundaries ('failure domains'). Connected by low-latency networks.\n\nFor low latency: deploy in multiple regions near users (US, EU, Asia). Use DNS geolocation routing.\nFor high availability: deploy across multiple AZs within each region. Use load balancers across AZs, replicate databases across AZs. Each AZ should handle full load if others fail.\n\nThis multi-region, multi-AZ architecture provides both geographic proximity and redundancy.",
    "difficulty": "hard",
    "topic_area": "Cloud Regions & Availability Zones"
  },
  {
    "id": "q009",
    "concept_ids": ["c-git-003"],
    "scenario": "A new developer wants to contribute to an open-source project hosted on GitHub. The project uses a forking workflow.",
    "question_text": "Explain the Git forking workflow and describe the step-by-step process the developer should follow to make a contribution, from forking the repository to getting their changes merged.",
    "model_answer": "Forking workflow: contributors work on their own copy before proposing changes.\n\nSteps:\n1. Fork: create personal copy on GitHub\n2. Clone: git clone <fork-url>\n3. Add upstream: git remote add upstream <original-url>\n4. Create branch: git checkout -b feature-name\n5. Make changes and commit with clear messages\n6. Push: git push origin feature-name\n7. Open Pull Request from fork to original repo\n8. Respond to review feedback\n9. Keep fork synced: git fetch upstream, git merge upstream/main\n10. Maintainers merge approved PR\n\nThis isolates contributions and allows review before merging.",
    "difficulty": "medium",
    "topic_area": "Git Forking Workflow"
  },
  {
    "id": "q010",
    "concept_ids": ["c-ci-001", "c-ghactions-001"],
    "scenario": "A development team wants to implement CI/CD. Every time code is pushed to the main branch, they want automated tests to run, and if tests pass, the application should be automatically deployed to staging.",
    "question_text": "Explain what Continuous Integration is and its three pillars. How can GitHub Actions be used to implement this workflow?",
    "model_answer": "CI: developers frequently integrate code into shared repository, verified by automated build and tests.\n\nThree pillars:\n1. Frequent Commits: integrate at least daily, small incremental changes\n2. Automated Build: every commit triggers automated build\n3. Automated Testing: tests run automatically on every integration\n\nGitHub Actions implementation:\n1. Create .github/workflows/ci-cd.yml\n2. Define trigger: on push to main branch\n3. Jobs: build (checkout, install deps, run tests), deploy (only if tests pass)\n4. Use marketplace actions or custom scripts\n5. Store deployment credentials in GitHub Secrets\n\nBenefits: early bug detection, faster feedback, consistent process, reduced manual work.",
    "difficulty": "medium",
    "topic_area": "Continuous Integration with GitHub Actions"
  },
  {
    "id": "q011",
    "concept_ids": ["c-testing-001", "c-testing-002", "c-testing-003"],
    "scenario": "A software team is building a new microservices application. The tech lead insists on implementing multiple types of testing. A junior developer asks why they need so many different types of tests.",
    "question_text": "Explain the testing pyramid and the differences between unit testing, integration testing, and end-to-end testing. For each type, describe what it tests and when it would catch a bug that other types might miss.",
    "model_answer": "Testing Pyramid: Base (widest) = Unit Tests (many, fast, isolated), Middle = Integration Tests (moderate, component interactions), Top (narrowest) = E2E Tests (few, slow, complete workflows). Optimizes for fast feedback in CI/CD.\n\nUnit Tests: verify single function/class in isolation, milliseconds, use mocks. Catches: logic errors in calculations that integration tests miss.\n\nIntegration Tests: verify component interactions (DB queries, API calls). Catches: schema mismatches, API contract violations that unit tests can't detect.\n\nE2E Tests: test complete user workflows. Catches: performance bottlenecks, UI issues, cross-service failures only visible in full system.\n\nEach type catches different bug categories — all are needed.",
    "difficulty": "medium",
    "topic_area": "Unit, Integration, Performance, and Load Testing"
  },
  {
    "id": "q012",
    "concept_ids": ["c-iac-001", "c-iac-002", "c-terraform-001"],
    "scenario": "A company currently manages their cloud infrastructure through the AWS console, clicking through the UI. This has led to inconsistencies between environments, configuration drift, and snowflake servers.",
    "question_text": "Explain what Infrastructure as Code (IaC) is and its core principles. What specific problems (configuration drift, snowflake servers) would adopting Terraform solve? Explain the 'Pets vs Cattle' mindset.",
    "model_answer": "IaC: infrastructure automation based on software development practices. Consistent, repeatable routines for provisioning and changing systems.\n\nCore principles: reproducibility, disposability, consistency, repeatability, continuous change.\n\nProblems solved:\n- Configuration drift: servers diverge over time from hotfixes/patches → IaC ensures consistent state\n- Snowflake servers: unique servers that can't be reproduced → IaC makes all servers identical and reproducible\n- No audit trail → Git history tracks all changes\n- Manual errors → Automation eliminates clicking mistakes\n\nPets vs Cattle: Pets are unique servers you name and nurse back to health. Cattle are identical, replaceable servers — if one is sick, you replace it. IaC enables the cattle mindset: treat servers as disposable and reproducible.",
    "difficulty": "medium",
    "topic_area": "Infrastructure as Code w/Terraform"
  },
  {
    "id": "q013",
    "concept_ids": ["c-iam-001", "c-iam-002"],
    "scenario": "A startup is using the root account credentials for everything in AWS. A security consultant warns this is dangerous.",
    "question_text": "Explain what IAM is and the two fundamental questions it answers. Why is using root account credentials risky? What IAM best practices should the startup implement?",
    "model_answer": "IAM controls who can access which resources and what actions they can perform.\nTwo questions: Authentication (Who are you?) and Authorization (What are you allowed to do?).\nSecure by default: deny all access until explicitly granted.\n\nRoot account risks: unlimited unrestricted access, no audit trail per user, can't be restricted, if compromised = complete control, easy accidental damage.\n\nBest practices:\n1. Never use root for daily operations\n2. Enable MFA on root\n3. Create individual IAM users (never share credentials)\n4. Use groups and roles for permissions\n5. Principle of least privilege\n6. Use IAM roles for applications (not access keys)\n7. Rotate credentials regularly\n8. Enable CloudTrail for audit logging",
    "difficulty": "medium",
    "topic_area": "Identity & Access Management (IAM)"
  },
  {
    "id": "q014",
    "concept_ids": ["c-firewall-001", "c-network-005"],
    "scenario": "A company's web application on EC2 needs to be accessible from the internet on port 443 (HTTPS), but the database servers should only be accessible from the web servers, not from the internet.",
    "question_text": "Explain the difference between AWS Security Groups and NACLs. How would you configure firewall rules to meet these security requirements?",
    "model_answer": "Security Groups: instance-level, stateful (return traffic auto-allowed), default deny all inbound/allow all outbound.\nNACLs: subnet-level, stateless (must explicitly allow return traffic), process rules in order.\n\nConfiguration:\nWeb Server Security Group:\n- Inbound: Allow TCP 443 from 0.0.0.0/0 (HTTPS), Allow TCP 22 from company IP (SSH)\n- Outbound: Allow all\n\nDatabase Security Group:\n- Inbound: Allow TCP 3306/5432 ONLY from web server security group\n- Outbound: Allow responses to web servers\n- NO rules allowing 0.0.0.0/0 inbound\n\nThis implements defense in depth and least privilege access.",
    "difficulty": "medium",
    "topic_area": "Network Firewall"
  },
  {
    "id": "q015",
    "concept_ids": ["c-vm-001", "c-vm-002"],
    "scenario": "A company is considering moving from physical servers to virtual machines. The CFO wants business benefits, the CTO wants technical details.",
    "question_text": "Explain what virtualization is and the difference between Type 1 and Type 2 hypervisors. Which type do cloud providers use and why? Discuss both technical and business benefits.",
    "model_answer": "Virtualization: using software to create virtual versions of hardware. Hypervisor runs on physical hardware, creates multiple isolated VMs. Traditional servers run at 10-15% capacity.\n\nType 1 (bare-metal): runs directly on hardware (VMware ESXi, KVM, Xen). Better performance, used by cloud providers.\nType 2 (hosted): runs on top of OS (VirtualBox, VMware Workstation). For development/testing.\n\nCloud providers use Type 1 with hardware-assisted virtualization (Intel VT-x, AMD-V) and custom hypervisors (AWS Nitro).\n\nTechnical benefits: resource efficiency, isolation, flexibility, hardware independence, easy snapshots.\nBusiness benefits: cost reduction (fewer physical servers), faster provisioning (minutes vs weeks), better ROI, reduced downtime, scalability.",
    "difficulty": "medium",
    "topic_area": "Virtual Machines"
  },
  {
    "id": "q016",
    "concept_ids": ["c-image-001", "c-image-003"],
    "scenario": "A company needs to deploy the same application stack across AWS, GCP, and Azure for different clients. They want consistency and to avoid manually configuring each cloud's machine images separately.",
    "question_text": "Explain what machine images are and the concept of immutable infrastructure. How can Packer solve this multi-cloud challenge?",
    "model_answer": "Machine images: pre-configured templates containing OS, software, configurations. AWS AMI, GCP Custom Image, Azure VM Image. Solves 'snowflake server' problem.\n\nImmutable infrastructure: instead of modifying running servers, replace them with new instances from updated images. More predictable, easier to debug.\n\nPacker: HashiCorp tool that automates image creation across platforms from single source configuration.\nBenefits: single source of truth (one template, all clouds), consistency, automation, version control, reproducibility, CI/CD integration.\n\nAdvantages over manual: build all cloud images simultaneously, eliminate manual errors, change template once → rebuild all, template serves as documentation.",
    "difficulty": "medium",
    "topic_area": "Custom Machine Images"
  },
  {
    "id": "q017",
    "concept_ids": ["c-cloudinit-001", "c-cloudinit-003"],
    "scenario": "A DevOps engineer needs to launch 100 EC2 instances that should automatically install dependencies, configure settings, and start services when they boot. Manually SSH-ing into each is not feasible.",
    "question_text": "Explain what cloud-init is and the 'Golden Image + cloud-init' pattern. What should be baked into the image vs. applied at boot time with cloud-init?",
    "model_answer": "cloud-init: industry standard for cloud instance initialization. Runs during first boot, executes user data scripts to set up instances automatically.\n\nGolden Image + cloud-init pattern:\n- Bake into image (static/shared): OS, common packages, security hardening, base configuration\n- Apply via cloud-init (dynamic/instance-specific): SSH keys, environment-specific config (DB hostnames, API endpoints), secrets/credentials, service discovery\n\nBenefits: fast boot (pre-baked image) + flexibility (cloud-init customization). Never bake secrets into images (they may be shared/stored).\n\ncloud-init vs Ansible/Puppet: cloud-init runs once at first boot (no agent needed), Ansible/Puppet manage ongoing configuration. Often used together: cloud-init bootstraps, then Ansible manages.",
    "difficulty": "medium",
    "topic_area": "cloud-init"
  },
  {
    "id": "q018",
    "concept_ids": ["c-systemd-001", "c-systemd-002"],
    "scenario": "A Linux administrator needs to ensure a web application automatically starts when the server boots and restarts if it crashes. They also need to view logs to troubleshoot issues.",
    "question_text": "Explain what systemd is and how it manages services. Describe how to create a systemd service unit file for the web application and the key commands to manage it.",
    "model_answer": "systemd: init system and service manager for modern Linux. Runs as PID 1. Manages boot, service lifecycle, dependency ordering, logging (journald).\n\nService unit file (/etc/systemd/system/webapp.service):\n[Unit]\nDescription=Web Application\nAfter=network.target\n\n[Service]\nType=simple\nUser=webapp\nExecStart=/usr/bin/node /opt/webapp/server.js\nRestart=always\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n\nCommands:\n- systemctl daemon-reload (after file changes)\n- systemctl enable webapp (start on boot)\n- systemctl start/stop/restart webapp\n- systemctl status webapp\n- journalctl -u webapp (view logs)\n- journalctl -u webapp -f (follow logs)",
    "difficulty": "medium",
    "topic_area": "systemd - System and Service Manager"
  },
  {
    "id": "q019",
    "concept_ids": ["c-sdlc-001", "c-sdlc-002"],
    "scenario": "A project manager is planning a new cloud-native application development project. They want to follow a structured development process but also ensure the application is designed for the cloud from the start.",
    "question_text": "Describe the phases of the Software Development Life Cycle (SDLC). For each phase, explain what cloud-native considerations should be taken into account.",
    "model_answer": "SDLC Phases with cloud-native considerations:\n\n1. Requirements: gather functional and non-functional requirements. Cloud-native: consider scalability, availability, and cloud service constraints.\n\n2. Design: architecture decisions (monolithic vs microservices, APIs, security). Cloud-native: embrace Twelve-Factor principles, design for statelessness and externalized configuration.\n\n3. Implementation: write code with version control, code reviews, unit testing. Cloud-native: code should be container-ready with externalized configuration.\n\n4. Testing: unit, integration, system, acceptance testing. Cloud-native: must include container orchestration scenarios.\n\n5. Deployment: release to production. Cloud-native: CI/CD pipelines, blue-green deployments.\n\n6. Maintenance: monitor, patch, update. Cloud-native: observability, auto-scaling, infrastructure as code.",
    "difficulty": "medium",
    "topic_area": "The Software Development Life Cycle (SDLC)"
  },
  {
    "id": "q020",
    "concept_ids": ["c-linux-001", "c-linux-003", "c-linux-004"],
    "scenario": "A new cloud engineer needs to manage Linux servers in a cloud environment. They come from a Windows background and are unfamiliar with the Linux command line.",
    "question_text": "Explain why Linux dominates cloud and server environments. Describe the Linux file system structure and permission model, including how to use chmod to set permissions.",
    "model_answer": "Why Linux dominates cloud: open source (no licensing costs at scale), stability (servers run for years), resource efficiency (lightweight), strong security model (permissions, rapid patches), flexibility (highly configurable). Over 90% of public cloud workloads run on Linux.\n\nFile system: hierarchical tree from root (/). Key directories: /home (user files), /etc (configuration), /var (variable data), /tmp (temporary).\n\nPermission model: three categories (owner, group, others) × three permissions (read=4, write=2, execute=1).\nExamples: chmod 755 file = rwxr-xr-x (owner: full, group/others: read+execute)\nchmod 644 file = rw-r--r-- (owner: read+write, group/others: read only)\nchown user:group file = change ownership",
    "difficulty": "medium",
    "topic_area": "The Linux Command Line"
  },
  {
    "id": "q021",
    "concept_ids": ["c-terraform-002", "c-terraform-003"],
    "scenario": "A DevOps engineer is setting up Terraform for the first time to manage AWS infrastructure. They need to create a VPC with subnets and an EC2 instance.",
    "question_text": "Describe the Terraform workflow (init, plan, apply, destroy) and explain what each command does. What is the Terraform state file and why is it important?",
    "model_answer": "Terraform workflow:\n1. terraform init: downloads provider plugins (e.g., AWS provider) to .terraform folder. Run first time or when adding providers.\n2. terraform plan: shows what will change without making changes. Preview of create/modify/destroy actions.\n3. terraform apply: creates/modifies actual infrastructure to match configuration.\n4. terraform destroy: removes all Terraform-managed resources.\n\nState file (.tfstate): tracks current infrastructure state. Maps Terraform config to real-world resources. Critical because:\n- Terraform compares desired state (config) with current state (state file) to determine changes\n- Without it, Terraform can't know what exists\n- Should be stored remotely (S3 + DynamoDB) for team collaboration\n- Contains sensitive data — treat as secret\n\nResources: aws_vpc, aws_subnet, aws_instance. Variables make configs reusable. Outputs display created resource info.",
    "difficulty": "medium",
    "topic_area": "Infrastructure as Code w/Terraform"
  },
{
    "id": "q022",
    "concept_ids": ["c-ghactions-001", "c-ghactions-002", "c-ghactions-003"],
    "scenario": "A team wants to set up a GitHub Actions workflow that runs tests on every pull request and deploys to staging when code is merged to main.",
    "question_text": "Describe the components of GitHub Actions (Workflows, Events, Jobs, Steps, Actions, Runners). How would you structure a workflow YAML file for this CI/CD pipeline? How are secrets managed?",
    "model_answer": "GitHub Actions components:\n- Workflow: automated procedure defined in YAML (.github/workflows/)\n- Events: triggers (push, pull_request, schedule, workflow_dispatch)\n- Jobs: set of steps running on same runner\n- Steps: individual tasks within a job (sequential)\n- Actions: reusable units of code (marketplace or custom)\n- Runners: servers executing jobs (GitHub-hosted: Ubuntu/Windows/macOS, or self-hosted)\n\nWorkflow structure:\nname: CI/CD\non:\n  pull_request: (run tests)\n  push:\n    branches: [main] (deploy on merge)\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps: checkout, install deps, run tests\n  deploy:\n    needs: test\n    if: github.ref == 'refs/heads/main'\n    steps: deploy to staging\n\nSecrets: encrypted environment variables stored in repo settings. Access via ${{ secrets.SECRET_NAME }}. Never hardcode credentials in workflow files.",
    "difficulty": "hard",
    "topic_area": "Continuous Integration with GitHub Actions"
  },
  {
    "id": "q023",
    "concept_ids": ["c-git-001", "c-git-002"],
    "scenario": "A junior developer accidentally committed sensitive credentials to the main branch and pushed to GitHub. The team lead needs to explain version control concepts while addressing this incident.",
    "question_text": "Explain the core concepts of Git (repository, commit, branch, merge) and the difference between centralized and distributed version control. How should the team handle the exposed credentials?",
    "model_answer": "Git core concepts:\n- Repository: database storing all files and complete history\n- Commit: snapshot at specific point (unique ID, message, metadata)\n- Branch: parallel version for experimentation\n- Merge: combining changes from different branches\n\nCentralized (SVN): single server, clients check out files. Risk: server failure = everything lost.\nDistributed (Git): every clone is full backup, works offline, fast operations.\n\nHandling exposed credentials:\n1. Immediately rotate/revoke the exposed credentials\n2. Remove from code: use git filter-branch or BFG Repo-Cleaner to purge from history\n3. Force push cleaned history\n4. Prevent future incidents: use .gitignore for sensitive files, use environment variables (Twelve-Factor: Factor 3), enable GitHub secret scanning, use pre-commit hooks",
    "difficulty": "hard",
    "topic_area": "Version Control with Git"
  },
  {
    "id": "q024",
    "concept_ids": ["c-iam-003"],
    "scenario": "A company is deploying applications across AWS, GCP, and Azure. The security team needs to understand how IAM works differently across these providers.",
    "question_text": "Compare and contrast IAM models across AWS, GCP, and Azure. What are the key differences in how each provider handles identities, roles, and policies?",
    "model_answer": "AWS IAM: Users, Groups, Roles with JSON policies. Two policy types: identity-based (attached to users/groups/roles) and resource-based (attached to resources like S3 buckets). Flat structure.\n\nGCP IAM: Google Accounts, Service Accounts. Roles: Basic (Owner/Editor/Viewer), Predefined (service-specific), Custom. Resource hierarchy: Organization → Folders → Projects → Resources. Policies inherit down the hierarchy.\n\nAzure: Azure Active Directory (Azure AD) for identity. RBAC roles assigned at Management Group → Subscription → Resource Group → Resource level. Strong integration with Microsoft ecosystem.\n\nKey differences:\n- AWS: flat, policy-based\n- GCP: hierarchical, role-based with inheritance\n- Azure: AD-centric, RBAC with management groups\n\nAll share: principle of least privilege, deny by default, support for service accounts/machine identities.",
    "difficulty": "hard",
    "topic_area": "Identity & Access Management (IAM)"
  },
{
    "id": "q025",
    "concept_ids": ["c-devops-003"],
    "scenario": "A CTO wants to measure whether their DevOps transformation is actually improving software delivery. They need concrete metrics to track progress.",
    "question_text": "Explain the CALMS framework for DevOps and describe the four key DORA metrics used to measure DevOps performance. How do these metrics relate to the DevOps principles?",
    "model_answer": "CALMS framework:\n- Culture: shared responsibility, blameless postmortems\n- Automation: CI/CD pipelines, IaC, automated testing\n- Lean: eliminate waste, small batch sizes\n- Measurement: data-driven decisions, track key metrics\n- Sharing: knowledge sharing across teams\n\nDORA metrics (from Measurement pillar):\n1. Deployment Frequency: how often code deploys to production (higher = better)\n2. Lead Time for Changes: time from commit to production (shorter = better)\n3. Mean Time to Recovery (MTTR): time to restore service after incident (shorter = better)\n4. Change Failure Rate: % of deployments causing failures (lower = better)\n\nRelationship: Automation (CI/CD) improves deployment frequency and lead time. Culture (blameless postmortems) improves MTTR. Lean (small batches) reduces change failure rate. Measurement tracks all of these for continuous improvement.",
    "difficulty": "hard",
    "topic_area": "Introduction to DevOps"
  },
  {
    "id": "q026",
    "concept_ids": ["c-cloudproviders-001"],
    "scenario": "A startup is choosing between AWS, GCP, and Azure for their cloud infrastructure. They need to understand the strengths of each provider.",
    "question_text": "Compare AWS, GCP, and Azure in terms of their key strengths, primary services, and ideal use cases. What factors should the startup consider when choosing a cloud provider?",
    "model_answer": "AWS: largest market share, broadest service offering. Key services: EC2, S3, RDS, Lambda. Strength: most mature, widest ecosystem.\n\nGCP: strong in data analytics and ML. Key services: Compute Engine, BigQuery, TensorFlow, GKE. Strength: Kubernetes originated here, excellent data tools.\n\nAzure: strong enterprise integration. Key services: Virtual Machines, Azure SQL, Azure AD. Strength: seamless Microsoft product integration (Office 365, Active Directory).\n\nSelection factors:\n1. Existing technology stack (Microsoft shop → Azure)\n2. Specific service needs (ML → GCP, broadest services → AWS)\n3. Pricing and cost structure\n4. Geographic availability (regions near users)\n5. Compliance requirements\n6. Team expertise and training\n7. Support and SLA requirements",
    "difficulty": "medium",
    "topic_area": "Overview of Public Cloud Providers"
  },
  {
    "id": "q027",
    "concept_ids": ["c-network-005", "c-network-006"],
    "scenario": "A cloud architect is designing the network for a new application on AWS. They need to set up a VPC with public and private subnets.",
    "question_text": "Explain what a VPC is, how subnets work with CIDR notation, and the role of NAT gateways and Internet Gateways. How would you design a network with public web servers and private database servers?",
    "model_answer": "VPC: isolated virtual network in the cloud. You control IP ranges, subnets, routing, security.\n\nCIDR notation: 10.0.0.0/16 = first 16 bits are network (65,536 IPs). /24 = 256 IPs.\n\nInternet Gateway: connects VPC to internet (for public subnets).\nNAT Gateway: allows private subnet instances to access internet (for updates) without being directly accessible.\n\nDesign:\n- VPC: 10.0.0.0/16\n- Public subnet (10.0.1.0/24): web servers, route table → Internet Gateway\n- Private subnet (10.0.2.0/24): databases, route table → NAT Gateway (outbound only)\n- Security Groups: web servers allow 443 from internet, databases allow 3306 only from web server SG\n\nThis ensures databases are never directly accessible from internet while web servers serve public traffic.",
    "difficulty": "hard",
    "topic_area": "Networking Fundamentals"
  },
  {
    "id": "q028",
    "concept_ids": ["c-terraform-network-001"],
    "scenario": "A team is migrating from manual AWS console management to Terraform. They need to define their existing VPC, subnets, and security groups as Terraform code.",
    "question_text": "How would you use Terraform to define cloud networking resources (VPC, subnets, security groups) across AWS? What are the key Terraform resource types for networking?",
    "model_answer": "Key Terraform networking resources:\n\naws_vpc: defines the virtual network\nresource \"aws_vpc\" \"main\" {\n  cidr_block = \"10.0.0.0/16\"\n}\n\naws_subnet: defines subnets within VPC\nresource \"aws_subnet\" \"public\" {\n  vpc_id = aws_vpc.main.id\n  cidr_block = \"10.0.1.0/24\"\n  availability_zone = \"us-east-1a\"\n}\n\naws_internet_gateway: connects VPC to internet\naws_route_table: defines routing rules\naws_security_group: defines firewall rules\n\nresource \"aws_security_group\" \"web\" {\n  vpc_id = aws_vpc.main.id\n  ingress { from_port=443, to_port=443, protocol=\"tcp\", cidr_blocks=[\"0.0.0.0/0\"] }\n}\n\nSame approach for GCP (google_compute_network, google_compute_firewall) and Azure (azurerm_virtual_network, azurerm_network_security_group).\n\nBenefits: version controlled, reproducible, consistent across environments.",
    "difficulty": "hard",
    "topic_area": "AWS, GCP & Azure Networking with Terraform"
  },
  {
    "id": "q029",
    "concept_ids": ["c-vm-004", "c-vm-005"],
    "scenario": "A company is running sensitive financial workloads on AWS EC2. The security team is concerned about multi-tenant risks and wants to understand instance pricing options.",
    "question_text": "Explain the different EC2 pricing models (On-Demand, Reserved, Spot). What are the Meltdown and Spectre vulnerabilities and why are they particularly concerning in multi-tenant cloud environments? What mitigation options exist?",
    "model_answer": "EC2 Pricing:\n- On-Demand: pay per hour/second, no commitment. Best for unpredictable workloads.\n- Reserved: 1-3 year commitment, significant discount (up to 72%). Best for steady-state workloads.\n- Spot: spare capacity at up to 90% discount, but can be interrupted. Best for fault-tolerant batch jobs.\n\nMeltdown/Spectre: CPU hardware vulnerabilities exploiting speculative execution. Attackers can read memory data from other processes/VMs on same physical host.\n\nWhy concerning in cloud: VMs from different customers share physical hardware. An attacker's VM could potentially read data from your VM on the same host.\n\nMitigations:\n1. Microcode updates and kernel patches (applied by cloud providers)\n2. Dedicated Hosts: physical server exclusively for your use\n3. Dedicated Instances: hardware not shared with other accounts\n4. Confidential computing: encrypted memory enclaves\n\nFor sensitive financial workloads: consider Dedicated Hosts for compliance.",
    "difficulty": "hard",
    "topic_area": "Virtual Machines"
  },
  {
    "id": "q030",
    "concept_ids": ["c-image-002"],
    "scenario": "A DevOps team is building a CI/CD pipeline that includes automated machine image creation. They want to ensure images are secure, versioned, and compliant.",
    "question_text": "Explain the concept of a Golden Image and why it matters for security, compliance, and operational consistency. What best practices should the team follow for machine image management?",
    "model_answer": "Golden Image: standardized base image with security hardening, compliance requirements, and operational consistency baked in. All production instances launch from this image.\n\nWhy it matters:\n- Security: pre-hardened with security patches, disabled unnecessary services, configured firewalls\n- Compliance: meets regulatory requirements (CIS benchmarks, SOC2)\n- Consistency: eliminates 'snowflake servers', every instance is identical\n\nBest practices:\n1. Versioning: tag images with version numbers and build dates\n2. Automated building: use Packer in CI/CD pipeline\n3. Security hardening: CIS benchmarks, remove default credentials\n4. Lifecycle policies: deprecate old images, enforce minimum versions\n5. Testing: validate images before promoting to production\n6. Documentation: template serves as documentation\n7. Immutable infrastructure: replace instances instead of patching\n8. Regular updates: rebuild images with latest security patches",
    "difficulty": "medium",
    "topic_area": "Building Custom Machine Images using Hashicorp Packer"
  },
  {
    "id": "q031",
    "concept_ids": ["c-cloud-001"],
    "scenario": "",
    "question_text": "Define cloud computing and explain how it differs from traditional on-premises infrastructure. What are the key characteristics that make a service 'cloud' rather than just 'hosted'? Discuss the relationship between cloud computing and concepts like on-demand provisioning, pay-per-use billing, and resource elasticity.",
    "model_answer": "Cloud computing is the delivery of computing services — servers, storage, databases, networking, software — over the internet ('the cloud') on an on-demand basis. Unlike traditional on-premises infrastructure where organizations purchase, install, and maintain physical hardware in their own data centers, cloud computing shifts this responsibility to a cloud provider.\n\nKey differences from traditional infrastructure:\n1. Capital vs Operational Expenditure: On-premises requires large upfront capital investment (CapEx) for hardware. Cloud uses pay-as-you-go operational expenditure (OpEx).\n2. Provisioning Speed: On-premises takes weeks/months to procure and set up hardware. Cloud provisions resources in minutes.\n3. Maintenance Responsibility: On-premises requires in-house teams for hardware maintenance, patching, cooling, power. Cloud providers handle physical infrastructure.\n4. Scalability: On-premises has fixed capacity requiring over-provisioning for peak loads. Cloud scales dynamically.\n\nWhat makes a service truly 'cloud' (not just 'hosted'):\n- On-demand self-service: Users provision resources automatically without human interaction with the provider.\n- Pay-per-use billing: Charges based on actual consumption (compute hours, storage GB, data transfer), not flat fees.\n- Resource elasticity: Ability to rapidly scale resources up or down to match demand, with resources appearing virtually unlimited.\n- Multi-tenancy: Provider's resources are pooled to serve multiple customers efficiently.\n- Broad network access: Services accessible over the internet via standard protocols.\n\nThese characteristics work together: on-demand provisioning enables rapid response to changing needs, pay-per-use billing ensures cost efficiency (no paying for idle resources), and elasticity means applications can handle traffic spikes without pre-planning capacity.",
    "difficulty": "medium",
    "topic_area": "Fundamentals of Cloud Computing"
  },
  {
    "id": "q032",
    "concept_ids": ["c-cloud-002"],
    "scenario": "",
    "question_text": "List and explain each of NIST's five essential characteristics of cloud computing. For each characteristic, describe what it means in practice and give an example of how a traditional data center might fail to meet that characteristic. Why is this framework important for distinguishing true cloud services from traditional hosting?",
    "model_answer": "NIST's five essential characteristics define what qualifies as cloud computing:\n\n1. On-Demand Self-Service: A consumer can provision computing resources (servers, storage, network) automatically without requiring human interaction with the service provider. In practice, this means a developer can spin up a VM via a web console or API in minutes. Traditional data center failure: Requesting a new server requires submitting a ticket to IT, waiting for procurement approval, hardware delivery, and manual setup — taking days or weeks.\n\n2. Broad Network Access: Resources are available over the network and accessed through standard mechanisms (HTTP/HTTPS, APIs) that promote use by diverse client platforms (phones, tablets, laptops, workstations). Traditional data center failure: Resources may only be accessible within the corporate LAN or require VPN with proprietary client software, limiting access from diverse devices.\n\n3. Resource Pooling: The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to demand. Consumers generally have no control or knowledge over the exact location of resources. Traditional data center failure: Each team gets dedicated physical servers that sit idle when not in use. Resources are statically assigned and cannot be shared or reallocated dynamically.\n\n4. Rapid Elasticity: Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, capabilities appear unlimited. Traditional data center failure: Scaling requires purchasing and installing new hardware — a process that takes weeks. Cannot scale down when demand drops, leading to wasted capacity.\n\n5. Measured Service: Cloud systems automatically control and optimize resource use by leveraging a metering capability. Resource usage is monitored, controlled, and reported, providing transparency for both provider and consumer (pay-per-use). Traditional data center failure: Organizations pay fixed costs for hardware regardless of utilization. No granular usage tracking or billing based on actual consumption.\n\nThis framework is important because many services are marketed as 'cloud' but are actually traditional hosting with a web interface. The NIST framework provides an objective standard: if a service lacks any of these five characteristics, it is not true cloud computing. For example, a co-location facility with VMs may have broad network access but lack self-service provisioning, resource pooling, elasticity, and measured service — making it virtualization, not cloud.",
    "difficulty": "hard",
    "topic_area": "Fundamentals of Cloud Computing"
  },
  {
    "id": "q033",
    "concept_ids": ["c-cloud-003"],
    "scenario": "",
    "question_text": "Compare and contrast horizontal scaling (scale out) and vertical scaling (scale up) in detail. For each approach, discuss the architectural implications, advantages, limitations, cost characteristics, and failure modes. Under what circumstances would you choose one over the other?",
    "model_answer": "Vertical Scaling (Scale Up):\n- Definition: Increasing the capacity of a single machine by adding more CPU, RAM, storage, or upgrading to a more powerful instance type.\n- Architectural implications: Application architecture remains simple — single server, no distributed systems complexity. No need to handle data partitioning, load balancing, or session management across nodes.\n- Advantages: Simplicity (no code changes needed), easier debugging (single server), lower operational complexity, works for legacy monolithic applications.\n- Limitations: Hardware ceiling — there is a maximum size for any single machine. Single point of failure — if the server goes down, everything goes down. Often requires downtime to resize. Cost increases non-linearly (doubling resources may more than double cost).\n- Cost: Expensive at scale. Larger instances have premium pricing. Paying for peak capacity even during low-demand periods.\n\nHorizontal Scaling (Scale Out):\n- Definition: Adding more machines to distribute the workload across multiple servers working in parallel.\n- Architectural implications: Requires distributed architecture — load balancers, stateless application design, shared/distributed data stores, session management (externalized sessions). Application must be designed for horizontal scaling from the start.\n- Advantages: No theoretical upper limit on scaling. Built-in redundancy (if one node fails, others continue). Cost-effective with commodity hardware. Can scale incrementally. Enables auto-scaling based on demand.\n- Limitations: Increased complexity (distributed systems challenges: network partitions, data consistency, service discovery). Requires application redesign if originally built for single server. More operational overhead (monitoring multiple nodes).\n- Cost: More cost-effective at scale. Can use smaller, cheaper instances. Pay only for what you need by adding/removing nodes.\n\nWhen to choose vertical scaling: Legacy applications that cannot be easily refactored, databases that are difficult to distribute (though read replicas help), small-to-medium workloads where simplicity is valued, quick fixes for immediate capacity needs.\n\nWhen to choose horizontal scaling: Applications with unpredictable or spiky traffic patterns, high-availability requirements, cloud-native applications designed for distributed operation, when you need to exceed the limits of any single machine, cost-sensitive workloads at scale.\n\nIn practice, most cloud architectures use a combination: vertically scale individual nodes to a reasonable size, then horizontally scale by adding more nodes.",
    "difficulty": "hard",
    "topic_area": "Fundamentals of Cloud Computing"
  },
  {
    "id": "q034",
    "concept_ids": ["c-cloud-004"],
    "scenario": "",
    "question_text": "Explain the three cloud service models — IaaS, PaaS, and SaaS — in terms of the shared responsibility model. For each model, describe what the cloud provider manages versus what the customer manages. Provide concrete examples of each and discuss how the choice of service model affects operational overhead, flexibility, and control.",
    "model_answer": "The three cloud service models represent different levels of abstraction, each shifting more responsibility from the customer to the provider:\n\nIaaS (Infrastructure as a Service):\n- Provider manages: Physical hardware, networking, storage, virtualization layer, data center facilities.\n- Customer manages: Operating system, middleware, runtime, applications, data, security patches, configuration.\n- Examples: AWS EC2, Google Compute Engine, Azure Virtual Machines.\n- Characteristics: Maximum flexibility and control. Customer has full access to the OS and can install any software. Highest operational overhead — customer responsible for patching, security hardening, scaling configuration. Best for: teams that need full control, custom software stacks, lift-and-shift migrations.\n\nPaaS (Platform as a Service):\n- Provider manages: Everything in IaaS plus OS, middleware, runtime environment, auto-scaling, load balancing.\n- Customer manages: Application code, data, and some configuration.\n- Examples: Heroku, AWS Elastic Beanstalk, Google App Engine, Azure App Service.\n- Characteristics: Reduced operational overhead — no OS patching or server management. Less flexibility — constrained to supported languages/frameworks. Faster development cycles — focus on code, not infrastructure. Best for: development teams wanting to focus on application logic, rapid prototyping, standard web applications.\n\nSaaS (Software as a Service):\n- Provider manages: Everything — infrastructure, platform, application, updates, availability.\n- Customer manages: Data input, user configuration, access management.\n- Examples: Gmail, Salesforce, Microsoft 365, Slack.\n- Characteristics: Zero operational overhead for the application itself. Least flexibility — use the application as provided. Fastest time to value — sign up and start using. Best for: standard business functions (email, CRM, collaboration) where customization needs are minimal.\n\nThe trade-off across models is control vs. convenience:\n- IaaS: High control, high overhead, high flexibility\n- PaaS: Moderate control, moderate overhead, moderate flexibility\n- SaaS: Low control, minimal overhead, low flexibility\n\nAs you move from IaaS to SaaS, you trade operational control for reduced management burden. The right choice depends on the team's expertise, the application's requirements, and how much infrastructure management the organization wants to handle.",
    "difficulty": "medium",
    "topic_area": "Fundamentals of Cloud Computing"
  },
  {
    "id": "q035",
    "concept_ids": ["c-sdlc-001"],
    "scenario": "",
    "question_text": "Describe each phase of the Software Development Life Cycle (SDLC) in detail. For each phase, explain its purpose, key activities, deliverables, and how it connects to the phases before and after it. Why is a structured SDLC important, and what risks arise when phases are skipped or poorly executed?",
    "model_answer": "The SDLC provides a structured framework for developing software through defined phases:\n\n1. Requirements Gathering:\n- Purpose: Understand what the software must do and the constraints it must operate within.\n- Key activities: Stakeholder interviews, user story creation, functional and non-functional requirements documentation, feasibility analysis.\n- Deliverables: Requirements specification document, user stories, acceptance criteria.\n- Connection: Feeds directly into Design phase. Poor requirements lead to incorrect designs.\n\n2. Design:\n- Purpose: Define the architecture and technical approach to meet requirements.\n- Key activities: System architecture decisions (monolithic vs microservices), API design, database schema design, security architecture, technology stack selection.\n- Deliverables: Architecture diagrams, API specifications, database schemas, design documents.\n- Connection: Translates requirements into a technical blueprint. Guides implementation.\n\n3. Implementation:\n- Purpose: Write the actual code based on the design.\n- Key activities: Coding, version control (Git), code reviews, pair programming, following coding standards.\n- Deliverables: Source code, unit tests, code documentation.\n- Connection: Implements the design. Produces artifacts for testing.\n\n4. Testing:\n- Purpose: Verify the software works correctly and meets requirements.\n- Key activities: Unit testing, integration testing, system testing, acceptance testing, performance testing.\n- Deliverables: Test results, bug reports, test coverage reports.\n- Connection: Validates implementation against requirements. Identifies defects before deployment.\n\n5. Deployment:\n- Purpose: Release the software to production for end users.\n- Key activities: Environment setup, deployment execution, smoke testing, rollback planning.\n- Deliverables: Production release, deployment documentation, release notes.\n- Connection: Makes tested software available. Transitions to maintenance.\n\n6. Maintenance:\n- Purpose: Keep the software running, fix bugs, and add enhancements.\n- Key activities: Monitoring, bug fixes, security patches, performance optimization, feature enhancements.\n- Deliverables: Patches, updates, monitoring reports.\n- Connection: Feeds back into requirements for the next iteration.\n\nImportance of structured SDLC: Provides predictability, ensures quality, manages risk, enables team coordination, creates documentation trail.\n\nRisks of skipping phases: Skipping requirements → building the wrong thing. Skipping design → spaghetti architecture. Skipping testing → production bugs. Skipping maintenance planning → system degradation over time.",
    "difficulty": "medium",
    "topic_area": "The Software Development Life Cycle (SDLC)"
  },
  {
    "id": "q036",
    "concept_ids": ["c-sdlc-002"],
    "scenario": "",
    "question_text": "Explain how cloud-native principles, particularly the Twelve-Factor App methodology, should influence each phase of the SDLC. What specific considerations must be addressed at the requirements, design, implementation, testing, deployment, and maintenance phases when building applications intended for cloud environments?",
    "model_answer": "Cloud-native SDLC integrates cloud principles into every phase:\n\n1. Requirements Phase — Cloud-Native Considerations:\n- Include non-functional requirements specific to cloud: auto-scaling thresholds, availability SLAs (e.g., 99.9%), data residency requirements (GDPR compliance), multi-region deployment needs.\n- Define cost constraints and budget for cloud resource consumption.\n- Consider cloud service limits and quotas as constraints.\n- Twelve-Factor connection: Factor 3 (Config) — requirements should specify which configurations vary by environment.\n\n2. Design Phase — Cloud-Native Considerations:\n- Embrace statelessness (Factor 6: Processes) — design services that don't rely on local state or sticky sessions. Use external stores (Redis, databases) for shared state.\n- Externalize configuration (Factor 3: Config) — design for environment variables, not hardcoded values.\n- Design for failure — assume any component can fail. Implement circuit breakers, retries, graceful degradation.\n- Twelve-Factor connection: Factor 4 (Backing Services) — design database, cache, and queue connections as swappable attached resources.\n\n3. Implementation Phase — Cloud-Native Considerations:\n- Code should be container-ready with externalized configuration.\n- Explicitly declare all dependencies (Factor 2: Dependencies) — use package managers, never rely on system-wide packages.\n- One codebase, many deploys (Factor 1: Codebase) — same code deploys to dev, staging, production.\n- Treat logs as event streams (Factor 11: Logs) — write to stdout/stderr, let the platform handle log aggregation.\n\n4. Testing Phase — Cloud-Native Considerations:\n- Test with containerized environments that mirror production.\n- Include infrastructure tests (Terraform plan validation, security group rules).\n- Test auto-scaling behavior and failure recovery.\n- Twelve-Factor connection: Factor 10 (Dev/Prod Parity) — test environments should closely match production.\n\n5. Deployment Phase — Cloud-Native Considerations:\n- Implement CI/CD pipelines for automated deployment.\n- Strictly separate build, release, and run stages (Factor 5: Build/Release/Run).\n- Use blue-green or canary deployments for zero-downtime releases.\n- Twelve-Factor connection: Factor 9 (Disposability) — fast startup and graceful shutdown enable rapid deployments.\n\n6. Maintenance Phase — Cloud-Native Considerations:\n- Implement comprehensive observability (metrics, logs, traces).\n- Use Infrastructure as Code for all environment changes.\n- Automate scaling and self-healing.\n- Twelve-Factor connection: Factor 12 (Admin Processes) — run maintenance tasks as one-off processes, not manual SSH sessions.",
    "difficulty": "hard",
    "topic_area": "The Software Development Life Cycle (SDLC)"
  },
  {
    "id": "q037",
    "concept_ids": ["c-12factor-001"],
    "scenario": "",
    "question_text": "Explain the origin and purpose of the Twelve-Factor App methodology. What problems was it designed to solve, and what are its core philosophical principles? How does following this methodology prepare an application for cloud deployment, continuous delivery, and horizontal scaling?",
    "model_answer": "Origin and Purpose:\nThe Twelve-Factor App methodology was published in 2011 by developers at Heroku (notably Adam Wiggins) based on their experience running thousands of applications on their cloud platform. It distills patterns observed in successful cloud-native SaaS applications into twelve best practices.\n\nProblems it was designed to solve:\n1. Environment drift: Applications that work in development but fail in production due to environmental differences.\n2. Deployment complexity: Manual, error-prone deployment processes that make releases risky and infrequent.\n3. Scaling difficulties: Applications tightly coupled to specific infrastructure that cannot scale horizontally.\n4. Vendor lock-in: Applications dependent on specific OS features or system-level packages.\n5. Configuration management: Secrets and configuration hardcoded in source code, making multi-environment deployment difficult.\n6. Operational opacity: Applications that are difficult to monitor, debug, and maintain in production.\n\nCore philosophical principles:\n1. Declarative formats for setup automation: Minimize time and cost for new developers joining the project.\n2. Clean contract with the underlying OS: Offer maximum portability between execution environments.\n3. Suitable for deployment on modern cloud platforms: Obviate the need for servers and systems administration.\n4. Minimize divergence between development and production: Enable continuous deployment for maximum agility.\n5. Scale up without significant changes: To tools, architecture, or development practices.\n\nHow it prepares for cloud deployment:\n- Externalized configuration (Factor 3) means the same build artifact deploys to any environment (dev, staging, production) — just change environment variables.\n- Stateless processes (Factor 6) enable horizontal scaling — any instance can handle any request.\n- Port binding (Factor 7) makes the app self-contained and deployable anywhere.\n- Disposability (Factor 9) with fast startup and graceful shutdown enables rapid scaling and deployment.\n\nHow it enables continuous delivery:\n- Single codebase (Factor 1) with many deploys ensures consistency.\n- Strict build/release/run separation (Factor 5) creates reproducible releases.\n- Dev/prod parity (Factor 10) reduces 'works on my machine' problems.\n\nHow it supports horizontal scaling:\n- Stateless processes (Factor 6) — no sticky sessions or local state.\n- Concurrency via process model (Factor 8) — scale by running more processes.\n- Backing services as attached resources (Factor 4) — shared state lives in external services accessible by all instances.",
    "difficulty": "hard",
    "topic_area": "The Twelve-Factor App"
  },
  {
    "id": "q038",
    "concept_ids": ["c-12factor-002"],
    "scenario": "",
    "question_text": "Describe all twelve factors of the Twelve-Factor App methodology. For each factor, explain the principle, the problem it solves, and what violating it looks like in practice. Group the factors by their primary concern (codebase management, dependency and configuration, runtime behavior, deployment and operations).",
    "model_answer": "Grouped by primary concern:\n\n**Codebase Management:**\nFactor 1 — Codebase: One codebase tracked in version control, many deploys. The same codebase is deployed to dev, staging, and production. Violation: multiple codebases for different environments, or multiple apps sharing one repo without clear boundaries. Problem solved: ensures consistency across all deployments.\n\nFactor 2 — Dependencies: Explicitly declare and isolate dependencies. Use a dependency manifest (package.json, requirements.txt, pom.xml) and never rely on system-wide packages. Violation: assuming certain tools or libraries are pre-installed on the system, or using implicit dependencies. Problem solved: reproducible builds on any machine.\n\n**Dependency and Configuration:**\nFactor 3 — Config: Store configuration in environment variables. Strict separation of config from code. Config varies between deploys (database URLs, API keys, feature flags); code does not. Violation: hardcoding database connection strings, API keys in source code, or using config files checked into version control. Problem solved: same build artifact works in any environment.\n\nFactor 4 — Backing Services: Treat backing services (databases, message queues, caches, SMTP servers) as attached resources accessed via URL/locator stored in config. Violation: assuming the database is always on localhost, or tightly coupling to a specific service instance. Problem solved: swap backing services (e.g., local MySQL to Amazon RDS) without code changes.\n\n**Runtime Behavior:**\nFactor 6 — Processes: Execute the app as one or more stateless processes. Any persistent data must be stored in a stateful backing service (database, cache). Violation: storing session data in local memory, writing to local filesystem expecting it to persist, using sticky sessions. Problem solved: enables horizontal scaling — any process instance can handle any request.\n\nFactor 7 — Port Binding: Export services via port binding. The app is completely self-contained and does not rely on runtime injection of a webserver. Violation: deploying into an external application server (e.g., Tomcat WAR deployment). Problem solved: app is self-contained and can serve as a backing service for other apps.\n\nFactor 8 — Concurrency: Scale out via the process model. Different types of work are assigned to different process types (web processes, worker processes). Violation: trying to handle everything in a single monolithic process, relying on threading within a single large process for all scaling. Problem solved: enables horizontal scaling by adding more processes.\n\nFactor 9 — Disposability: Maximize robustness with fast startup and graceful shutdown. Processes can be started or stopped at a moment's notice. Violation: long startup times (minutes), not handling SIGTERM gracefully, not completing in-progress work before shutdown. Problem solved: enables rapid deployment, elastic scaling, and robust production operation.\n\n**Deployment and Operations:**\nFactor 5 — Build, Release, Run: Strictly separate build (compile code + dependencies), release (combine build with config), and run (execute in environment) stages. Every release has a unique ID and is immutable. Violation: making code changes directly on production servers, or modifying a release after it's been created. Problem solved: reproducible deployments with clear audit trail.\n\nFactor 10 — Dev/Prod Parity: Keep development, staging, and production as similar as possible. Minimize gaps in time (deploy quickly), personnel (developers involved in deployment), and tools (same backing services). Violation: using SQLite in development but PostgreSQL in production, or deploying only monthly. Problem solved: reduces 'works on my machine' failures.\n\nFactor 11 — Logs: Treat logs as event streams. The app should never concern itself with routing or storage of its output stream — just write to stdout. Violation: writing logs to local files, managing log rotation within the app. Problem solved: execution environment handles log aggregation, enabling centralized logging and analysis.\n\nFactor 12 — Admin Processes: Run admin/management tasks (database migrations, console sessions, one-time scripts) as one-off processes in identical environments. Violation: running migrations manually via SSH, using different code for admin tasks. Problem solved: admin tasks are reproducible, version-controlled, and run in the same environment as the app.",
    "difficulty": "hard",
    "topic_area": "The Twelve-Factor App"
  },
  {
    "id": "q039",
    "concept_ids": ["c-devops-001"],
    "scenario": "",
    "question_text": "Define DevOps and explain why it emerged as a movement. How does DevOps differ from simply using automation tools? Discuss the cultural, process, and tooling dimensions of DevOps, and explain how it relates to but does not replace the SDLC.",
    "model_answer": "DevOps is a combination of cultural philosophies, practices, and tools that integrates software development (Dev) and IT operations (Ops) to shorten the software development life cycle and deliver high-quality software continuously.\n\nWhy DevOps emerged:\nHistorically, development and operations teams worked in silos. Developers focused on writing new features quickly, while operations focused on system stability. This created a fundamental conflict: developers wanted change (new features), operations wanted stability (no changes). The result was slow release cycles, finger-pointing when things broke, and the infamous 'throw it over the wall' handoff where developers would write code and toss it to operations to figure out deployment.\n\nDevOps vs. just automation tools:\nDevOps is NOT simply using Jenkins, Docker, or Terraform. Many organizations adopt DevOps tools without changing their culture and see minimal improvement. True DevOps requires:\n\n1. Cultural dimension: Shared responsibility between dev and ops. 'You build it, you run it' mentality. Blameless post-mortems. Breaking down organizational silos. Everyone is responsible for the entire pipeline from code to production.\n\n2. Process dimension: Continuous Integration and Continuous Delivery (CI/CD). Infrastructure as Code. Automated testing at every stage. Monitoring and observability as first-class concerns. Fast feedback loops.\n\n3. Tooling dimension: Version control (Git), CI/CD pipelines (GitHub Actions, Jenkins), configuration management (Ansible, Terraform), containerization (Docker, Kubernetes), monitoring (Prometheus, Grafana). Tools enable the practices but are not DevOps themselves.\n\nRelationship to SDLC:\nDevOps does not replace the SDLC — it accelerates it. The same phases exist (requirements, design, implementation, testing, deployment, maintenance), but DevOps automates transitions between phases, creates faster feedback loops, and enables continuous iteration rather than waterfall-style sequential execution. The Twelve-Factor App methodology aligns with DevOps: Factor V (Build/Release/Run) maps to CI/CD pipelines, Factor X (Dev/Prod Parity) enables reliable deployments, Factor XI (Logs) supports observability.",
    "difficulty": "medium",
    "topic_area": "Introduction to DevOps"
  },
  {
    "id": "q040",
    "concept_ids": ["c-devops-002"],
    "scenario": "",
    "question_text": "Explain the Three Ways of DevOps in depth. For each Way, describe the underlying principle, how it manifests in practice, and what specific practices or tools support it. How do the Three Ways build upon each other to create a high-performing engineering organization?",
    "model_answer": "The Three Ways of DevOps, articulated by Gene Kim in 'The Phoenix Project' and 'The DevOps Handbook,' describe the principles underlying DevOps behaviors and patterns:\n\nFirst Way — Flow (Systems Thinking):\n- Principle: Optimize the performance of the entire system, not individual departments or silos. Focus on the flow of work from Development to Operations to the customer.\n- In practice: Visualize the entire value stream from code commit to production. Identify and eliminate bottlenecks. Reduce work-in-progress (WIP) limits. Make work visible (Kanban boards). Never pass known defects downstream.\n- Supporting practices/tools: CI/CD pipelines (automate the flow), small batch sizes (frequent small releases instead of large infrequent ones), limiting WIP, value stream mapping, automated testing (prevent defects from flowing downstream).\n- Key insight: A local optimization (e.g., developers writing code faster) is meaningless if the deployment pipeline is a bottleneck. Optimize the whole system.\n\nSecond Way — Feedback:\n- Principle: Create fast, constant feedback loops from right to left — from operations back to development. The goal is to detect and fix problems as early as possible.\n- In practice: Implement comprehensive monitoring and alerting. Create automated test suites that run on every commit. Enable developers to see production metrics. Conduct peer code reviews. Use A/B testing to validate features.\n- Supporting practices/tools: Application performance monitoring (APM), centralized logging (ELK stack), real-time alerting (PagerDuty), automated testing in CI pipelines, feature flags for gradual rollouts, blameless post-mortems after incidents.\n- Key insight: Without fast feedback, problems compound. A bug caught in unit testing costs minutes to fix; the same bug found in production costs hours or days.\n\nThird Way — Continuous Learning and Experimentation:\n- Principle: Foster a culture of continuous experimentation, learning from both successes and failures. Accept that failure is inevitable and use it as a learning opportunity.\n- In practice: Conduct blameless post-mortems (focus on systemic causes, not individual blame). Allocate time for experimentation and innovation. Practice chaos engineering (intentionally inject failures to test resilience). Share knowledge across teams.\n- Supporting practices/tools: Chaos engineering tools (Chaos Monkey), game days (simulated failure scenarios), internal tech talks and knowledge sharing, hackathons, documentation culture, learning from incidents.\n- Key insight: High-performing organizations have higher change rates AND lower failure rates because they learn and improve continuously.\n\nHow they build upon each other:\nThe First Way (Flow) establishes the pipeline from dev to production. The Second Way (Feedback) creates signals flowing back from production to development, enabling rapid detection and correction. The Third Way (Learning) uses insights from both flow and feedback to continuously improve the entire system. Together, they create a virtuous cycle: faster flow → faster feedback → faster learning → even faster flow.",
    "difficulty": "hard",
    "topic_area": "Introduction to DevOps"
  },
  {
    "id": "q041",
    "concept_ids": ["c-devops-003"],
    "scenario": "",
    "question_text": "Explain the CALMS framework for DevOps adoption. For each pillar (Culture, Automation, Lean, Measurement, Sharing), describe what it means and why it is essential. Additionally, explain the four DORA metrics and how they relate to the Measurement pillar.",
    "model_answer": "CALMS is a framework for assessing an organization's readiness and progress in adopting DevOps:\n\nC — Culture:\nDevOps starts with culture, not tools. This means shared responsibility between development and operations ('you build it, you run it'), blameless post-mortems that focus on systemic improvement rather than individual blame, trust and psychological safety to experiment and fail, and breaking down organizational silos. Without cultural change, DevOps tools become 'automation of dysfunction.' Essential because: tools without cultural change lead to the same problems with fancier technology.\n\nA — Automation:\nAutomate repetitive, error-prone manual tasks: CI/CD pipelines for build/test/deploy, Infrastructure as Code (Terraform, CloudFormation) for environment provisioning, automated testing at all levels (unit, integration, E2E), automated monitoring and alerting, automated incident response and remediation. Essential because: manual processes are slow, inconsistent, and don't scale. Automation enables speed and reliability simultaneously.\n\nL — Lean:\nApply Lean manufacturing principles to software delivery: eliminate waste (unnecessary handoffs, waiting, manual approvals), reduce batch sizes (small, frequent releases instead of large, infrequent ones), limit work-in-progress (WIP), optimize the value stream end-to-end, continuous improvement (kaizen). Essential because: large batches increase risk, delay feedback, and make it harder to identify the source of problems.\n\nM — Measurement:\nMeasure what matters to drive improvement. You cannot improve what you cannot measure. Key areas: deployment frequency, lead time for changes, mean time to recovery (MTTR), change failure rate, system availability, customer satisfaction. Essential because: without data, decisions are based on opinions. Measurement provides objective evidence of improvement or regression.\n\nS — Sharing:\nShare knowledge, tools, and practices across teams: internal documentation and wikis, cross-functional teams, shared tooling and platforms, open communication channels, learning from incidents (shared post-mortems), communities of practice. Essential because: knowledge silos create bottlenecks and single points of failure. Sharing accelerates learning across the organization.\n\nDORA Metrics (DevOps Research and Assessment):\nFour key metrics that measure software delivery performance, directly supporting the Measurement pillar:\n\n1. Deployment Frequency: How often code is deployed to production. Elite performers deploy on-demand (multiple times per day). Measures the flow of the First Way.\n\n2. Lead Time for Changes: Time from code commit to code running in production. Elite performers achieve less than one hour. Measures pipeline efficiency.\n\n3. Mean Time to Recovery (MTTR): How quickly a team can recover from a production failure. Elite performers recover in less than one hour. Measures feedback loop effectiveness (Second Way).\n\n4. Change Failure Rate: Percentage of deployments that cause a failure in production. Elite performers have 0-15%. Measures quality of the delivery process.\n\nThese four metrics are correlated: organizations that deploy more frequently also have lower failure rates and faster recovery times, disproving the myth that speed and stability are trade-offs.",
    "difficulty": "hard",
    "topic_area": "Introduction to DevOps"
  },
  {
    "id": "q042",
    "concept_ids": ["c-linux-001"],
    "scenario": "",
    "question_text": "Explain what the Linux kernel is and its role in the operating system. Describe the concept of kernel space (ring zero) versus user space, and explain why this separation exists. Why does Linux dominate cloud computing environments, running over 90% of public cloud workloads?",
    "model_answer": "The Linux Kernel:\nLinux technically refers to the kernel — the core component of the operating system that manages hardware resources and provides services to software. The kernel is the first program loaded after the bootloader (which is started by BIOS/UEFI) and remains in memory for the entire uptime of the system. It manages: CPU scheduling (which processes run when), memory management (allocation, virtual memory, paging), device drivers (hardware communication), file systems (data storage and retrieval), network stack (TCP/IP, sockets), and process management (creation, scheduling, termination).\n\nKernel Space (Ring Zero) vs User Space:\nModern CPUs implement protection rings — privilege levels that control what code can do:\n\n- Ring 0 (Kernel Space): The most privileged level. The kernel runs here with unrestricted access to all hardware, memory, and CPU instructions. It can execute any instruction, access any memory address, and directly control hardware devices. Only trusted kernel code runs at this level.\n\n- Ring 3 (User Space): The least privileged level. All user applications (web servers, databases, shells) run here. They cannot directly access hardware or other processes' memory. When a user-space program needs to perform a privileged operation (read a file, send a network packet), it must make a system call — a controlled request to the kernel.\n\nWhy this separation exists:\n1. Security: A buggy or malicious user application cannot crash the entire system or access other processes' data.\n2. Stability: If a user-space program crashes, only that program is affected. The kernel and other programs continue running.\n3. Isolation: Processes are isolated from each other, preventing interference.\n4. Resource management: The kernel arbitrates access to shared resources (CPU, memory, I/O), preventing any single process from monopolizing them.\n\nWhy Linux dominates cloud (90%+ of public cloud workloads):\n1. Open source / No licensing costs: At cloud scale (thousands of instances), Windows licensing costs become enormous. Linux is free.\n2. Stability: Linux servers routinely achieve uptimes of years without rebooting.\n3. Resource efficiency: Linux is lightweight — it can run effectively on minimal resources, maximizing the compute available for workloads.\n4. Strong security model: Robust permission system, rapid security patches from the open-source community, SELinux/AppArmor for mandatory access control.\n5. Flexibility: Highly configurable — strip down to minimal installations for containers, or build full desktop environments.\n6. Automation-friendly: Powerful CLI and scripting capabilities essential for DevOps and Infrastructure as Code.\n7. Container ecosystem: Docker, Kubernetes, and the entire container ecosystem are built on Linux kernel features (cgroups, namespaces).\n8. Community and ecosystem: Vast package repositories, extensive documentation, and a massive community of contributors.",
    "difficulty": "hard",
    "topic_area": "The Linux Command Line"
  },
  {
    "id": "q043",
    "concept_ids": ["c-linux-002"],
    "scenario": "",
    "question_text": "Describe the major components of a Linux operating system: bootloader, kernel, daemons, desktop environment, applications, and shell. Explain the role of each component and how they interact during system startup and normal operation. Why are cloud servers typically 'headless,' and what implications does this have?",
    "model_answer": "Major Linux OS Components:\n\n1. Bootloader:\n- Role: Manages the boot process. Loads the kernel into memory and starts it.\n- Examples: GRUB (GRand Unified Bootloader), systemd-boot.\n- Boot sequence: BIOS/UEFI firmware → Bootloader → Kernel.\n- The bootloader presents a menu (if configured) allowing selection of different kernels or boot options. It passes control to the kernel with specified parameters.\n\n2. Kernel:\n- Role: Core of the operating system. Manages all hardware resources and provides services to user-space programs.\n- Responsibilities: CPU scheduling, memory management, device drivers, file systems, networking, process management.\n- Runs in kernel space (ring 0) with full hardware access.\n- Once loaded by the bootloader, the kernel initializes hardware, mounts the root filesystem, and starts the init system (systemd).\n\n3. Init System / Daemons:\n- Role: The init system (systemd on modern Linux) is the first user-space process (PID 1). It starts and manages all other system services (daemons).\n- Daemons are background services that run without user interaction: sshd (SSH server), httpd/nginx (web server), mysqld (database), crond (scheduled tasks), networkd (networking).\n- Daemons are started by systemd based on unit files that define dependencies, startup order, and restart policies.\n\n4. Desktop Environment:\n- Role: Provides the graphical user interface (GUI) for user interaction.\n- Examples: GNOME, KDE, XFCE.\n- Components: Window manager, file manager, system tray, application launcher.\n- Runs on top of the X Window System (X11) or Wayland display server.\n\n5. Applications:\n- Role: User-installed software for specific tasks.\n- Installed via package managers: apt (Debian/Ubuntu), yum/dnf (RHEL/CentOS), pacman (Arch).\n- Range from command-line tools to full GUI applications.\n\n6. Shell:\n- Role: Command-line interface (CLI) that interprets user commands and arranges for their execution by the kernel.\n- Examples: bash (Bourne Again Shell — most common), zsh, fish, sh.\n- The login program validates user credentials, then starts the user's configured shell.\n- Shell is a Command Line Interpreter: reads commands, parses them, and executes them (either built-in commands or external programs).\n\nInteraction during startup:\nBIOS/UEFI → Bootloader loads kernel → Kernel initializes hardware → Kernel starts systemd (PID 1) → systemd starts daemons in dependency order → Login prompt (CLI) or Desktop Environment (GUI) → User interacts via Shell or GUI.\n\nHeadless cloud servers:\nCloud servers are typically headless — they have no desktop environment, no monitor, no GUI. Reasons:\n- Resource efficiency: GUI components consume CPU, RAM, and storage that should be used for workloads.\n- Security: Fewer installed packages means smaller attack surface.\n- Remote management: Cloud servers are managed remotely via SSH (command line) or APIs, not physical consoles.\n- Automation: All management is done through CLI commands and scripts, which are automatable. GUIs are not.\n\nImplications: Cloud engineers must be proficient with the Linux CLI. All server management — configuration, monitoring, troubleshooting, deployment — is done through the shell. This is why CLI skills are essential for cloud computing.",
    "difficulty": "medium",
    "topic_area": "The Linux Command Line"
  },
  {
    "id": "q044",
    "concept_ids": ["c-linux-003"],
    "scenario": "",
    "question_text": "Explain the role of the Linux shell as an interface between the user and the kernel. Describe the categories of essential CLI commands (file management, process management, text processing, permissions, system information) and provide key examples from each category. How does the shell interpret and execute commands?",
    "model_answer": "The Linux Shell as Interface:\nThe shell sits between the user and the kernel. When a user types a command, the shell interprets it, determines what needs to be done, and communicates with the kernel to execute it. The shell is a Command Line Interpreter (CLI) — it reads input, parses it into commands and arguments, and arranges for execution. The login process validates user credentials and then starts the user's configured shell (typically bash).\n\nHow the shell interprets and executes commands:\n1. Read: Shell displays a prompt and reads the command line input.\n2. Parse: Breaks the input into tokens (command, arguments, options, operators).\n3. Expand: Performs variable expansion ($HOME), glob expansion (*.txt), command substitution ($(date)).\n4. Execute: Determines if the command is a built-in (cd, echo, export) or external program. For external programs, the shell searches the PATH, forks a child process, and executes the program.\n5. Wait: Shell waits for the command to complete (unless run in background with &).\n6. Return: Displays output and the next prompt.\n\nEssential CLI Command Categories:\n\nFile Management:\n- ls: List directory contents (-l for long format, -a for hidden files)\n- cd: Change directory (cd /etc, cd .., cd ~)\n- pwd: Print working directory\n- mkdir: Create directories (-p for nested)\n- rm: Remove files/directories (-r for recursive, -f for force)\n- cp: Copy files/directories (-r for recursive)\n- mv: Move or rename files\n- cat: Display file contents\n- touch: Create empty file or update timestamp\n- find: Search for files (find /var -name '*.log')\n\nText Processing:\n- grep: Search text patterns in files (grep -r 'error' /var/log)\n- sed: Stream editor for text transformation\n- awk: Pattern scanning and processing\n- head/tail: View first/last lines of a file (tail -f for live following)\n- wc: Word, line, character count\n- sort: Sort lines of text\n- cut: Extract columns from text\n\nProcess Management:\n- ps: List running processes (ps aux for all processes)\n- top/htop: Real-time process monitoring\n- kill: Send signals to processes (kill -9 for force kill)\n- bg/fg: Background/foreground process control\n- nohup: Run command immune to hangups\n- systemctl: Manage systemd services\n\nPermissions:\n- chmod: Change file permissions (chmod 755 file, chmod u+x file)\n- chown: Change file ownership (chown user:group file)\n- chgrp: Change group ownership\n- sudo: Execute command as superuser\n\nSystem Information:\n- uname: System information (-a for all)\n- df: Disk space usage (-h for human-readable)\n- du: Directory space usage\n- free: Memory usage\n- uptime: System uptime and load averages\n- whoami: Current user\n- hostname: System hostname\n\nPipe and Redirection:\n- | (pipe): Send output of one command as input to another (ps aux | grep nginx)\n- > : Redirect output to file (overwrite)\n- >> : Redirect output to file (append)\n- < : Redirect input from file\n- 2> : Redirect error output\n\nThese commands form the foundation of Linux system administration and are essential for cloud engineering, where all server management is done via CLI.",
    "difficulty": "medium",
    "topic_area": "The Linux Command Line"
  },
  {
    "id": "q045",
    "concept_ids": ["c-linux-004"],
    "scenario": "",
    "question_text": "Explain the Linux file system hierarchy in detail, describing the purpose of key directories (/, /home, /etc, /var, /tmp, /usr, /opt, /proc). Then explain the Linux permission model: how permissions are represented (symbolic and numeric notation), how chmod and chown work, and why the permission model is critical for security in multi-user and cloud environments.",
    "model_answer": "Linux File System Hierarchy:\nLinux uses a single hierarchical tree structure starting from the root directory (/). Everything is a file (including devices and processes). Key directories:\n\n- / (root): The top of the file system tree. All other directories branch from here.\n- /home: User home directories (/home/username). Each user has their own directory for personal files.\n- /etc: System-wide configuration files. Examples: /etc/ssh/sshd_config (SSH configuration), /etc/passwd (user accounts), /etc/hosts (hostname resolution), /etc/systemd/ (systemd unit files).\n- /var: Variable data — files that change during operation. /var/log (system logs), /var/www (web server files), /var/lib (application state data).\n- /tmp: Temporary files. Cleared on reboot. Any user can write here (with sticky bit protection).\n- /usr: User system resources — installed software, libraries, documentation. /usr/bin (user commands), /usr/lib (libraries), /usr/share (shared data).\n- /opt: Optional/third-party software installed outside the package manager.\n- /proc: Virtual filesystem providing process and kernel information. /proc/cpuinfo (CPU details), /proc/meminfo (memory), /proc/[PID]/ (per-process info). Not actual files on disk — generated by the kernel.\n- /dev: Device files representing hardware (disks, terminals, etc.).\n- /boot: Boot loader files, kernel images.\n\nLinux Permission Model:\nEvery file and directory has three permission categories and three permission types:\n\nCategories: Owner (u), Group (g), Others (o)\nPermission types: Read (r), Write (w), Execute (x)\n\nSymbolic notation: rwxr-xr-- means:\n- Owner: rwx (read, write, execute)\n- Group: r-x (read, execute)\n- Others: r-- (read only)\n\nNumeric (octal) notation: Each permission has a numeric value:\n- Read = 4, Write = 2, Execute = 1\n- Sum for each category: rwx = 4+2+1 = 7, r-x = 4+0+1 = 5, r-- = 4+0+0 = 4\n- So rwxr-xr-- = 754\n\nCommon permission sets:\n- 755 (rwxr-xr-x): Executable files, directories — owner full access, others can read/execute\n- 644 (rw-r--r--): Regular files — owner read/write, others read only\n- 700 (rwx------): Private directories — owner only\n- 600 (rw-------): Private files (SSH keys) — owner only\n\nchmod (change mode): Modify permissions\n- Numeric: chmod 755 file\n- Symbolic: chmod u+x file (add execute for owner), chmod go-w file (remove write for group and others)\n\nchown (change owner): Modify ownership\n- chown user:group file — change both owner and group\n- chown user file — change owner only\n- chown :group file — change group only\n- chown -R user:group directory — recursive ownership change\n\nWhy permissions are critical for security:\n1. Multi-user isolation: Prevents users from reading/modifying each other's files.\n2. Principle of least privilege: Services run as non-root users with minimal permissions (e.g., nginx runs as www-data).\n3. System protection: Critical system files (/etc/shadow, /etc/sudoers) are restricted to root.\n4. Cloud security: SSH private keys must be 600 (owner-only read/write) — SSH refuses to use keys with broader permissions. Web server files should not be writable by the web server process to prevent defacement.\n5. Defense in depth: Even if an attacker compromises one service, file permissions limit what they can access.",
    "difficulty": "hard",
    "topic_area": "The Linux Command Line"
  },
  {
    "id": "q046",
    "concept_ids": ["c-regions-001"],
    "scenario": "",
    "question_text": "Explain what cloud regions are and why cloud providers organize their infrastructure into regions. Discuss the concept of data sovereignty and how regulations like GDPR influence region selection. Why don't resources automatically replicate across regions, and what are the implications of this for application architecture?",
    "model_answer": "Cloud Regions Defined:\nA cloud region is a specific geographical location where a cloud provider operates a cluster of data centers. Each region is a collection of availability zones and operates independently from other regions. Examples include US East (N. Virginia), EU West (Ireland), and Asia Pacific (Tokyo).\n\nWhy providers organize into regions:\n1. Latency reduction: Placing resources close to users minimizes network round-trip time. A user in Tokyo experiences much lower latency connecting to the Asia Pacific region than to US East.\n2. Data sovereignty compliance: Laws and regulations require data to remain within specific geographic boundaries.\n3. Fault isolation: Regions are completely independent — a failure in one region does not affect others.\n4. Disaster recovery: Multiple regions enable geographic redundancy for business continuity.\n5. Market presence: Regions in different countries serve local markets and meet local compliance requirements.\n\nData Sovereignty and GDPR:\nData sovereignty refers to the concept that data is subject to the laws of the country where it is stored. GDPR (General Data Protection Regulation) in the EU requires that personal data of EU citizens be processed and stored within the EU (or in countries with adequate data protection laws). This means:\n- An application serving EU customers must store their personal data in EU regions (e.g., eu-west-1 Ireland, eu-central-1 Frankfurt).\n- Transferring data to non-EU regions requires specific legal mechanisms (Standard Contractual Clauses, adequacy decisions).\n- Similar regulations exist globally: China's data localization laws, Russia's data residency requirements, Brazil's LGPD.\n- Region selection is not just a technical decision — it's a legal and compliance requirement.\n\nWhy resources don't auto-replicate across regions:\nRegions are designed as independent, isolated units. Resources created in one region (EC2 instances, S3 buckets, RDS databases) exist only in that region by default. This is intentional:\n1. Cost control: Cross-region replication incurs data transfer costs and additional resource costs.\n2. Data sovereignty: Automatic replication could violate data residency laws by copying data to unauthorized jurisdictions.\n3. Consistency: Cross-region replication introduces latency and consistency challenges (CAP theorem).\n4. Explicit architecture: Forces architects to make deliberate decisions about data placement and redundancy.\n\nImplications for application architecture:\n- Multi-region deployment requires explicit configuration: cross-region replication for databases, CDN for static content, DNS-based routing for traffic distribution.\n- Architects must design for eventual consistency when data spans regions.\n- Backup and disaster recovery strategies must explicitly include cross-region replication.\n- Each region may have different service availability and pricing, requiring architecture adaptation.",
    "difficulty": "hard",
    "topic_area": "Cloud Regions & Availability Zones"
  },
  {
    "id": "q047",
    "concept_ids": ["c-regions-002"],
    "scenario": "",
    "question_text": "Explain what Availability Zones (AZs) are and why the distinction between an AZ and a data center is important. Describe the concept of 'failure domains' and how AZs provide fault isolation within a region. How are AZs connected to each other, and what guarantees does this connectivity provide?",
    "model_answer": "Availability Zones Defined:\nAn Availability Zone (AZ) is a deployment area within a cloud region. Each AZ represents one or more discrete data centers with redundant power, networking, and connectivity. AZs within a region are physically separated (typically by meaningful distance — several kilometers) but connected through low-latency, high-bandwidth private network links.\n\nAZ ≠ Data Center — Why This Distinction Matters:\nA common misconception is that one AZ equals one data center. In reality:\n- An AZ is a logical isolation boundary, not necessarily a single physical building.\n- One AZ may span multiple data centers, or one large data center complex may host multiple AZs.\n- Cloud providers deliberately abstract the physical infrastructure — customers work with logical AZs, not physical locations.\n- AZ identifiers (e.g., us-east-1a) may map to different physical locations for different AWS accounts, preventing all customers from concentrating in the same physical zone.\n\nThis abstraction matters because it allows providers to evolve their physical infrastructure without affecting customers' logical architecture.\n\nFailure Domains:\nA failure domain is a boundary within which a failure is contained. AZs are designed as independent failure domains:\n- Each AZ has independent power supplies (separate utility feeds, backup generators).\n- Each AZ has independent cooling systems.\n- Each AZ has independent networking infrastructure.\n- Physical separation protects against localized disasters (fire, flooding, power grid failure).\n\nThe key principle: a failure in one AZ should NOT cascade to other AZs. If AZ-A loses power, AZ-B and AZ-C continue operating normally. This is the foundation of high availability in cloud architecture.\n\nConnectivity Between AZs:\nAZs within a region are connected by dedicated, high-bandwidth, low-latency private fiber links:\n- Latency between AZs is typically single-digit milliseconds (often < 2ms).\n- This low latency enables synchronous replication of databases across AZs.\n- The connection is private (not over the public internet), providing consistent performance and security.\n- Bandwidth is sufficient for real-time data replication and distributed application communication.\n\nThis connectivity enables architectures where:\n- Application servers in AZ-A can communicate with databases in AZ-B with minimal latency.\n- Load balancers can distribute traffic across instances in multiple AZs.\n- Synchronous database replication across AZs provides data durability without significant performance impact.\n\nThe combination of fault isolation (independent failure domains) and low-latency connectivity (enabling cross-AZ communication) is what makes multi-AZ deployment the standard pattern for high availability in cloud computing.",
    "difficulty": "hard",
    "topic_area": "Cloud Regions & Availability Zones"
  },
  {
    "id": "q048",
    "concept_ids": ["c-regions-003"],
    "scenario": "",
    "question_text": "Describe the key factors that should guide the selection of cloud regions and availability zones for a production deployment. Explain how compliance requirements, performance optimization, disaster recovery planning, and cost management each influence this decision. What is a multi-AZ deployment strategy and why is it considered a best practice?",
    "model_answer": "Region and AZ selection is one of the most important architectural decisions in cloud computing. Four key factors guide this decision:\n\n1. Compliance Requirements (Data Residency):\n- Legal regulations may mandate where data can be stored and processed. GDPR requires EU citizen data to stay in the EU. Healthcare regulations (HIPAA) may require data within specific jurisdictions.\n- Some industries (government, financial services) have strict data residency requirements.\n- Selection approach: Identify all applicable regulations first — compliance is non-negotiable and overrides other factors. Choose regions that satisfy all legal requirements.\n\n2. Performance Optimization (Latency Reduction):\n- Network latency is primarily determined by physical distance between users and servers.\n- Every 1000 km adds approximately 5-10ms of latency.\n- For interactive applications, latency directly impacts user experience (Google found 500ms delay reduces traffic by 20%).\n- Selection approach: Identify where your users are geographically. Deploy in regions closest to the largest user concentrations. Use latency-based DNS routing to direct users to the nearest region. Consider CDNs for static content to reduce latency globally.\n\n3. Disaster Recovery Planning:\n- Single-region deployment is vulnerable to region-wide failures (rare but possible — natural disasters, widespread power outages).\n- Recovery Time Objective (RTO): How quickly must the system recover?\n- Recovery Point Objective (RPO): How much data loss is acceptable?\n- Selection approach: For critical applications, deploy across multiple regions. Choose a secondary region that is geographically distant from the primary (different natural disaster risk profile). Implement cross-region replication for data. Test failover procedures regularly.\n\n4. Cost Management:\n- Pricing varies between regions (US regions are typically cheapest, Asia Pacific regions often more expensive).\n- Data transfer between regions incurs costs.\n- Some regions offer specific instance types or services not available elsewhere.\n- Selection approach: Compare pricing across regions that meet compliance and performance requirements. Factor in data transfer costs for multi-region architectures. Consider reserved instances or committed use discounts in primary regions.\n\nMulti-AZ Deployment Strategy:\nA multi-AZ deployment distributes application components across two or more Availability Zones within a region:\n\n- Application tier: Run instances in at least 2 AZs behind a load balancer. If one AZ fails, the load balancer routes traffic to healthy instances in other AZs.\n- Database tier: Use multi-AZ database deployments (e.g., RDS Multi-AZ) with synchronous replication. Primary in one AZ, standby in another. Automatic failover if primary AZ fails.\n- Storage: Use services that automatically replicate across AZs (e.g., S3 stores data across minimum 3 AZs).\n\nWhy it's a best practice:\n- Protects against AZ-level failures (hardware failure, power outage, network issues).\n- Low-latency cross-AZ connectivity means minimal performance impact.\n- Relatively low additional cost compared to the availability improvement.\n- Cloud providers' managed services (RDS, ELB) have built-in multi-AZ support.\n- It's the minimum recommended architecture for any production workload.",
    "difficulty": "hard",
    "topic_area": "Cloud Regions & Availability Zones"
  },
  {
    "id": "q049",
    "concept_ids": ["c-testing-001"],
    "scenario": "",
    "question_text": "Explain the testing pyramid concept in detail. Why is it shaped like a pyramid — what does the width at each level represent? Discuss the trade-offs between the three levels (unit, integration, E2E) in terms of speed, cost, reliability, and maintenance. How does the testing pyramid optimize for fast feedback in CI/CD pipelines?",
    "model_answer": "The Testing Pyramid:\nThe testing pyramid is a visual metaphor for organizing automated tests, originally described by Mike Cohn. It has three levels:\n\nBase (widest) — Unit Tests:\n- The largest number of tests. Should comprise 70-80% of all tests.\n- Test individual functions, methods, or classes in complete isolation.\n- Execute in milliseconds. A suite of thousands can run in seconds.\n- No external dependencies — databases, networks, file systems are replaced with test doubles (mocks, stubs, fakes).\n- Highly reliable and deterministic — same input always produces same output.\n\nMiddle — Integration Tests:\n- Moderate number of tests. Should comprise 15-20% of all tests.\n- Test interactions between components: service-to-database, service-to-service, API contracts.\n- Execute in seconds to minutes. Require real (or containerized) external dependencies.\n- Less deterministic — can be affected by network issues, database state, timing.\n\nTop (narrowest) — End-to-End (E2E) Tests:\n- The fewest tests. Should comprise 5-10% of all tests.\n- Test complete user workflows through the entire system.\n- Execute in minutes to hours. Require full system deployment.\n- Most brittle — many moving parts mean many potential failure points unrelated to actual bugs.\n\nWhat the width represents:\nThe width at each level represents the relative number of tests. More unit tests (wide base), fewer integration tests (middle), fewest E2E tests (narrow top). This ratio is intentional and optimizes for several factors.\n\nTrade-offs by level:\n\nSpeed:\n- Unit: Milliseconds per test. Entire suite in seconds.\n- Integration: Seconds per test. Suite in minutes.\n- E2E: Minutes per test. Suite in tens of minutes to hours.\n\nCost (to write and maintain):\n- Unit: Cheapest. Simple setup, no infrastructure needed.\n- Integration: Moderate. Requires test databases, mock services, or containers.\n- E2E: Most expensive. Requires full environment, complex setup, frequent maintenance as UI changes.\n\nReliability:\n- Unit: Most reliable. Deterministic, no external factors.\n- Integration: Moderately reliable. External dependencies can cause flaky failures.\n- E2E: Least reliable. Many components mean many potential non-bug failures (flaky tests).\n\nBug localization:\n- Unit: Pinpoints exact function/line with the bug.\n- Integration: Narrows to interaction between specific components.\n- E2E: Tells you something is broken but not where.\n\nOptimizing for CI/CD fast feedback:\nIn a CI/CD pipeline, fast feedback is critical — developers need to know quickly if their change broke something.\n\n1. Unit tests run first (seconds): Catch logic errors immediately. If they fail, the developer knows within seconds.\n2. Integration tests run next (minutes): Catch interaction bugs. Developer waits minutes, not hours.\n3. E2E tests run last (if at all in CI): Catch system-level issues. May run on a schedule rather than every commit.\n\nThe pyramid shape ensures that most bugs are caught at the fastest, cheapest level. Only bugs that unit tests cannot detect (interaction issues, system-level problems) need the slower, more expensive upper levels. An inverted pyramid (many E2E tests, few unit tests) — sometimes called the 'ice cream cone anti-pattern' — results in slow, flaky, expensive test suites that provide slow feedback and discourage frequent testing.",
    "difficulty": "hard",
    "topic_area": "Unit, Integration, Performance, and Load Testing"
  },
  {
    "id": "q050",
    "concept_ids": ["c-testing-002"],
    "scenario": "",
    "question_text": "Explain unit testing in depth: what constitutes a 'unit,' what isolation means in the context of unit testing, and why determinism is essential. Describe the role of test doubles (mocks, stubs, and fakes) and explain when and why each type is used. What makes a good unit test versus a bad one?",
    "model_answer": "What is a Unit?\nA unit is the smallest testable piece of code — typically a single function, method, or class. The exact boundary depends on context: in functional programming, a unit is usually a pure function; in object-oriented programming, it might be a method or an entire class. The key criterion is that the unit has a clear, well-defined responsibility that can be tested independently.\n\nIsolation in Unit Testing:\nIsolation means the unit under test runs without any real external dependencies. This includes:\n- No database connections\n- No network calls (HTTP, gRPC)\n- No file system access\n- No calls to other services\n- No dependency on system time, random numbers, or environment state\n\nAll external dependencies are replaced with test doubles. This ensures the test verifies only the logic of the unit itself, not the behavior of its dependencies.\n\nWhy Determinism is Essential:\nA deterministic test produces the same result every time it runs with the same input, regardless of when, where, or how many times it runs. Determinism is essential because:\n- Flaky tests (sometimes pass, sometimes fail) destroy developer trust in the test suite.\n- Non-deterministic tests make it impossible to know if a failure indicates a real bug or a test environment issue.\n- CI/CD pipelines rely on test results to gate deployments — flaky tests either block good code or allow bad code through.\n- Sources of non-determinism to avoid: system time, random values, network calls, shared mutable state, test execution order dependencies.\n\nTest Doubles:\nTest doubles replace real dependencies to achieve isolation:\n\n1. Stubs: Provide predetermined responses to method calls. They don't verify how they were called — they just return canned data.\n- Use when: You need a dependency to return specific data for your test. Example: A stub repository that always returns a specific user object, regardless of the query.\n\n2. Mocks: Like stubs but also verify that specific interactions occurred (method was called with expected arguments, called the right number of times).\n- Use when: You need to verify the unit interacts correctly with its dependencies. Example: Verify that a service calls the email sender exactly once with the correct recipient.\n\n3. Fakes: Simplified but working implementations of a dependency. They have real behavior but take shortcuts.\n- Use when: You need realistic behavior without the overhead of the real dependency. Example: An in-memory database implementation instead of a real PostgreSQL connection.\n\nCharacteristics of Good Unit Tests (often summarized as FIRST):\n- Fast: Execute in milliseconds. A slow unit test is a sign of poor isolation.\n- Isolated: Test one thing. Failure points to exactly one cause.\n- Repeatable: Same result every time, in any environment.\n- Self-validating: Pass or fail automatically — no manual inspection of output.\n- Timely: Written close to the code they test (ideally before, in TDD).\n\nAdditional qualities:\n- Test behavior, not implementation: Test what the function does, not how it does it. This prevents tests from breaking when you refactor.\n- Clear naming: Test name describes the scenario and expected outcome.\n- Arrange-Act-Assert pattern: Clear structure — set up data, execute the action, verify the result.\n\nCharacteristics of Bad Unit Tests:\n- Testing implementation details (breaks on refactoring).\n- Multiple assertions testing unrelated things.\n- Depending on test execution order.\n- Requiring complex setup (sign of poor code design).\n- Testing trivial code (getters/setters) with no logic.",
    "difficulty": "hard",
    "topic_area": "Unit, Integration, Performance, and Load Testing"
  },
  {
    "id": "q051",
    "concept_ids": ["c-testing-003"],
    "scenario": "",
    "question_text": "Compare and contrast integration tests and end-to-end (E2E) tests. For each type, explain what is being tested, what kinds of bugs they catch that unit tests cannot, the typical setup and infrastructure required, and the challenges of maintaining them. Why are both necessary despite their higher cost compared to unit tests?",
    "model_answer": "Integration Tests:\n\nWhat is tested: The interactions and interfaces between two or more components. Integration tests verify that components work correctly together — that data flows properly across boundaries.\n\nExamples of what they test:\n- Service layer correctly queries the database and maps results to domain objects.\n- REST API endpoint correctly serializes/deserializes JSON and returns proper HTTP status codes.\n- Message queue producer and consumer correctly exchange messages.\n- Service A correctly calls Service B's API and handles responses.\n\nBugs they catch that unit tests cannot:\n- SQL queries that are syntactically correct but semantically wrong (wrong JOIN, missing WHERE clause).\n- Serialization/deserialization mismatches (field name differences between services).\n- API contract violations (Service A expects a field that Service B doesn't send).\n- Database schema mismatches (code expects a column that doesn't exist).\n- Transaction boundary issues (data not committed when expected).\n\nSetup required:\n- Real or containerized databases (Docker containers with PostgreSQL, MySQL).\n- Mock or stub external services (WireMock, MockServer).\n- Test data setup and teardown scripts.\n- Moderate CI/CD infrastructure (containers, test databases).\n\nChallenges:\n- Slower than unit tests (seconds per test due to database/network I/O).\n- Test data management — ensuring consistent starting state.\n- Potential for flaky tests due to timing, network issues.\n- More complex setup and teardown.\n\nEnd-to-End (E2E) Tests:\n\nWhat is tested: Complete user workflows through the entire system, from the user interface (or API entry point) through all services, databases, and external integrations to the final output.\n\nExamples of what they test:\n- User can register, log in, add items to cart, and complete checkout.\n- Admin can create a report that queries multiple services and generates a PDF.\n- A webhook from a payment provider triggers order fulfillment through the entire pipeline.\n\nBugs they catch that unit and integration tests cannot:\n- Cross-service workflow failures (Service A → B → C pipeline breaks at B→C).\n- UI rendering issues that affect functionality.\n- Performance bottlenecks only visible under realistic conditions.\n- Configuration mismatches between services in a deployed environment.\n- Authentication/authorization flows across the full stack.\n\nSetup required:\n- Full system deployment (all services, databases, message queues, caches).\n- Browser automation tools for UI testing (Selenium, Cypress, Playwright).\n- Test environment that mirrors production.\n- Significant infrastructure and maintenance effort.\n\nChallenges:\n- Slowest tests (minutes per test, hours for full suite).\n- Most brittle — any change in any component can break tests.\n- Difficult to debug failures (which component caused the failure?).\n- Expensive to maintain (UI changes require test updates).\n- Flaky tests are common due to timing, network, and state issues.\n\nWhy both are necessary despite higher cost:\n1. Defense in depth: Each test level catches different categories of bugs. Unit tests catch logic errors, integration tests catch interface errors, E2E tests catch workflow errors.\n2. Confidence gradient: Unit tests give confidence in individual components, integration tests in component interactions, E2E tests in the complete system.\n3. Real-world validation: Only E2E tests verify that the system works as a user would experience it.\n4. Cost of production bugs: The cost of finding and fixing a bug in production far exceeds the cost of maintaining integration and E2E tests.\n5. Regression prevention: Integration and E2E tests catch regressions that occur when changes in one component break another component's expectations.\n\nThe testing pyramid's guidance is not to eliminate upper-level tests but to have the right ratio: many unit tests for fast feedback, moderate integration tests for interface verification, and few targeted E2E tests for critical workflow validation.",
    "difficulty": "hard",
    "topic_area": "Unit, Integration, Performance, and Load Testing"
  },
  {
    "id": "q052",
    "concept_ids": ["c-ci-001"],
    "scenario": "",
    "question_text": "Define Continuous Integration (CI) and explain its three foundational pillars. Why is frequent integration important, and what problems does CI solve compared to traditional integration approaches where developers work in isolation for weeks before merging? How does CI relate to the broader DevOps practice?",
    "model_answer": "Continuous Integration (CI) Defined:\nCI is a software development practice where developers frequently integrate their code changes into a shared repository — ideally multiple times per day. Each integration is verified by an automated build and automated tests to detect errors as quickly as possible.\n\nThe Three Foundational Pillars:\n\n1. Frequent Commits:\n- Developers integrate their work at least daily, ideally multiple times per day.\n- Each commit should be small and incremental — a few hours of work at most.\n- Small commits are easier to review, easier to test, and easier to debug if they cause problems.\n- The longer code stays unintegrated, the higher the risk of merge conflicts and integration failures.\n\n2. Automated Build:\n- Every commit to the shared repository triggers an automated build process.\n- The build compiles code, resolves dependencies, and produces deployable artifacts.\n- No manual steps — the entire build process is scripted and reproducible.\n- If the build fails, the team is notified immediately.\n\n3. Automated Testing:\n- Automated tests run as part of every build — unit tests at minimum, ideally integration tests as well.\n- Tests verify that the new code doesn't break existing functionality (regression testing).\n- Test results provide immediate feedback to the developer who made the change.\n- A failing test is treated as a build failure that must be fixed immediately.\n\nProblems CI Solves (vs. Traditional Integration):\nIn traditional development, developers work in isolation on feature branches for weeks or months, then attempt to merge everything together ('integration hell'):\n\n1. Merge conflicts: Long-lived branches diverge significantly, making merging painful and error-prone. CI's frequent small merges minimize conflicts.\n2. Late bug discovery: Bugs from incompatible changes are only found during the integration phase, weeks after the code was written. CI catches them within minutes of the commit.\n3. 'Works on my machine': Developers test only on their local environment. CI builds and tests in a consistent, shared environment.\n4. Integration risk: The longer integration is delayed, the riskier it becomes. CI makes integration a non-event by doing it continuously.\n5. Slow feedback: Developers don't know if their code works with others' changes until integration. CI provides feedback within minutes.\n\nRelationship to DevOps:\nCI is a core DevOps practice that directly supports the Three Ways:\n- First Way (Flow): CI automates the flow from code commit to verified build, removing manual bottlenecks.\n- Second Way (Feedback): Automated tests provide fast feedback to developers about the quality of their changes.\n- Third Way (Learning): CI metrics (build success rate, test coverage, build time) provide data for continuous improvement.\n\nCI is also the foundation for Continuous Delivery (CD) — you cannot reliably deliver software continuously if you cannot first integrate it continuously. CI connects to the Twelve-Factor App: Factor 1 (single codebase), Factor 5 (build/release/run separation), and Factor 10 (dev/prod parity).",
    "difficulty": "medium",
    "topic_area": "Continuous Integration with GitHub Actions"
  },
  {
    "id": "q053",
    "concept_ids": ["c-ci-002"],
    "scenario": "",
    "question_text": "Describe the core practices of Continuous Integration beyond the three pillars. Explain why maintaining a single source repository, making the build self-testing, building on every commit to mainline, fixing broken builds immediately, and keeping the build fast are each essential. What happens when these practices are not followed?",
    "model_answer": "Core CI Practices in Detail:\n\n1. Maintain a Single Source Repository:\n- All source code, configuration, test scripts, build scripts, and database schemas live in one version control repository (or a well-organized monorepo).\n- Everyone commits to and pulls from the same repository.\n- Why essential: Ensures everyone is working with the same codebase. Eliminates 'which version is the real one?' confusion. Makes it possible to build the entire system from a single checkout.\n- Without it: Teams work on different copies, versions diverge, integration becomes a nightmare. Code that works in one copy may not work in another.\n\n2. Automate the Build:\n- The entire build process — compilation, dependency resolution, artifact creation — must be executable with a single command.\n- No manual steps, no 'click here then there' instructions.\n- Why essential: Reproducibility. Any developer or CI server can build the project identically. Eliminates 'it builds on my machine' problems.\n- Without it: Builds depend on individual developer's environment setup. New team members spend days getting the build working. CI cannot function.\n\n3. Make the Build Self-Testing:\n- Automated tests are an integral part of the build process, not a separate step.\n- A build that compiles but has failing tests is a failed build.\n- Why essential: Compilation only proves the code is syntactically correct, not that it works. Self-testing builds verify functional correctness automatically.\n- Without it: Code compiles but contains bugs that are only discovered later (in QA, staging, or production). The build gives false confidence.\n\n4. Every Commit Builds on Mainline:\n- Every commit to the main branch triggers a full build and test cycle.\n- Developers should integrate with mainline frequently (at least daily).\n- Why essential: Ensures that the mainline is always in a known state. Problems are detected immediately when they're introduced, not days later.\n- Without it: Commits accumulate without verification. When a build finally runs, multiple commits may have introduced problems, making it difficult to identify which commit caused the failure.\n\n5. Fix Broken Builds Immediately:\n- A broken build is the highest priority issue for the team.\n- The developer who broke the build is responsible for fixing it immediately (within minutes, not hours).\n- If it can't be fixed quickly, the breaking commit should be reverted.\n- Why essential: A broken build blocks the entire team. No one can verify their changes against a broken build. The longer a build stays broken, the more changes pile up on top of the broken state.\n- Without it: Broken builds become normalized. Developers stop trusting the build. The CI pipeline becomes meaningless — 'it's always red anyway.'\n\n6. Keep the Build Fast:\n- The build (including tests) should complete in minutes, not hours. Target: under 10 minutes.\n- Use techniques: parallel test execution, test categorization (fast unit tests first, slow integration tests later), incremental builds, caching.\n- Why essential: Fast builds enable fast feedback. If the build takes an hour, developers context-switch to other work and lose the mental model of what they changed. Fast builds encourage frequent commits.\n- Without it: Developers avoid committing frequently because they don't want to wait. They batch changes into large commits, defeating the purpose of CI. Feedback is delayed, bugs are harder to trace.\n\nAll these practices reinforce each other. A single repository with automated, self-testing, fast builds that run on every commit and are fixed immediately creates a virtuous cycle of quality and speed.",
    "difficulty": "hard",
    "topic_area": "Continuous Integration with GitHub Actions"
  },
  {
    "id": "q054",
    "concept_ids": ["c-ci-003"],
    "scenario": "",
    "question_text": "Describe the stages of a typical CI pipeline from code commit to deployment artifact. For each stage, explain what happens, what can go wrong, and what feedback is provided to the developer. How does the CI pipeline connect to the Twelve-Factor App's Factor V (Build, Release, Run)?",
    "model_answer": "CI Pipeline Stages:\n\n1. Code Commit (Trigger):\n- What happens: Developer pushes code to the shared repository (e.g., git push to main or a pull request). This event triggers the CI pipeline.\n- What can go wrong: Merge conflicts if multiple developers modified the same files. Large commits that are difficult to review.\n- Feedback: Immediate — developer sees if the push succeeded and the pipeline was triggered.\n\n2. Source Checkout:\n- What happens: The CI server checks out the latest code from the repository, including the new commit.\n- What can go wrong: Repository access issues, submodule failures, large repository clone times.\n- Feedback: Pipeline status shows 'in progress' or fails at this stage.\n\n3. Dependency Resolution:\n- What happens: Install all declared dependencies (npm install, pip install -r requirements.txt, mvn dependency:resolve). Dependencies are typically cached between builds for speed.\n- What can go wrong: Dependency conflicts, unavailable packages (registry down), version incompatibilities, security vulnerabilities in dependencies.\n- Feedback: Build fails with dependency error messages. Some pipelines include dependency vulnerability scanning.\n\n4. Compilation / Build:\n- What happens: Source code is compiled (for compiled languages) or bundled (for interpreted languages). Linting and static analysis may run here.\n- What can go wrong: Syntax errors, type errors, linting violations, static analysis warnings.\n- Feedback: Specific error messages with file and line numbers. Developer knows exactly what to fix.\n\n5. Automated Testing:\n- What happens: Test suites execute in order — unit tests first (fast feedback), then integration tests, then any other automated tests.\n- What can go wrong: Test failures (actual bugs), flaky tests (intermittent failures), slow tests exceeding timeout.\n- Feedback: Test results with pass/fail counts, specific failure messages, code coverage reports. Developer sees which tests failed and why.\n\n6. Artifact Creation:\n- What happens: If all previous stages pass, the pipeline produces deployable artifacts — compiled binaries, Docker images, JAR files, ZIP packages. Artifacts are versioned (often with the commit hash or build number).\n- What can go wrong: Artifact creation failures, storage issues, incorrect packaging.\n- Feedback: Artifact is published to an artifact repository (Docker Hub, Nexus, S3) with version information.\n\n7. Reporting and Notification:\n- What happens: Pipeline results are reported — build status (pass/fail), test results, code coverage, artifact location. Notifications sent via email, Slack, or GitHub status checks.\n- What can go wrong: Notification system failures (non-critical).\n- Feedback: Comprehensive build report. Pull request status checks show green (pass) or red (fail).\n\nConnection to Twelve-Factor App Factor V (Build, Release, Run):\nFactor V requires strict separation of three stages:\n\n- Build stage: Convert code into an executable bundle. This maps directly to the CI pipeline stages 2-6 above. The build stage takes the code repo as input and produces a build artifact as output. The build is triggered by the developer.\n\n- Release stage: Combine the build artifact with environment-specific configuration (environment variables, config files). Each release has a unique identifier (timestamp or version number). Releases are immutable — you never modify a release, you create a new one.\n\n- Run stage: Execute the application in the target environment using the release. This is the deployment.\n\nThe CI pipeline implements the Build stage. The deployment pipeline (CD) implements the Release and Run stages. By keeping these stages strictly separate, you ensure that the same build artifact can be deployed to any environment (dev, staging, production) simply by combining it with different configuration — a core principle of both Twelve-Factor and CI/CD.\n\nCommon CI tools: Jenkins, GitHub Actions, GitLab CI, CircleCI, Travis CI, Azure DevOps Pipelines.",
    "difficulty": "hard",
    "topic_area": "Continuous Integration with GitHub Actions"
  },
  {
    "id": "q055",
    "concept_ids": ["c-git-001"],
    "scenario": "",
    "question_text": "Explain what version control is and why it is essential for software development. Compare the three types of version control systems (local, centralized, distributed) and explain why Git, as a distributed VCS, became the dominant standard. What advantages does Git's distributed model provide over centralized systems like SVN?",
    "model_answer": "Version Control Defined:\nVersion control is a system that records changes to files over time so that you can recall specific versions later. It maintains a complete history of every modification — who changed what, when, and why. This is essential for software development because:\n- Collaboration: Multiple developers can work on the same codebase simultaneously.\n- History: Every change is recorded with context (commit messages), creating an audit trail.\n- Recovery: Any previous version can be restored if a change introduces problems.\n- Branching: Developers can work on features in isolation without affecting the main codebase.\n- Accountability: Every change is attributed to a specific developer.\n\nThree Types of Version Control Systems:\n\n1. Local VCS:\n- Changes tracked in a local database on the developer's machine.\n- Example: RCS (Revision Control System).\n- Limitation: No collaboration — only one developer can use it. If the local disk fails, all history is lost.\n\n2. Centralized VCS (CVCS):\n- Single central server stores all versioned files. Developers check out files from this central server.\n- Examples: SVN (Subversion), CVS, Perforce.\n- Advantages over local: Enables collaboration, administrators have control over access.\n- Limitations: Single point of failure — if the central server goes down, no one can commit or access history. Requires network connection for most operations. Slow for large repositories.\n\n3. Distributed VCS (DVCS):\n- Every developer has a complete copy (clone) of the entire repository, including full history.\n- Examples: Git, Mercurial.\n- Advantages: No single point of failure — every clone is a full backup. Most operations are local (fast, work offline). Flexible workflows (forking, multiple remotes).\n\nWhy Git Became Dominant:\nGit was created by Linus Torvalds in 2005 for Linux kernel development. It became dominant because:\n1. Speed: Most operations are local — commit, branch, merge, log, diff all happen without network access.\n2. Branching model: Branches are lightweight (just pointers to commits). Creating, switching, and merging branches is nearly instantaneous. This enables feature branch workflows.\n3. Data integrity: Every file and commit is checksummed (SHA-1 hash). Corruption is immediately detectable.\n4. Full backup: Every clone contains the complete history. If any server is lost, any clone can restore it.\n5. Offline capability: Developers can commit, branch, and view history without network access. Sync when connected.\n6. GitHub/GitLab ecosystem: Platforms built around Git added collaboration features (pull requests, code review, issue tracking) that made Git the center of modern development workflows.\n\nGit vs SVN Key Differences:\n- SVN: Commit requires network → Git: Commit is local, push syncs\n- SVN: Branching is expensive (copies directories) → Git: Branching is instant (pointer)\n- SVN: History on server only → Git: Full history on every clone\n- SVN: Single point of failure → Git: Every clone is a backup\n- SVN: Slow for large repos → Git: Fast local operations\n- SVN: Linear workflow → Git: Flexible workflows (forking, feature branches)",
    "difficulty": "medium",
    "topic_area": "Version Control with Git"
  },
  {
    "id": "q056",
    "concept_ids": ["c-git-002"],
    "scenario": "",
    "question_text": "Explain Git's core concepts in depth: repository, commit, branch, merge, staging area, and working directory. Describe Git's three-area architecture (working directory, staging area, repository) and how files move between these areas. Why does Git have a staging area, and what workflow advantages does it provide?",
    "model_answer": "Git Core Concepts:\n\n1. Repository:\nA repository (repo) is the database that stores all files, their complete history, branches, and metadata. It lives in the hidden .git directory at the root of the project. The repository contains every version of every file ever committed, all branch pointers, tags, and configuration. Think of it as 'the project folder with superpowers.'\n\n2. Commit:\nA commit is a snapshot of the entire project at a specific point in time. Each commit contains:\n- A unique identifier (SHA-1 hash, e.g., a1b2c3d)\n- The complete state of all tracked files\n- Author name and email\n- Timestamp\n- Commit message describing the change\n- Pointer to parent commit(s) — creating a linked chain of history\nCommits are immutable — once created, they cannot be changed (only new commits can be created).\n\n3. Branch:\nA branch is a lightweight, movable pointer to a specific commit. The default branch is typically called 'main' (or 'master'). When you create a new branch, Git creates a new pointer — it doesn't copy any files. When you make a commit on a branch, the branch pointer moves forward to the new commit. Branches enable parallel development: multiple features can be developed simultaneously without interfering with each other.\n\n4. Merge:\nMerging combines changes from one branch into another. Git supports several merge strategies:\n- Fast-forward merge: If the target branch hasn't diverged, Git simply moves the pointer forward.\n- Three-way merge: If both branches have new commits, Git creates a merge commit that combines both sets of changes.\n- Merge conflicts occur when both branches modified the same lines — these must be resolved manually.\n\n5. Staging Area (Index):\nThe staging area is an intermediate zone between the working directory and the repository. It holds the changes that will be included in the next commit. Files are added to the staging area with 'git add'.\n\n6. Working Directory:\nThe working directory is the actual files on your filesystem — what you see and edit. It represents a single checkout of one version of the project.\n\nGit's Three-Area Architecture:\n\nWorking Directory → (git add) → Staging Area → (git commit) → Repository\n\nFile states and transitions:\n- Untracked: New file in working directory, not known to Git.\n- Modified: Tracked file changed in working directory but not staged.\n- Staged: Changes added to staging area, ready for next commit.\n- Committed: Changes safely stored in the repository.\n\nFlow: Edit files (working directory) → git add (move to staging) → git commit (move to repository).\nReverse: git checkout (restore from repository to working directory), git reset (unstage from staging area).\n\nWhy the Staging Area Exists:\nThe staging area is Git's unique feature (most other VCS go directly from working directory to repository). It provides:\n\n1. Selective commits: You can stage only some of your changes, creating focused commits. If you fixed a bug AND refactored a function, you can commit them separately — stage the bug fix, commit it, then stage the refactoring, commit it.\n\n2. Commit crafting: Review exactly what will be committed before committing. 'git diff --staged' shows what's staged. This enables clean, logical commit history.\n\n3. Partial file staging: 'git add -p' lets you stage individual hunks within a file — committing some changes to a file while keeping others unstaged.\n\n4. Safety net: Changes must be explicitly staged before committing. This prevents accidental commits of unfinished work, debug code, or sensitive files.\n\n5. Workflow flexibility: Developers can work on multiple logical changes simultaneously, then organize them into clean, separate commits using the staging area.",
    "difficulty": "hard",
    "topic_area": "Version Control with Git"
  },
  {
    "id": "q057",
    "concept_ids": ["c-git-003"],
    "scenario": "",
    "question_text": "Explain the Git forking workflow in detail and why it is the standard for open-source collaboration. Describe each step from fork to merge, the role of the 'upstream' remote, and how contributors keep their forks synchronized. How does the forking workflow differ from other Git workflows (feature branch, Gitflow), and what makes it particularly suited for open-source projects?",
    "model_answer": "The Git Forking Workflow:\nThe forking workflow is a collaboration model where each contributor has their own server-side copy (fork) of the repository. Contributors make changes in their fork and propose them to the original repository via pull requests. This is the standard workflow for open-source projects on GitHub.\n\nStep-by-Step Process:\n\n1. Fork: Create a personal copy of the original repository on GitHub. This creates a complete copy under your account that you have full write access to. The original repository is called 'upstream.'\n\n2. Clone: Clone your fork to your local machine: git clone https://github.com/YOUR-USERNAME/project.git\n\n3. Add upstream remote: Connect your local repo to the original: git remote add upstream https://github.com/ORIGINAL-OWNER/project.git. Now you have two remotes: 'origin' (your fork) and 'upstream' (original repo).\n\n4. Create feature branch: Never work directly on main. Create a descriptive branch: git checkout -b fix-login-validation\n\n5. Make changes: Edit code, write tests, commit with clear messages following project conventions. Make small, focused commits.\n\n6. Push to fork: git push origin fix-login-validation — pushes your branch to YOUR fork on GitHub.\n\n7. Open Pull Request: On GitHub, create a PR from your fork's branch to the original repo's main branch. Include a clear description of what the change does and why.\n\n8. Code review: Maintainers review the PR, request changes, discuss implementation. Respond to feedback by making additional commits to the same branch.\n\n9. Merge: Once approved, maintainers merge the PR into the original repository.\n\nKeeping Forks Synchronized:\nThe original repository continues to evolve while you work. To stay current:\n- git fetch upstream — download latest changes from original repo\n- git checkout main — switch to your main branch\n- git merge upstream/main — merge upstream changes into your local main\n- git push origin main — update your fork's main branch\n\nDo this regularly to avoid large merge conflicts when submitting PRs.\n\nRole of 'upstream' remote:\n- 'origin': Your fork — you have write access. Push your branches here.\n- 'upstream': The original repository — you typically have read-only access. Pull updates from here.\nThis two-remote setup is the key to the forking workflow.\n\nComparison with Other Workflows:\n\nFeature Branch Workflow:\n- All developers clone the same repository and create feature branches.\n- Developers push branches directly to the shared repository.\n- Requires write access to the repository for all contributors.\n- Suited for: Small teams where all members are trusted contributors.\n\nGitflow Workflow:\n- Uses specific branch types: main, develop, feature/*, release/*, hotfix/*.\n- More structured with defined branch naming and merging rules.\n- Suited for: Projects with scheduled releases and formal release processes.\n\nForking Workflow:\n- Each contributor has their own fork — no write access to the original needed.\n- Changes proposed via pull requests, reviewed by maintainers.\n- Suited for: Open-source projects with untrusted contributors.\n\nWhy Forking is Ideal for Open Source:\n1. No trust required: Contributors don't need write access to the original repo. Anyone can fork and propose changes.\n2. Maintainer control: All changes go through pull request review. Maintainers decide what gets merged.\n3. Isolation: A contributor's experiments in their fork cannot affect the original repository.\n4. Scale: Thousands of contributors can work simultaneously without coordination — each in their own fork.\n5. Quality gate: The PR review process ensures code quality, style consistency, and test coverage before merging.",
    "difficulty": "hard",
    "topic_area": "Git Forking Workflow"
  },
  {
    "id": "q058",
    "concept_ids": ["c-ghactions-001"],
    "scenario": "",
    "question_text": "Explain GitHub Actions as a CI/CD platform. Describe its event-driven architecture and each of its core components: Workflows, Events, Jobs, Steps, Actions, and Runners. How do these components relate to each other, and what makes GitHub Actions different from traditional CI/CD tools like Jenkins?",
    "model_answer": "GitHub Actions Overview:\nGitHub Actions is a CI/CD automation platform built directly into GitHub. It uses an event-driven architecture where repository events (push, pull request, schedule) trigger automated workflows. Unlike traditional CI/CD tools that require separate server setup, GitHub Actions is integrated into the repository.\n\nCore Components and Their Relationships:\n\n1. Workflows:\n- The top-level automation unit. A workflow is an automated procedure defined in a YAML file.\n- Stored in .github/workflows/ directory of the repository.\n- A repository can have multiple workflows (e.g., ci.yml, deploy.yml, release.yml).\n- Each workflow is triggered by specific events and contains one or more jobs.\n\n2. Events:\n- Events are triggers that start a workflow. They represent something that happened in or to the repository.\n- Repository events: push (code pushed), pull_request (PR opened/updated), issues (issue created), release (release published).\n- Scheduled events: schedule (cron syntax, e.g., run nightly at midnight).\n- Manual events: workflow_dispatch (manually triggered via UI/API), repository_dispatch (triggered by external webhook).\n- Events can be filtered: on push to specific branches, on pull_request for specific file paths.\n\n3. Jobs:\n- A job is a set of steps that execute on the same runner (virtual machine).\n- A workflow can contain multiple jobs.\n- By default, jobs run in parallel. Dependencies between jobs can be defined with the 'needs' keyword to create sequential execution.\n- Each job runs in a fresh runner instance — no state is shared between jobs unless explicitly passed via artifacts.\n\n4. Steps:\n- Steps are individual tasks within a job. They execute sequentially within their job.\n- A step can be either: a shell command (run: npm test) or a reusable action (uses: actions/checkout@v4).\n- Steps within a job share the same filesystem and environment, so earlier steps can set up the environment for later steps.\n- Each step can have conditions (if: success(), if: failure()) to control execution.\n\n5. Actions:\n- Actions are reusable, packaged units of automation. They are the building blocks of steps.\n- Types: JavaScript actions, Docker container actions, composite actions.\n- Sources: GitHub Marketplace (thousands of community actions), same repository, other repositories.\n- Examples: actions/checkout (clone repo), actions/setup-node (install Node.js), actions/upload-artifact (save build outputs).\n\n6. Runners:\n- Runners are the servers (virtual machines) that execute workflow jobs.\n- GitHub-hosted runners: Managed by GitHub. Available as Ubuntu Linux, Windows Server, macOS. Pre-installed with common tools. Fresh instance for every job (clean environment).\n- Self-hosted runners: Your own servers registered with GitHub. For custom environments, specific hardware, or private network access.\n\nComponent Hierarchy:\nWorkflow → triggered by Event → contains Jobs → each Job runs on a Runner → each Job contains Steps → Steps use Actions or run commands.\n\nGitHub Actions vs Jenkins:\n\n1. Setup: Jenkins requires dedicated server installation, maintenance, and updates. GitHub Actions is zero-setup — built into GitHub.\n2. Configuration: Jenkins uses Groovy-based Jenkinsfile or UI configuration. GitHub Actions uses simple YAML files.\n3. Infrastructure: Jenkins requires managing build agents. GitHub Actions provides managed runners (or optional self-hosted).\n4. Integration: Jenkins integrates with GitHub via plugins. GitHub Actions is natively integrated — events, status checks, and PR comments are built-in.\n5. Marketplace: GitHub Actions has a rich marketplace of reusable actions. Jenkins has plugins but they require installation and management.\n6. Cost: Jenkins is free but requires infrastructure. GitHub Actions is free for public repos, with usage limits for private repos.\n7. Scalability: Jenkins scaling requires adding and managing agents. GitHub Actions auto-scales with managed runners.",
    "difficulty": "hard",
    "topic_area": "Continuous Integration with GitHub Actions"
  },
  {
    "id": "q059",
    "concept_ids": ["c-ghactions-002"],
    "scenario": "",
    "question_text": "Explain how to configure a GitHub Actions workflow using YAML. Describe the structure of a workflow file, including the key directives: name, on (trigger events), jobs, runs-on, and steps. What types of events can trigger workflows, and how can you control when and how workflows execute using event filters and conditions?",
    "model_answer": "GitHub Actions Workflow YAML Structure:\n\nA workflow file is a YAML document stored in .github/workflows/ with the following structure:\n\nTop-level directives:\n\n1. name: (optional) Human-readable name displayed in the GitHub Actions UI.\n   name: CI Pipeline\n\n2. on: (required) Defines which events trigger the workflow.\n   Simple: on: push\n   Multiple events: on: [push, pull_request]\n   With filters:\n   on:\n     push:\n       branches: [main, develop]\n       paths: ['src/**', 'tests/**']\n     pull_request:\n       branches: [main]\n       types: [opened, synchronize]\n\n3. env: (optional) Environment variables available to all jobs and steps.\n   env:\n     NODE_VERSION: '18'\n\n4. jobs: (required) Defines one or more jobs to execute.\n\nJob configuration:\n   jobs:\n     build:\n       runs-on: ubuntu-latest\n       steps: [...]\n     deploy:\n       needs: build\n       runs-on: ubuntu-latest\n       steps: [...]\n\nKey job directives:\n- runs-on: Specifies the runner environment (ubuntu-latest, windows-latest, macos-latest, or self-hosted runner labels).\n- needs: Defines job dependencies for sequential execution. Without 'needs', jobs run in parallel.\n- if: Conditional execution (if: github.ref == 'refs/heads/main').\n- strategy.matrix: Run the same job with different configurations (multiple Node versions, OS versions).\n- timeout-minutes: Maximum job execution time.\n- environment: Reference a deployment environment with protection rules.\n\nStep configuration:\n   steps:\n     - name: Checkout code\n       uses: actions/checkout@v4\n     - name: Setup Node.js\n       uses: actions/setup-node@v4\n       with:\n         node-version: '18'\n     - name: Install dependencies\n       run: npm ci\n     - name: Run tests\n       run: npm test\n\nKey step directives:\n- uses: Reference a reusable action (with version tag).\n- run: Execute a shell command.\n- with: Pass input parameters to an action.\n- env: Step-specific environment variables.\n- if: Conditional step execution.\n- name: Human-readable step name.\n\nEvent Types and Triggers:\n\n1. Repository events:\n- push: Code pushed to repository. Filter by branches, tags, paths.\n- pull_request: PR opened, updated, closed. Filter by branches, types (opened, synchronize, closed), paths.\n- create/delete: Branch or tag created/deleted.\n- release: Release published, created, edited.\n- issues: Issue opened, closed, labeled.\n\n2. Scheduled events:\n- schedule: Cron syntax for periodic execution.\n  on:\n    schedule:\n      - cron: '0 0 * * *'  # Daily at midnight UTC\n\n3. Manual events:\n- workflow_dispatch: Manual trigger via GitHub UI or API. Can define input parameters.\n  on:\n    workflow_dispatch:\n      inputs:\n        environment:\n          description: 'Deploy environment'\n          required: true\n          default: 'staging'\n\n- repository_dispatch: Triggered by external webhook via GitHub API.\n\n4. Workflow events:\n- workflow_run: Triggered when another workflow completes.\n- workflow_call: Allows a workflow to be called by another workflow (reusable workflows).\n\nControlling Execution:\n\n1. Branch filters: Only trigger on specific branches.\n2. Path filters: Only trigger when specific files change (paths: ['src/**']).\n3. Path ignore: Skip trigger for specific paths (paths-ignore: ['docs/**']).\n4. Concurrency: Prevent multiple runs of the same workflow.\n   concurrency:\n     group: ${{ github.workflow }}-${{ github.ref }}\n     cancel-in-progress: true\n5. Conditional steps/jobs: Use 'if' expressions with GitHub context variables.\n   if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n6. Environment protection rules: Require manual approval before deployment jobs run.",
    "difficulty": "hard",
    "topic_area": "Continuous Integration with GitHub Actions"
  },
  {
    "id": "q060",
    "concept_ids": ["c-ghactions-003"],
    "scenario": "",
    "question_text": "Explain the difference between GitHub-hosted runners and self-hosted runners. For each type, describe the characteristics, advantages, limitations, and appropriate use cases. Then explain GitHub Actions Secrets: what they are, how they are stored and accessed, and best practices for managing sensitive data in CI/CD workflows.",
    "model_answer": "GitHub-Hosted Runners:\n\nCharacteristics:\n- Virtual machines managed entirely by GitHub.\n- Available OS options: Ubuntu Linux (most common), Windows Server, macOS.\n- Fresh instance for every job — clean environment, no state from previous runs.\n- Pre-installed with common tools: Git, Docker, Node.js, Python, Java, .NET, Terraform, AWS CLI, and many more.\n- Automatically updated with latest tool versions and security patches.\n\nAdvantages:\n- Zero maintenance — GitHub handles provisioning, updates, and security.\n- Clean environment every run — no 'works on the build server' issues.\n- Immediate availability — no setup required.\n- Auto-scaling — GitHub manages capacity.\n- Free for public repositories; included minutes for private repos.\n\nLimitations:\n- Fixed hardware specs (typically 2 vCPU, 7GB RAM for Linux).\n- Cannot access private networks (your internal databases, services behind firewalls).\n- Limited customization — you get what GitHub provides.\n- Usage limits and billing for private repositories.\n- Job execution time limit (6 hours for GitHub-hosted).\n\nUse cases: Standard CI/CD (build, test, deploy to cloud), open-source projects, teams that don't want to manage build infrastructure.\n\nSelf-Hosted Runners:\n\nCharacteristics:\n- Your own servers (physical or virtual) registered with GitHub as runners.\n- You install the GitHub Actions runner application on your machine.\n- Can be any OS: Linux, Windows, macOS, or even ARM devices.\n- Persistent environment — state carries over between jobs (unless you clean it).\n\nAdvantages:\n- Custom hardware — GPU for ML workloads, high-memory for large builds, specific CPU architectures.\n- Private network access — can reach internal databases, services, and APIs behind firewalls.\n- No usage limits — run as many jobs as your hardware supports.\n- Persistent caching — dependencies and build caches persist between runs (faster builds).\n- Full customization — install any tools, configure any environment.\n\nLimitations:\n- You are responsible for maintenance: OS updates, security patches, runner application updates.\n- Security risk: workflow code from pull requests runs on your infrastructure. Malicious PRs could compromise the runner.\n- You manage scaling — need to provision enough runners for peak demand.\n- Persistent state can cause 'works on the runner' issues if environment drifts.\n- Infrastructure cost and management overhead.\n\nUse cases: Builds requiring private network access, specialized hardware (GPU, ARM), compliance requirements (data must not leave your network), very large builds exceeding GitHub-hosted specs.\n\nGitHub Actions Secrets:\n\nWhat they are:\nSecrets are encrypted environment variables that store sensitive data — API keys, passwords, tokens, certificates, SSH keys. They are used to provide credentials to workflows without exposing them in code.\n\nHow they are stored:\n- Encrypted at rest using libsodium sealed boxes.\n- Never exposed in logs — GitHub automatically redacts secret values from workflow logs.\n- Stored at different levels: repository secrets (available to all workflows in the repo), environment secrets (available only to jobs referencing that environment), organization secrets (shared across repositories).\n\nHow they are accessed in workflows:\n- Referenced using the secrets context: ${{ secrets.AWS_ACCESS_KEY_ID }}\n- Available as environment variables in steps:\n  env:\n    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n- Cannot be read or exported by workflows — only used as values.\n\nBest practices:\n1. Never hardcode secrets in workflow files or source code.\n2. Use the minimum scope — prefer repository secrets over organization secrets.\n3. Rotate secrets regularly — update API keys and tokens periodically.\n4. Use environment protection rules — require approval before jobs can access production secrets.\n5. Limit secret access — use environments to restrict which branches/workflows can access sensitive secrets.\n6. Use OIDC (OpenID Connect) where possible — instead of storing long-lived credentials, use short-lived tokens (e.g., AWS OIDC provider for GitHub Actions).\n7. Audit secret usage — review which workflows access which secrets.\n8. Never log secrets — even accidentally (e.g., printing environment variables).",
    "difficulty": "hard",
    "topic_area": "Continuous Integration with GitHub Actions"
  },
  {
    "id": "q061",
    "concept_ids": ["c-network-001"],
    "scenario": "",
    "question_text": "Explain the fundamental concepts of computer networking: what a network is, the difference between LAN and WAN, the role of switches and routers, and the concept of bandwidth. How do communication links (coaxial, fiber, wireless) differ, and why is understanding networking essential for cloud computing?",
    "model_answer": "Fundamental Networking Concepts:\n\nWhat is a Network:\nA network is a collection of computing systems connected via communication links that can exchange data. Networks enable resource sharing (files, printers, internet access), communication (email, messaging), and distributed computing (cloud services, microservices).\n\nLAN vs WAN:\n- LAN (Local Area Network): A network covering a small geographic area — a building, office floor, or campus. Characteristics: high bandwidth (1-10 Gbps), low latency, typically owned and managed by a single organization. Technologies: Ethernet, Wi-Fi.\n- WAN (Wide Area Network): A network spanning large geographic areas — cities, countries, or globally. Connects multiple LANs together. Characteristics: lower bandwidth than LAN, higher latency, often uses leased lines or internet connections. The internet is the largest WAN.\n\nSwitches and Routers:\n- Switch: Operates at Layer 2 (Data Link). Connects devices within a LAN. Forwards frames based on MAC addresses. Creates a local network segment where devices can communicate directly.\n- Router: Operates at Layer 3 (Network). Connects different networks (LANs to WANs, or different subnets). Forwards packets based on IP addresses. Makes routing decisions about the best path for data to travel between networks.\n- Systems connect indirectly through these devices — data passes through switches within a LAN and through routers between networks.\n\nBandwidth:\nBandwidth is the maximum rate of data transfer across a network path, measured in bits per second (bps). Important distinctions:\n- Bandwidth is measured in bits per second (bps), not bytes per second. 1 Gbps = 125 MB/s.\n- Bandwidth is the theoretical maximum — actual throughput is typically lower due to overhead, congestion, and protocol efficiency.\n- Latency (delay) is different from bandwidth. A high-bandwidth connection can still have high latency (satellite links have high bandwidth but ~600ms latency).\n\nCommunication Links:\n- Coaxial cable: Older technology, moderate bandwidth, used in cable TV/internet. Being replaced by fiber.\n- Copper (twisted pair): Ethernet cables (Cat5e, Cat6). Common in LANs. Up to 10 Gbps over short distances.\n- Fiber optic: Light-based transmission. Highest bandwidth (100+ Gbps), lowest latency, longest distance. Used for backbone connections, data center interconnects, and increasingly for last-mile.\n- Wireless (radio waves): Wi-Fi (LAN), cellular (WAN). Convenient but lower bandwidth and higher latency than wired. Subject to interference.\n\nWhy Networking is Essential for Cloud Computing:\n1. Cloud IS networking: All cloud services are accessed over the network. Understanding protocols, latency, and bandwidth is fundamental.\n2. VPC design: Cloud architects design virtual networks (VPCs, subnets, routing) — requiring networking knowledge.\n3. Security: Firewalls, security groups, NACLs all operate on network concepts (ports, protocols, IP ranges).\n4. Performance: Application performance depends on network design — latency between services, bandwidth for data transfer, DNS resolution.\n5. Troubleshooting: Network issues are among the most common cloud problems. Understanding TCP/IP, DNS, and routing is essential for diagnosis.\n6. Cost: Data transfer is a significant cloud cost. Understanding network topology helps optimize costs.",
    "difficulty": "medium",
    "topic_area": "Networking Fundamentals"
  },
  {
    "id": "q062",
    "concept_ids": ["c-network-002"],
    "scenario": "",
    "question_text": "Explain the OSI 7-layer model and the TCP/IP 4-layer model. For each layer in both models, describe its purpose and give examples of protocols that operate at that layer. How do the two models map to each other, and why is the layered approach to networking important?",
    "model_answer": "OSI 7-Layer Model (Open Systems Interconnection):\nA conceptual framework that standardizes network communication into seven distinct layers:\n\nLayer 7 — Application:\n- Purpose: Provides network services directly to end-user applications.\n- Protocols: HTTP/HTTPS (web), SMTP (email), FTP (file transfer), DNS (name resolution), SSH (secure shell).\n- What it does: Defines how applications interact with the network. This is where users and software interact with the network.\n\nLayer 6 — Presentation:\n- Purpose: Data translation, encryption, and compression.\n- Functions: Character encoding (ASCII, UTF-8), data encryption (SSL/TLS), data compression.\n- What it does: Ensures data from the application layer is in a format the receiving system can understand.\n\nLayer 5 — Session:\n- Purpose: Manages sessions (connections) between applications.\n- Functions: Session establishment, maintenance, and termination. Synchronization and checkpointing.\n- What it does: Controls the dialog between two computers — who talks when, session recovery.\n\nLayer 4 — Transport:\n- Purpose: End-to-end communication, reliability, and flow control.\n- Protocols: TCP (reliable, ordered), UDP (fast, unreliable).\n- What it does: Segments data, provides port numbers for multiplexing, ensures reliable delivery (TCP) or fast delivery (UDP).\n\nLayer 3 — Network:\n- Purpose: Logical addressing and routing between networks.\n- Protocols: IP (IPv4, IPv6), ICMP (ping), routing protocols (BGP, OSPF).\n- What it does: Determines the best path for data to travel from source to destination across multiple networks. Uses IP addresses.\n\nLayer 2 — Data Link:\n- Purpose: Node-to-node data transfer within a local network.\n- Protocols: Ethernet, Wi-Fi (802.11), ARP (Address Resolution Protocol).\n- What it does: Frames data for transmission on the physical medium. Uses MAC addresses. Error detection (CRC).\n\nLayer 1 — Physical:\n- Purpose: Physical transmission of raw bits over a communication medium.\n- Technologies: Ethernet cables, fiber optics, radio waves, hubs, repeaters.\n- What it does: Defines electrical signals, light pulses, or radio frequencies that represent 0s and 1s.\n\nTCP/IP 4-Layer Model:\nThe practical model used in real-world networking:\n\nLayer 4 — Application: Combines OSI layers 5, 6, 7. Protocols: HTTP, SMTP, DNS, SSH, FTP.\nLayer 3 — Transport: Maps to OSI layer 4. Protocols: TCP, UDP.\nLayer 2 — Internet: Maps to OSI layer 3. Protocols: IP, ICMP.\nLayer 1 — Network Interface (Link): Combines OSI layers 1, 2. Protocols: Ethernet, Wi-Fi, ARP.\n\nMapping between models:\nOSI Layer 7 (Application) + 6 (Presentation) + 5 (Session) → TCP/IP Layer 4 (Application)\nOSI Layer 4 (Transport) → TCP/IP Layer 3 (Transport)\nOSI Layer 3 (Network) → TCP/IP Layer 2 (Internet)\nOSI Layer 2 (Data Link) + 1 (Physical) → TCP/IP Layer 1 (Network Interface)\n\nWhy the Layered Approach is Important:\n1. Abstraction: Each layer only needs to know about the layer directly above and below it. A web developer doesn't need to understand electrical signals.\n2. Modularity: Layers can be developed and updated independently. Switching from Ethernet to Wi-Fi (Layer 1-2) doesn't affect HTTP (Layer 7).\n3. Interoperability: Standardized layers allow different vendors' equipment to work together.\n4. Troubleshooting: Problems can be isolated to specific layers. 'Can you ping the server?' (Layer 3) vs 'Is the cable plugged in?' (Layer 1) vs 'Is the web server responding?' (Layer 7).\n5. Encapsulation: Each layer wraps data with its own header (and sometimes trailer), creating a nested structure. Application data → TCP segment → IP packet → Ethernet frame → physical bits.",
    "difficulty": "hard",
    "topic_area": "Networking Fundamentals"
  },
  {
    "id": "q063",
    "concept_ids": ["c-network-003"],
    "scenario": "",
    "question_text": "Provide a thorough comparison of TCP and UDP. Explain the TCP three-way handshake process in detail, including the purpose of each step. Describe the mechanisms TCP uses to ensure reliable delivery (sequence numbers, acknowledgments, retransmission, flow control, congestion control). Why would an application choose UDP despite its lack of reliability guarantees?",
    "model_answer": "TCP (Transmission Control Protocol):\n\nCharacteristics:\n- Connection-oriented: Establishes a connection before data transfer.\n- Reliable: Guarantees delivery of all data in the correct order.\n- Ordered: Data arrives in the same sequence it was sent.\n- Error-checked: Detects and retransmits corrupted or lost data.\n- Flow-controlled: Prevents sender from overwhelming receiver.\n- Higher overhead: Headers are 20-60 bytes; connection setup adds latency.\n\nTCP Three-Way Handshake:\nBefore any data is exchanged, TCP establishes a connection:\n\nStep 1 — SYN (Synchronize):\n- Client sends a SYN packet to the server with an initial sequence number (ISN).\n- Purpose: 'I want to establish a connection. My starting sequence number is X.'\n- Client enters SYN_SENT state.\n\nStep 2 — SYN-ACK (Synchronize-Acknowledge):\n- Server responds with a SYN-ACK packet: acknowledges the client's SYN (ACK = client's ISN + 1) and sends its own SYN with its own ISN.\n- Purpose: 'I acknowledge your request. My starting sequence number is Y.'\n- Server enters SYN_RECEIVED state.\n\nStep 3 — ACK (Acknowledge):\n- Client sends an ACK packet acknowledging the server's SYN (ACK = server's ISN + 1).\n- Purpose: 'I acknowledge your response. Connection established.'\n- Both sides enter ESTABLISHED state. Data transfer can begin.\n\nTCP Reliability Mechanisms:\n\n1. Sequence Numbers: Every byte of data is assigned a sequence number. The receiver uses these to reassemble data in the correct order, even if packets arrive out of order.\n\n2. Acknowledgments (ACKs): The receiver sends ACK packets confirming receipt of data. The ACK number indicates the next byte expected. If the sender doesn't receive an ACK within a timeout period, it retransmits.\n\n3. Retransmission: Lost or corrupted packets are automatically retransmitted. TCP uses adaptive timeout (RTT estimation) to determine when to retransmit. Fast retransmit: if three duplicate ACKs are received, retransmit immediately without waiting for timeout.\n\n4. Flow Control (Sliding Window): The receiver advertises a 'window size' — how much data it can accept. The sender limits its transmission to this window. Prevents a fast sender from overwhelming a slow receiver.\n\n5. Congestion Control: TCP monitors network congestion and adjusts sending rate. Algorithms: Slow Start (exponential increase), Congestion Avoidance (linear increase), Fast Recovery. Prevents the sender from overwhelming the network.\n\nUDP (User Datagram Protocol):\n\nCharacteristics:\n- Connectionless: No connection setup — just send data.\n- Unreliable: No guarantee of delivery, ordering, or duplicate protection.\n- No flow control or congestion control.\n- Minimal overhead: 8-byte header (vs TCP's 20-60 bytes). No handshake latency.\n- Faster: No connection setup, no acknowledgment waiting, no retransmission delays.\n\nWhy Choose UDP Despite No Reliability:\n\n1. Real-time applications: Video streaming, voice calls (VoIP), online gaming. A retransmitted packet arriving late is worse than a lost packet — the moment has passed. Better to skip it and show the next frame.\n\n2. Speed-critical applications: DNS queries (small, single request-response), DHCP. The overhead of TCP connection setup is disproportionate for tiny exchanges.\n\n3. Broadcast/multicast: UDP supports sending to multiple recipients simultaneously. TCP is point-to-point only.\n\n4. Application-level reliability: Some applications implement their own reliability on top of UDP, tailored to their specific needs (e.g., QUIC protocol, game networking with client-side prediction).\n\n5. IoT and telemetry: Sensor data where individual lost readings are acceptable. The simplicity and low overhead of UDP is preferred.\n\nSummary comparison:\n- TCP: Reliable, ordered, connection-oriented, higher overhead. Use for: web (HTTP), email (SMTP), file transfer (FTP), database connections.\n- UDP: Fast, simple, connectionless, no guarantees. Use for: streaming, gaming, DNS, VoIP, live broadcasts.",
    "difficulty": "hard",
    "topic_area": "Networking Fundamentals"
  },
  {
    "id": "q064",
    "concept_ids": ["c-network-004"],
    "scenario": "",
    "question_text": "Explain how DNS (Domain Name System) works, including the resolution process from domain name to IP address. Describe the DNS hierarchy (root servers, TLD servers, authoritative servers) and the role of recursive resolvers. What is DNS TTL, how does caching work at each level, and what is the recommended strategy for managing TTL during infrastructure migrations?",
    "model_answer": "How DNS Works:\nDNS is the 'phone book of the internet' — it translates human-readable domain names (www.example.com) into machine-readable IP addresses (93.184.216.34). Without DNS, users would need to remember IP addresses for every website.\n\nDNS Hierarchy:\nDNS is organized as a hierarchical, distributed database:\n\n1. Root Servers (.):\n- 13 root server clusters (A through M) distributed globally.\n- They don't know the IP of www.example.com, but they know which servers are responsible for .com, .org, .net, etc.\n- The starting point for all DNS resolution.\n\n2. TLD (Top-Level Domain) Servers:\n- Responsible for top-level domains: .com, .org, .net, .io, country codes (.kr, .uk, .jp).\n- They don't know the IP of www.example.com, but they know which authoritative name server handles example.com.\n\n3. Authoritative Name Servers:\n- The definitive source for a domain's DNS records.\n- They hold the actual DNS records (A records, CNAME records, MX records, etc.) for a specific domain.\n- When queried for www.example.com, they return the actual IP address.\n\n4. Recursive Resolver (DNS Resolver):\n- Usually operated by your ISP or a public DNS service (Google 8.8.8.8, Cloudflare 1.1.1.1).\n- Acts as an intermediary between the client and the DNS hierarchy.\n- Performs the full resolution process and caches results.\n\nDNS Resolution Process (for www.example.com):\n1. Client checks its local DNS cache. If found and TTL not expired, use cached result.\n2. Client queries the recursive resolver (configured in OS network settings).\n3. Recursive resolver checks its cache. If found, return cached result.\n4. If not cached, resolver queries a root server: 'Where is www.example.com?'\n5. Root server responds: 'I don't know, but .com is handled by these TLD servers.'\n6. Resolver queries the .com TLD server: 'Where is www.example.com?'\n7. TLD server responds: 'example.com is handled by these authoritative name servers.'\n8. Resolver queries the authoritative name server: 'What is the IP for www.example.com?'\n9. Authoritative server responds: 'The A record for www.example.com is 93.184.216.34, TTL 3600.'\n10. Resolver caches the result and returns it to the client.\n11. Client caches the result and connects to the IP address.\n\nDNS TTL (Time To Live):\nTTL is a value (in seconds) attached to every DNS record that specifies how long the record can be cached before it must be re-queried from the authoritative source.\n\nCaching at each level:\n- Browser cache: Stores DNS results for the TTL duration. Fastest lookup.\n- OS cache: Operating system DNS cache (systemd-resolved on Linux, DNS Client service on Windows).\n- Recursive resolver cache: ISP or public resolver caches results. Serves all users of that resolver.\n- Each level respects the TTL — when TTL expires, the cached entry is discarded and a fresh query is made.\n\nTTL Trade-offs:\n- High TTL (e.g., 86400 = 24 hours): Fewer DNS queries (less load on authoritative servers), faster resolution for repeat visits. But changes take up to 24 hours to propagate.\n- Low TTL (e.g., 300 = 5 minutes): Changes propagate quickly. But more DNS queries, slightly higher latency for first resolution after cache expiry.\n\nMigration TTL Strategy:\nWhen migrating infrastructure (changing IP addresses), TTL management is critical:\n\n1. Days before migration: Lower TTL from normal (e.g., 86400s) to short (e.g., 300s). This must be done at least one full TTL period before the migration — if current TTL is 24 hours, lower it at least 24 hours in advance.\n\n2. Wait for old TTL to expire: All caches worldwide must refresh with the new short TTL. This takes as long as the old TTL value.\n\n3. Perform migration: Change the DNS record to point to the new IP address. With the short TTL, all caches will pick up the change within 5 minutes.\n\n4. Verify: Confirm the new server is receiving traffic and functioning correctly.\n\n5. After successful migration: Gradually increase TTL back to normal (e.g., 86400s) to reduce DNS query load.\n\nThis strategy minimizes the window during which some users are directed to the old server and others to the new server.",
    "difficulty": "hard",
    "topic_area": "Networking Fundamentals"
  },
  {
    "id": "q065",
    "concept_ids": ["c-network-005"],
    "scenario": "",
    "question_text": "Explain IP addressing in detail: the structure of IPv4 and IPv6 addresses, the purpose of subnet masks, and CIDR notation. How does subnetting work — what does it mean to divide a network, and how do you calculate the number of available hosts in a CIDR block like 10.0.0.0/24? Why is subnetting important for cloud network design?",
    "model_answer": "IP Addressing:\n\nIPv4:\n- 32-bit address written as four octets in decimal: 192.168.1.100\n- Each octet is 8 bits (0-255), separated by dots.\n- Total address space: 2^32 = ~4.3 billion addresses (insufficient for modern internet).\n- Address classes (historical): Class A (1.0.0.0 - 126.0.0.0, /8), Class B (128.0.0.0 - 191.255.0.0, /16), Class C (192.0.0.0 - 223.255.255.0, /24).\n- Private address ranges (RFC 1918): 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16 — used within private networks, not routable on the internet.\n\nIPv6:\n- 128-bit address written as eight groups of four hexadecimal digits: 2001:0db8:85a3:0000:0000:8a2e:0370:7334\n- Total address space: 2^128 = ~340 undecillion addresses (effectively unlimited).\n- Designed to solve IPv4 address exhaustion.\n- Simplified notation: leading zeros can be omitted, consecutive zero groups replaced with :: (e.g., 2001:db8::8a2e:370:7334).\n\nSubnet Masks:\nA subnet mask determines which portion of an IP address identifies the network and which identifies the host:\n- Network portion: Identifies which network the device belongs to. All devices on the same network share this portion.\n- Host portion: Identifies the specific device within the network.\n- Example: IP 192.168.1.100 with mask 255.255.255.0 → Network: 192.168.1.0, Host: .100\n- The mask is a series of 1s (network bits) followed by 0s (host bits): 255.255.255.0 = 11111111.11111111.11111111.00000000\n\nCIDR Notation (Classless Inter-Domain Routing):\nCIDR replaces the old class-based system with a flexible prefix length:\n- Format: IP_address/prefix_length (e.g., 10.0.0.0/16)\n- The prefix length indicates how many bits are the network portion.\n- 10.0.0.0/16: First 16 bits are network (10.0), remaining 16 bits are hosts.\n- 10.0.0.0/24: First 24 bits are network (10.0.0), remaining 8 bits are hosts.\n\nCalculating Available Hosts:\nFormula: 2^(32 - prefix_length) - 2 = available host addresses\n(Subtract 2: one for network address, one for broadcast address)\n\nExamples:\n- /24: 2^(32-24) - 2 = 2^8 - 2 = 254 hosts\n- /16: 2^(32-16) - 2 = 2^16 - 2 = 65,534 hosts\n- /28: 2^(32-28) - 2 = 2^4 - 2 = 14 hosts\n- /32: Single host (used for specific host routes)\n\nSubnetting — Dividing a Network:\nSubnetting splits a larger network into smaller, isolated segments:\n- A /16 network (65,534 hosts) can be divided into 256 /24 subnets (254 hosts each).\n- Each subnet is an independent broadcast domain — traffic in one subnet doesn't affect others.\n- Routers connect subnets and control traffic flow between them.\n\nWhy Subnetting is Important for Cloud Network Design:\n1. VPC architecture: Cloud VPCs use CIDR blocks. You must plan subnet sizes for different tiers (public, private, database).\n2. Security isolation: Different subnets can have different security rules (NACLs). Web servers in public subnets, databases in private subnets.\n3. Availability Zones: Each AZ typically gets its own subnet. A /16 VPC might have /24 subnets in each AZ.\n4. IP address planning: Avoid overlapping CIDR blocks between VPCs (important for VPC peering). Plan for growth — it's difficult to resize subnets later.\n5. Cost optimization: Smaller subnets waste fewer IP addresses. Larger subnets provide room for scaling.\n6. Routing control: Route tables direct traffic between subnets, to the internet (via Internet Gateway), or to other networks (via VPN/peering).",
    "difficulty": "hard",
    "topic_area": "Networking Fundamentals"
  },
  {
    "id": "q066",
    "concept_ids": ["c-network-006"],
    "scenario": "",
    "question_text": "Explain the concepts of VPC (Virtual Private Cloud), routing, and NAT in cloud networking. Describe how a VPC provides network isolation, how route tables control traffic flow, and how NAT enables private instances to access the internet. What is the role of an Internet Gateway, and how do public and private subnets differ architecturally?",
    "model_answer": "VPC (Virtual Private Cloud):\nA VPC is a logically isolated virtual network within a cloud provider's infrastructure. It is your own private section of the cloud where you define the network topology.\n\nKey characteristics:\n- Isolation: Your VPC is completely isolated from other customers' VPCs. Traffic cannot cross VPC boundaries unless explicitly configured (VPC peering, transit gateway).\n- CIDR block: You define the IP address range (e.g., 10.0.0.0/16) when creating the VPC. All resources within the VPC get IP addresses from this range.\n- Subnets: The VPC is divided into subnets, each in a specific Availability Zone. Subnets can be public (internet-accessible) or private (no direct internet access).\n- Full control: You control IP addressing, subnets, route tables, network gateways, and security settings.\n\nRoute Tables:\nRoute tables contain rules (routes) that determine where network traffic is directed. Each subnet is associated with a route table.\n\nRoute table entries have two components:\n- Destination: The CIDR block of the target network (e.g., 0.0.0.0/0 for all traffic, 10.0.0.0/16 for VPC-internal traffic).\n- Target: Where to send matching traffic (local for VPC-internal, Internet Gateway for internet, NAT Gateway for private-to-internet, VPC peering connection, etc.).\n\nExample route table for a public subnet:\n- 10.0.0.0/16 → local (traffic within VPC stays within VPC)\n- 0.0.0.0/0 → igw-xxxxx (all other traffic goes to Internet Gateway)\n\nExample route table for a private subnet:\n- 10.0.0.0/16 → local (traffic within VPC stays within VPC)\n- 0.0.0.0/0 → nat-xxxxx (all other traffic goes through NAT Gateway)\n\nInternet Gateway (IGW):\nAn Internet Gateway is a VPC component that enables communication between the VPC and the internet.\n- Horizontally scaled, redundant, and highly available.\n- Performs network address translation for instances with public IP addresses.\n- A VPC can have only one Internet Gateway.\n- Adding an IGW alone doesn't make instances internet-accessible — the subnet's route table must have a route to the IGW, and instances must have public IP addresses.\n\nNAT (Network Address Translation):\nNAT translates private IP addresses to public IP addresses, enabling instances in private subnets to initiate outbound connections to the internet (for software updates, API calls, etc.) without being directly accessible from the internet.\n\nNAT Gateway (managed service) or NAT Instance (self-managed EC2):\n- Placed in a public subnet (has a route to the Internet Gateway).\n- Private subnet route table directs internet-bound traffic (0.0.0.0/0) to the NAT Gateway.\n- NAT Gateway translates the private source IP to its own public IP for outbound traffic.\n- Return traffic is translated back and forwarded to the originating private instance.\n- Inbound connections from the internet cannot reach private instances through NAT — it only supports outbound-initiated connections.\n\nPublic vs Private Subnets:\n\nPublic Subnet:\n- Route table has a route to the Internet Gateway (0.0.0.0/0 → IGW).\n- Instances can have public IP addresses (or Elastic IPs).\n- Directly accessible from the internet (subject to security group rules).\n- Use for: Load balancers, bastion hosts, web servers that need direct internet access.\n\nPrivate Subnet:\n- Route table does NOT have a direct route to the Internet Gateway.\n- Instances have only private IP addresses.\n- Not directly accessible from the internet.\n- Outbound internet access (if needed) goes through a NAT Gateway in a public subnet.\n- Use for: Application servers, databases, internal services — anything that should not be directly exposed to the internet.\n\nTypical Architecture:\nInternet → Internet Gateway → Public Subnet (load balancer, bastion host) → Private Subnet (application servers) → Private Subnet (database servers). The NAT Gateway in the public subnet allows private instances to reach the internet for updates while remaining inaccessible from outside.",
    "difficulty": "hard",
    "topic_area": "Networking Fundamentals"
  },
  {
    "id": "q067",
    "concept_ids": ["c-iac-001"],
    "scenario": "",
    "question_text": "Define Infrastructure as Code (IaC) and explain how it applies software development practices to infrastructure management. What are the core goals of IaC, and how does it connect to version control, automated testing, and CI/CD? Compare managing infrastructure through IaC versus managing it through a cloud provider's web console.",
    "model_answer": "Infrastructure as Code (IaC) Defined:\nIaC is the practice of managing and provisioning computing infrastructure through machine-readable definition files rather than manual processes or interactive configuration tools. It applies software development practices — version control, code review, testing, CI/CD — to infrastructure management.\n\nCore Goals of IaC:\n1. Infrastructure supports change: Systems should be easy to modify, not fragile artifacts that everyone is afraid to touch.\n2. Routine changes without drama: Changes should be small, frequent, and low-risk — not rare, large, and terrifying.\n3. Self-service provisioning: Teams can provision their own infrastructure without waiting for a central IT team.\n4. Easy recovery from failures: If something breaks, it can be quickly rebuilt from code.\n5. Continuous improvement: Infrastructure evolves through the same iterative process as application code.\n\nConnection to Software Development Practices:\n\nVersion Control:\n- All infrastructure definitions are stored in Git alongside application code.\n- Every change is tracked: who changed what, when, and why (commit messages).\n- Changes can be reviewed before applying (pull requests for infrastructure changes).\n- Any previous version can be restored by reverting to a previous commit.\n- Branching enables testing infrastructure changes in isolation before merging.\n\nAutomated Testing:\n- Syntax validation: terraform validate, linting tools check for errors before applying.\n- Plan review: terraform plan shows what will change before any changes are made.\n- Integration testing: Provision test infrastructure, run tests, tear it down.\n- Policy testing: Tools like Open Policy Agent (OPA) or Sentinel enforce compliance rules.\n- Security scanning: Tools like tfsec or Checkov scan IaC for security misconfigurations.\n\nCI/CD:\n- Infrastructure changes go through the same pipeline as application code.\n- Commit → automated validation → plan review → approval → apply.\n- Automated deployment of infrastructure changes reduces manual errors.\n- Rollback is possible by applying a previous version of the code.\n\nIaC vs Web Console Management:\n\nWeb Console (Manual):\n- Click through UI to create resources. Fast for one-off tasks.\n- No record of what was done or why (no audit trail beyond cloud provider logs).\n- Cannot be reproduced exactly — 'click here, then there, then set this value.'\n- Prone to human error — wrong click, wrong value, forgotten step.\n- Cannot be reviewed by peers before applying.\n- Difficult to replicate across environments (dev, staging, production).\n- No version history — cannot roll back to a previous state.\n- Doesn't scale — managing hundreds of resources manually is impractical.\n\nIaC:\n- Define resources in code files. Slower initial setup but faster for repeated operations.\n- Complete audit trail in Git — every change tracked with context.\n- Fully reproducible — run the same code, get the same infrastructure.\n- Peer review through pull requests — catch errors before they reach production.\n- Consistent across environments — same code deploys to dev, staging, production.\n- Version history — roll back to any previous state.\n- Scales to thousands of resources — code manages complexity.\n- Testable — validate before applying.\n\nThe fundamental shift: IaC treats infrastructure as software — it's versioned, tested, reviewed, and deployed through automated pipelines, not manually configured through a UI.",
    "difficulty": "medium",
    "topic_area": "Infrastructure as Code w/Terraform"
  },
  {
    "id": "q068",
    "concept_ids": ["c-iac-002"],
    "scenario": "",
    "question_text": "Explain the core principles of Infrastructure as Code: reproducibility, disposability, consistency, and repeatability. Then describe the common problems IaC solves: server sprawl, configuration drift, and snowflake servers. Finally, explain the 'Pets vs Cattle' analogy and why the 'cattle' mindset is essential for cloud-native infrastructure.",
    "model_answer": "Core IaC Principles:\n\n1. Reproducibility:\n- Any infrastructure environment can be recreated exactly from its code definition.\n- If a server is destroyed, an identical replacement can be provisioned automatically.\n- The same code produces the same infrastructure every time, regardless of who runs it or when.\n- This eliminates 'it worked when I set it up' problems.\n\n2. Disposability:\n- Infrastructure components should be easy to create and destroy.\n- Servers are not precious — they can be terminated and replaced at any time.\n- This enables: auto-scaling (create/destroy instances based on demand), blue-green deployments (create new environment, switch traffic, destroy old), disaster recovery (rebuild from code in a new region).\n- Disposability requires that no important state lives on the server itself — all state must be in external stores (databases, object storage).\n\n3. Consistency:\n- All environments (development, staging, production) are defined by the same code with environment-specific variables.\n- Every server of the same type is identical — no manual tweaks or one-off configurations.\n- Consistency eliminates 'works in staging but not in production' problems.\n- Changes are applied uniformly across all instances of a resource type.\n\n4. Repeatability:\n- Infrastructure operations can be repeated reliably. Running the same code twice produces the same result (idempotency).\n- New environments can be spun up on demand for testing, demos, or new customers.\n- Disaster recovery is a matter of running the code in a new region, not a manual reconstruction effort.\n\nProblems IaC Solves:\n\n1. Server Sprawl:\n- Problem: Cloud makes it easy to provision resources quickly. Without discipline, organizations accumulate hundreds of servers with no clear ownership, purpose, or documentation.\n- How IaC solves it: All resources are defined in code. If it's not in the code, it shouldn't exist. Code review prevents unnecessary resource creation. Resources can be tagged and tracked through code.\n\n2. Configuration Drift:\n- Problem: Servers that were initially configured identically gradually diverge over time. Manual hotfixes, ad-hoc patches, and one-off changes accumulate. Eventually, no two servers are exactly alike, and no one knows the exact state of any server.\n- How IaC solves it: The code IS the desired state. Running the IaC tool detects and corrects drift. Any manual change is overwritten on the next apply. Changes must go through code, ensuring all servers stay consistent.\n\n3. Snowflake Servers:\n- Problem: Servers that have been manually configured over months or years become unique, irreproducible artifacts. They have undocumented configurations, manually installed packages, and custom tweaks. If a snowflake server fails, it cannot be recreated — the knowledge of how it was configured exists only in the memories of the people who built it.\n- How IaC solves it: Every aspect of the server's configuration is captured in code. The server can be destroyed and recreated identically at any time. The code serves as living documentation of the server's configuration.\n\nPets vs Cattle Analogy:\n\nPets:\n- Each server is unique, with a name (db-server-01, web-prod-primary).\n- When a pet gets sick, you nurse it back to health — you SSH in, diagnose the problem, apply fixes.\n- Pets are irreplaceable — losing one is a crisis.\n- Pets represent the old way: manually configured, carefully maintained, feared to change.\n\nCattle:\n- Servers are identical and numbered (web-001, web-002, web-003).\n- When a cattle member gets sick, you replace it — terminate the unhealthy instance, launch a new one from the same template.\n- Cattle are interchangeable — losing one is a non-event.\n- Cattle represent the cloud-native way: automatically provisioned, disposable, easily replaced.\n\nWhy the cattle mindset is essential:\n1. Auto-scaling requires identical, disposable instances that can be created and destroyed automatically.\n2. Self-healing requires the ability to replace failed instances without manual intervention.\n3. Immutable infrastructure (replace, don't repair) is only possible with cattle.\n4. Disaster recovery is fast when infrastructure can be recreated from code.\n5. The cattle mindset is a prerequisite for DevOps practices like CI/CD, blue-green deployments, and chaos engineering.",
    "difficulty": "hard",
    "topic_area": "Infrastructure as Code w/Terraform"
  },
  {
    "id": "q069",
    "concept_ids": ["c-iac-003"],
    "scenario": "",
    "question_text": "Compare declarative and imperative approaches to Infrastructure as Code. Explain what each approach means, provide examples of tools that use each approach, and discuss the concept of idempotency. Why is the declarative approach generally preferred for IaC, and in what situations might an imperative approach be more appropriate?",
    "model_answer": "Declarative Approach:\n- Definition: You define the desired end state of your infrastructure — WHAT you want — and the tool figures out HOW to achieve it.\n- You describe: 'I want 3 web servers, each with 4GB RAM, running Ubuntu 22.04, in subnet X, with security group Y.'\n- The tool compares the desired state with the current state and makes only the necessary changes to reach the desired state.\n\nExamples of declarative tools:\n- Terraform: HCL configuration files define desired infrastructure state.\n- AWS CloudFormation: JSON/YAML templates define AWS resources.\n- Kubernetes manifests: YAML files define desired cluster state.\n- Pulumi: Infrastructure defined in general-purpose languages but with declarative semantics.\n\nImperative Approach:\n- Definition: You specify the exact sequence of commands to execute — HOW to achieve the result — step by step.\n- You describe: 'First, create a VPC. Then create a subnet. Then launch an instance in that subnet. Then attach a security group.'\n- The tool executes commands in order. It doesn't know the current state or the desired end state — it just runs the commands.\n\nExamples of imperative tools:\n- Shell scripts (bash): Sequential commands to provision infrastructure.\n- Ansible playbooks: While Ansible has some declarative features, playbooks are fundamentally ordered task lists.\n- AWS CLI scripts: Sequential API calls to create resources.\n- Chef recipes: Ruby-based scripts that execute in order.\n\nIdempotency:\nAn operation is idempotent if running it multiple times produces the same result as running it once.\n\nDeclarative tools are naturally idempotent:\n- Running 'terraform apply' when the infrastructure already matches the desired state results in no changes.\n- Running it again after a partial failure completes the remaining changes without duplicating what was already done.\n- This is safe: you can run the tool repeatedly without fear of creating duplicate resources or breaking existing ones.\n\nImperative tools are NOT naturally idempotent:\n- Running 'create a server' twice creates two servers.\n- A script that failed halfway through cannot simply be re-run — it may duplicate the steps that already succeeded.\n- Achieving idempotency with imperative tools requires explicit checks: 'if server doesn't exist, create it.'\n\nWhy Declarative is Generally Preferred for IaC:\n1. Idempotency: Safe to run repeatedly. Handles partial failures gracefully.\n2. State awareness: The tool knows the current state and calculates the minimum changes needed.\n3. Drift detection: Can compare actual infrastructure with desired state and report differences.\n4. Simplicity: You describe what you want, not how to get there. Less code, fewer bugs.\n5. Self-documenting: The code IS the documentation of the desired state.\n6. Easier to reason about: Reading declarative code tells you what the infrastructure looks like, not the history of how it was built.\n\nWhen Imperative Might Be More Appropriate:\n1. One-time setup tasks: Initial bootstrapping, database migrations, data seeding.\n2. Complex procedural logic: Operations that require conditional branching, loops with side effects, or specific ordering that's difficult to express declaratively.\n3. Configuration management: Installing packages, configuring services on existing servers (Ansible excels here).\n4. Orchestration: Coordinating multi-step deployment processes with rollback logic.\n5. Ad-hoc operations: Quick, one-off tasks that don't need to be repeatable.\n\nIn practice, many organizations use both: Terraform (declarative) for infrastructure provisioning and Ansible (imperative) for configuration management on provisioned servers.",
    "difficulty": "hard",
    "topic_area": "Infrastructure as Code w/Terraform"
  },
  {
    "id": "q070",
    "concept_ids": ["c-terraform-001"],
    "scenario": "",
    "question_text": "Explain what Terraform is, its role in the IaC ecosystem, and how it works at a high level. Describe HCL (HashiCorp Configuration Language) and the structure of .tf files. What are Terraform providers, and what happens during 'terraform init'? How does Terraform's multi-cloud capability differentiate it from cloud-specific tools like CloudFormation?",
    "model_answer": "What is Terraform:\nTerraform is an open-source Infrastructure as Code tool created by HashiCorp. It allows you to define, provision, and manage cloud infrastructure using a declarative configuration language. Terraform is cloud-agnostic — it can manage resources across AWS, GCP, Azure, and hundreds of other providers through a unified workflow.\n\nHow Terraform Works (High Level):\n1. Write: Define infrastructure in .tf files using HCL.\n2. Plan: Terraform compares desired state (code) with current state (state file) and generates an execution plan showing what will change.\n3. Apply: Terraform makes API calls to the cloud provider to create, modify, or delete resources to match the desired state.\n4. State: Terraform maintains a state file (.tfstate) that maps the code to real-world resources.\n\nHCL (HashiCorp Configuration Language):\nHCL is Terraform's domain-specific language designed to be both human-readable and machine-parseable.\n\nKey characteristics:\n- Declarative: Describes desired end state, not procedural steps.\n- Block-based syntax: Resources, variables, outputs are defined in blocks.\n- Supports expressions, functions, and conditional logic.\n- Files use .tf extension.\n\nStructure of .tf files:\n\nProvider block — specifies which cloud to use:\nprovider \"aws\" {\n  region = \"us-east-1\"\n}\n\nResource block — defines an infrastructure object:\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-0c55b159cbfafe1f0\"\n  instance_type = \"t2.micro\"\n  tags = {\n    Name = \"WebServer\"\n  }\n}\n\nVariable block — defines input parameters:\nvariable \"instance_type\" {\n  description = \"EC2 instance type\"\n  type        = string\n  default     = \"t2.micro\"\n}\n\nOutput block — exposes values after apply:\noutput \"instance_ip\" {\n  value = aws_instance.web.public_ip\n}\n\nTerraform Providers:\nProviders are plugins that enable Terraform to interact with specific cloud platforms, SaaS services, or APIs.\n\n- Each provider offers a set of resource types and data sources.\n- AWS provider: aws_instance, aws_vpc, aws_s3_bucket, etc.\n- GCP provider: google_compute_instance, google_storage_bucket, etc.\n- Azure provider: azurerm_virtual_machine, azurerm_resource_group, etc.\n- There are 3000+ providers in the Terraform Registry covering clouds, DNS, monitoring, databases, and more.\n\nWhat happens during 'terraform init':\n1. Reads the provider blocks in .tf files to determine which providers are needed.\n2. Downloads the specified provider plugins from the Terraform Registry.\n3. Stores provider plugins in the .terraform directory.\n4. Initializes the backend (where the state file is stored — local or remote).\n5. Downloads any referenced modules.\n6. Creates a .terraform.lock.hcl file to lock provider versions for reproducibility.\n\nThis must be run once before any other Terraform command, and again whenever providers or modules change.\n\nTerraform vs CloudFormation:\n\nCloudFormation (AWS-specific):\n- Only manages AWS resources.\n- JSON or YAML syntax.\n- Deeply integrated with AWS (automatic rollback, drift detection, stack events).\n- No state file management — AWS manages state.\n- Free to use (you pay only for resources created).\n\nTerraform (Multi-cloud):\n- Manages resources across any cloud and hundreds of services.\n- HCL syntax (more readable than JSON/YAML for complex configurations).\n- Single tool and workflow for all infrastructure, regardless of provider.\n- Requires state file management (local or remote backend like S3).\n- Enables multi-cloud strategies and avoids vendor lock-in.\n- Larger community and ecosystem of modules and providers.\n\nTerraform's multi-cloud capability is its key differentiator: organizations using multiple clouds (or planning to) can use one tool, one language, and one workflow for all their infrastructure.",
    "difficulty": "medium",
    "topic_area": "Infrastructure as Code w/Terraform"
  },
  {
    "id": "q071",
    "concept_ids": ["c-terraform-002"],
    "scenario": "",
    "question_text": "Describe the Terraform workflow in detail: init, plan, apply, and destroy. For each command, explain what it does, what output it produces, and when it should be used. Then explain the Terraform state file (.tfstate): what it contains, why it is critical, and what risks are associated with managing it. What is the difference between local and remote state?",
    "model_answer": "Terraform Workflow Commands:\n\n1. terraform init:\n- What it does: Initializes the working directory. Downloads provider plugins, initializes the backend (state storage), and downloads referenced modules.\n- Output: Lists downloaded providers and their versions. Confirms successful initialization.\n- When to use: First time setting up a Terraform project, after adding new providers or modules, after changing backend configuration.\n- Idempotent: Safe to run multiple times.\n\n2. terraform plan:\n- What it does: Creates an execution plan by comparing the desired state (code) with the current state (state file). Shows what actions Terraform will take WITHOUT making any changes.\n- Output: A detailed list of resources to be created (+), modified (~), or destroyed (-). Shows specific attribute changes for each resource.\n- When to use: Before every apply — to review changes before they happen. In CI/CD pipelines for review. To verify that code changes produce the expected infrastructure changes.\n- Key benefit: 'Dry run' that prevents surprises. Reviewers can see exactly what will change.\n\n3. terraform apply:\n- What it does: Executes the changes described in the plan. Makes API calls to the cloud provider to create, modify, or delete resources. Updates the state file to reflect the new state.\n- Output: Progress of each resource operation (creating, modifying, destroying). Final summary of changes made. Any output values defined in the configuration.\n- When to use: After reviewing the plan and confirming the changes are correct. Can be run with -auto-approve to skip confirmation (used in CI/CD).\n- Important: By default, terraform apply runs a plan first and asks for confirmation before proceeding.\n\n4. terraform destroy:\n- What it does: Destroys all resources managed by the current Terraform configuration. The reverse of apply — removes everything.\n- Output: Plan showing all resources to be destroyed. After confirmation, progress of each destruction.\n- When to use: Tearing down test environments, decommissioning infrastructure, cleaning up after experiments.\n- Caution: This is destructive and irreversible. Always review the plan before confirming.\n\nTerraform State File (.tfstate):\n\nWhat it contains:\n- A JSON file that maps every resource in your Terraform code to a real-world resource in the cloud.\n- For each resource: the resource type, name, provider, all attributes (IDs, IPs, ARNs), and dependencies.\n- Metadata: Terraform version, serial number (incremented on each change), lineage (unique ID for the state).\n\nWhy it is critical:\n1. Source of truth: Terraform uses the state file to know what currently exists. Without it, Terraform doesn't know what resources it manages.\n2. Plan calculation: terraform plan compares code (desired state) with state file (current state) to determine what changes are needed.\n3. Resource tracking: Maps logical resource names in code (aws_instance.web) to physical resource IDs in the cloud (i-0abc123def456).\n4. Dependency resolution: Tracks dependencies between resources to determine correct creation/destruction order.\n5. Performance: Caches resource attributes to avoid querying the cloud provider for every plan.\n\nRisks of state file management:\n1. Contains sensitive data: State may include database passwords, API keys, and other secrets in plain text.\n2. Single point of failure: If the state file is lost or corrupted, Terraform loses track of all managed resources. Resources still exist in the cloud but Terraform can't manage them.\n3. Concurrent access: If two people run terraform apply simultaneously, they can corrupt the state file or create conflicting changes.\n4. Drift: If someone modifies resources outside of Terraform (via console), the state file becomes out of sync.\n\nLocal vs Remote State:\n\nLocal state:\n- State file stored on the developer's local filesystem (terraform.tfstate).\n- Simple setup, no additional infrastructure needed.\n- Risks: Not shared — only one person can manage the infrastructure. No locking — concurrent access can corrupt state. No backup — local disk failure loses state. Sensitive data on local disk.\n- Appropriate for: Learning, personal projects, single-developer setups.\n\nRemote state:\n- State file stored in a shared, remote backend (S3 + DynamoDB, Terraform Cloud, GCS, Azure Blob).\n- Benefits: Shared access — entire team can manage infrastructure. State locking — prevents concurrent modifications (DynamoDB lock table). Encryption — state encrypted at rest and in transit. Versioning — S3 versioning enables state file recovery. Backup — cloud storage provides durability.\n- Appropriate for: Team environments, production infrastructure, CI/CD pipelines.\n\nBest practice: Always use remote state with locking for any shared or production infrastructure.",
    "difficulty": "hard",
    "topic_area": "Infrastructure as Code w/Terraform"
  },
  {
    "id": "q072",
    "concept_ids": ["c-terraform-003"],
    "scenario": "",
    "question_text": "Explain Terraform's key configuration constructs: resources, variables, outputs, data sources, and modules. For each, describe its purpose, syntax, and when to use it. How do these constructs work together to create reusable, maintainable infrastructure code?",
    "model_answer": "Terraform Configuration Constructs:\n\n1. Resources:\n- Purpose: Define the infrastructure objects Terraform manages — the core building blocks.\n- Syntax:\n  resource \"<provider>_<type>\" \"<local_name>\" {\n    attribute1 = value1\n    attribute2 = value2\n  }\n- Example:\n  resource \"aws_instance\" \"web\" {\n    ami           = \"ami-0c55b159cbfafe1f0\"\n    instance_type = var.instance_type\n    subnet_id     = aws_subnet.public.id\n  }\n- Key concepts: The provider_type (aws_instance) determines what cloud resource is created. The local_name (web) is used to reference this resource elsewhere in the code. Resources can reference other resources' attributes (aws_subnet.public.id), creating implicit dependencies.\n- When to use: For every infrastructure component you want Terraform to create and manage.\n\n2. Variables:\n- Purpose: Make configurations reusable and customizable by parameterizing values.\n- Syntax:\n  variable \"instance_type\" {\n    description = \"The EC2 instance type\"\n    type        = string\n    default     = \"t2.micro\"\n    validation {\n      condition     = contains([\"t2.micro\", \"t2.small\", \"t2.medium\"], var.instance_type)\n      error_message = \"Must be a valid instance type.\"\n    }\n  }\n- Types: string, number, bool, list, map, object, tuple.\n- Value sources (in order of precedence): command-line flags (-var), .tfvars files, environment variables (TF_VAR_name), default values.\n- When to use: For any value that varies between environments (instance types, CIDR blocks, region names, counts) or that you want to be configurable.\n\n3. Outputs:\n- Purpose: Expose information about created resources — useful for displaying results, passing data between modules, or providing values to other tools.\n- Syntax:\n  output \"instance_public_ip\" {\n    description = \"The public IP of the web server\"\n    value       = aws_instance.web.public_ip\n    sensitive   = false\n  }\n- When to use: To display important information after apply (IP addresses, DNS names, resource IDs). To pass data from a child module to the parent module. To share data between Terraform configurations via remote state data sources.\n\n4. Data Sources:\n- Purpose: Read information about existing resources that are NOT managed by the current Terraform configuration. Query external data without creating or modifying anything.\n- Syntax:\n  data \"aws_ami\" \"ubuntu\" {\n    most_recent = true\n    filter {\n      name   = \"name\"\n      values = [\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\"]\n    }\n    owners = [\"099720109477\"]  # Canonical\n  }\n- Usage: Reference with data.<type>.<name>.<attribute> (e.g., data.aws_ami.ubuntu.id)\n- When to use: Looking up AMI IDs, querying existing VPCs or subnets, reading information from other Terraform state files, fetching current AWS account ID or region.\n\n5. Modules:\n- Purpose: Organize and reuse Terraform configurations. A module is a container for multiple resources that are used together.\n- Syntax:\n  module \"vpc\" {\n    source = \"./modules/vpc\"\n    cidr_block = \"10.0.0.0/16\"\n    environment = \"production\"\n  }\n- Sources: Local paths (./modules/vpc), Terraform Registry (hashicorp/consul/aws), Git repositories, S3 buckets.\n- When to use: When you have a group of resources that are always created together (VPC + subnets + route tables). When you want to reuse the same infrastructure pattern across projects. When you want to enforce standards (a 'blessed' VPC module that includes security best practices).\n\nHow They Work Together:\nA typical Terraform project uses all five constructs:\n\n1. Variables parameterize the configuration (environment name, instance size, CIDR blocks).\n2. Data sources look up existing information (latest AMI, existing VPC).\n3. Resources define the infrastructure to create, referencing variables and data sources.\n4. Outputs expose important values (IP addresses, resource IDs).\n5. Modules package related resources together for reuse.\n\nExample flow: A module for a web application tier accepts variables (instance_type, subnet_id), uses a data source to find the latest AMI, creates resources (EC2 instances, security groups, load balancer), and outputs the load balancer DNS name. This module can be reused for dev, staging, and production by passing different variable values.",
    "difficulty": "hard",
    "topic_area": "Infrastructure as Code w/Terraform"
  },
  {
    "id": "q073",
    "concept_ids": ["c-iam-001"],
    "scenario": "",
    "question_text": "Explain the fundamental concepts of Identity and Access Management (IAM): authentication and authorization. What does 'deny by default' mean, and why is it the foundation of cloud security? Describe the Principle of Least Privilege and explain why misconfigured IAM is one of the most common causes of cloud security breaches.",
    "model_answer": "IAM Fundamentals:\n\nIdentity and Access Management (IAM) is the security framework that controls who can access cloud resources and what actions they can perform. It answers two fundamental questions:\n\n1. Authentication — 'Who are you?'\n- The process of verifying the identity of a user, service, or application.\n- Methods: Username/password, API access keys, SSH key pairs, X.509 certificates, multi-factor authentication (MFA), federated identity (SAML, OIDC).\n- Authentication proves identity but does NOT determine what the identity can do.\n\n2. Authorization — 'What are you allowed to do?'\n- The process of determining what actions an authenticated identity can perform on which resources.\n- Defined through policies: rules that specify allowed or denied actions on specific resources.\n- Authorization happens AFTER authentication — you must prove who you are before the system checks what you can do.\n\nDeny by Default:\n- In cloud IAM, all access is denied unless explicitly granted. A new IAM user or role has ZERO permissions.\n- This is the opposite of traditional systems where users might have broad default access.\n- Why this is the foundation of cloud security:\n  1. Blast radius minimization: If an identity is compromised, the attacker can only do what was explicitly permitted — not everything.\n  2. Intentional access: Every permission is a deliberate decision, not an oversight.\n  3. Auditability: You can review exactly what each identity is allowed to do.\n  4. Defense in depth: Even if one layer of security fails, deny-by-default limits the damage.\n\nPrinciple of Least Privilege:\n- Grant only the minimum permissions necessary for an identity to perform its intended function — nothing more.\n- A web application that only needs to read from an S3 bucket should NOT have permission to delete objects, write to other buckets, or manage EC2 instances.\n- Least privilege applies to: users (human operators), service accounts (applications), roles (assumed by services), and even temporary credentials.\n\nImplementing least privilege:\n1. Start with zero permissions (deny by default).\n2. Add only the specific permissions needed for the task.\n3. Scope permissions to specific resources (not wildcard *).\n4. Use conditions to further restrict (e.g., only from specific IP ranges, only during business hours).\n5. Regularly review and remove unused permissions.\n6. Use temporary credentials (roles) instead of long-lived access keys.\n\nWhy Misconfigured IAM Causes Breaches:\nIAM misconfiguration is consistently ranked as one of the top cloud security risks:\n\n1. Overly permissive policies: Granting AdministratorAccess or * permissions when specific permissions would suffice. If the identity is compromised, the attacker has full access.\n\n2. Long-lived credentials: Using access keys that never expire and are never rotated. If leaked (in code, logs, or Git history), they provide persistent access.\n\n3. Public access: S3 buckets or resources accidentally made publicly accessible through misconfigured policies.\n\n4. Shared credentials: Multiple people using the same access keys. No accountability, no audit trail, and revoking access affects everyone.\n\n5. Unused permissions: Permissions granted for a one-time task but never revoked. Accumulate over time, expanding the attack surface.\n\n6. Root account usage: Using the root account (which cannot be restricted) for daily operations. If compromised, the attacker has unrestricted access to everything.\n\n7. Missing MFA: Not requiring multi-factor authentication for privileged operations. Password alone is insufficient for sensitive access.\n\nThe common thread: IAM breaches are almost always about excessive permissions, not insufficient authentication. Organizations authenticate correctly but authorize too broadly.",
    "difficulty": "hard",
    "topic_area": "Identity & Access Management (IAM)"
  },
  {
    "id": "q074",
    "concept_ids": ["c-iam-002"],
    "scenario": "",
    "question_text": "Describe the core components of an IAM system: identities (users, groups, service accounts), policies, roles, and credentials. Explain how these components work together to implement access control. What is the difference between identity-based policies and resource-based policies, and when would you use each?",
    "model_answer": "Core IAM Components:\n\n1. Identities (Principals):\nAn identity is any entity that can request access to resources.\n\n- Users: Individual human identities. Each person who needs access gets their own user account with unique credentials. Never share user accounts.\n- Groups: Collections of users who share the same permissions. Instead of attaching policies to each user individually, attach policies to groups and add users to groups. Example: 'Developers' group, 'DBAdmins' group, 'ReadOnly' group.\n- Service Accounts: Machine identities for applications, services, and automated processes. Applications should never use human user credentials. Service accounts have programmatic access (API keys, tokens) but no console password.\n- Roles: Not a permanent identity but a set of permissions that can be assumed temporarily. Users, services, or even other AWS accounts can assume a role to get temporary credentials with specific permissions.\n\n2. Policies:\nPolicies are documents (typically JSON) that define permissions — what actions are allowed or denied on which resources.\n\nPolicy structure (AWS example):\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [{\n    \"Effect\": \"Allow\" or \"Deny\",\n    \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n    \"Resource\": [\"arn:aws:s3:::my-bucket/*\"],\n    \"Condition\": { ... }\n  }]\n}\n\nPolicy components:\n- Effect: Allow or Deny the specified actions.\n- Action: The specific API operations (s3:GetObject, ec2:RunInstances).\n- Resource: The specific resources the policy applies to (ARNs).\n- Condition: Optional conditions (IP range, time, MFA required).\n\n3. Roles:\nRoles provide temporary security credentials for specific tasks.\n\n- Unlike users, roles don't have permanent credentials (no password or access keys).\n- An entity 'assumes' a role and receives temporary credentials (typically valid for 1-12 hours).\n- Use cases: EC2 instances accessing S3 (instance role), cross-account access, federated users, Lambda functions.\n- Benefits: No long-lived credentials to manage or rotate. Credentials automatically expire. Fine-grained, task-specific permissions.\n\n4. Credentials:\nCredentials prove an identity's authenticity.\n\n- Passwords: For console (human) access. Should be strong, rotated, and protected by MFA.\n- Access Keys: Key ID + Secret Key pair for programmatic (API/CLI) access. Should be rotated regularly. Never embed in code.\n- SSH Key Pairs: For SSH access to EC2 instances.\n- X.509 Certificates: For specific AWS services and HTTPS.\n- Temporary Security Credentials: Short-lived credentials from assuming roles (STS). Preferred over long-lived access keys.\n\nHow Components Work Together:\n1. A user authenticates with credentials (password + MFA for console, access keys for API).\n2. The user belongs to groups that have policies attached.\n3. The user may also have individual policies attached.\n4. The user may assume a role to get additional temporary permissions.\n5. When the user makes a request, IAM evaluates ALL applicable policies (user, group, role, resource-based).\n6. If any policy explicitly denies the action → DENIED.\n7. If any policy explicitly allows the action (and no deny) → ALLOWED.\n8. If no policy addresses the action → DENIED (deny by default).\n\nIdentity-Based vs Resource-Based Policies:\n\nIdentity-based policies:\n- Attached to IAM identities (users, groups, roles).\n- Define what the identity can do.\n- Example: 'User Alice can read objects from any S3 bucket.'\n- Use when: Controlling what a specific user/role can access across multiple resources.\n\nResource-based policies:\n- Attached to resources (S3 buckets, SQS queues, Lambda functions).\n- Define who can access the resource.\n- Example: 'This S3 bucket can be accessed by Account 123456789012.'\n- Use when: Granting cross-account access, making resources accessible to specific principals, controlling access at the resource level.\n\nKey difference: Identity-based policies travel with the identity. Resource-based policies stay with the resource. For cross-account access, resource-based policies are often simpler because they don't require role assumption.",
    "difficulty": "hard",
    "topic_area": "Identity & Access Management (IAM)"
  },
  {
    "id": "q075",
    "concept_ids": ["c-iam-003"],
    "scenario": "",
    "question_text": "Compare the IAM implementations across AWS, GCP, and Azure. For each provider, describe the key identity types, how policies are structured and attached, and the role model. What are the fundamental architectural differences (flat vs hierarchical policy models), and how does policy inheritance work in GCP's resource hierarchy?",
    "model_answer": "AWS IAM:\n\nIdentity types: IAM Users (individual accounts), IAM Groups (collections of users), IAM Roles (assumable permission sets), Root Account (unrestricted, should never be used daily).\n\nPolicy structure: JSON documents with Effect, Action, Resource, Condition. Two types:\n- Identity-based policies: Attached to users, groups, or roles. Define what the identity can do.\n- Resource-based policies: Attached to resources (S3 bucket policies, SQS queue policies). Define who can access the resource.\n\nPolicy types:\n- AWS Managed Policies: Pre-built by AWS (e.g., AmazonS3ReadOnlyAccess).\n- Customer Managed Policies: Custom policies created by the organization.\n- Inline Policies: Embedded directly in a user, group, or role (not reusable).\n\nRole model: Roles are assumed via AWS STS (Security Token Service). Returns temporary credentials. Used for: EC2 instance profiles, cross-account access, federated users, Lambda execution roles.\n\nArchitecture: Relatively flat. Policies are attached directly to identities or resources. No inherent hierarchy (though AWS Organizations adds some structure with Service Control Policies).\n\nGCP IAM:\n\nIdentity types: Google Accounts (individual users), Google Groups, Service Accounts (for applications), Google Workspace domains.\n\nPolicy structure: Bindings that associate members with roles on specific resources. Format: member (who) + role (what permissions) + resource (where).\n\nRole types:\n- Basic Roles: Owner, Editor, Viewer (broad, legacy — avoid in production).\n- Predefined Roles: Fine-grained, service-specific (e.g., roles/storage.objectViewer).\n- Custom Roles: Organization-defined combinations of permissions.\n\nResource hierarchy and policy inheritance:\nGCP has a hierarchical resource model: Organization → Folders → Projects → Resources.\n- Policies set at a higher level are inherited by all lower levels.\n- A policy granting Editor on a Folder applies to ALL projects and resources within that folder.\n- Child policies can add permissions but cannot restrict inherited permissions (additive model).\n- This hierarchy enables centralized governance: security policies at the Organization level apply everywhere.\n\nArchitecture: Hierarchical. Policy inheritance flows down the resource tree. This is fundamentally different from AWS's flat model.\n\nAzure IAM:\n\nIdentity types: Azure Active Directory (Azure AD) Users, Groups, Service Principals (application identities), Managed Identities (Azure-managed service identities).\n\nPolicy structure: Role-Based Access Control (RBAC). Assignments bind a security principal + role definition + scope.\n\nRole types:\n- Built-in Roles: Owner, Contributor, Reader, plus hundreds of service-specific roles.\n- Custom Roles: Organization-defined roles with specific permissions.\n\nResource hierarchy: Management Groups → Subscriptions → Resource Groups → Resources.\n- RBAC assignments at higher levels are inherited by lower levels.\n- Similar to GCP's hierarchical model.\n\nUnique features: Azure AD provides enterprise identity services (SSO, MFA, conditional access) that integrate with Microsoft 365 and thousands of SaaS applications.\n\nKey Architectural Differences:\n\n1. Flat vs Hierarchical:\n- AWS: Primarily flat. Policies attached directly to identities/resources. AWS Organizations adds hierarchy via SCPs but it's an overlay.\n- GCP: Natively hierarchical. Organization → Folders → Projects. Policy inheritance is a core feature.\n- Azure: Natively hierarchical. Management Groups → Subscriptions → Resource Groups. RBAC inheritance built-in.\n\n2. Policy model:\n- AWS: Explicit Allow/Deny in JSON policies. Deny always wins. Complex policy evaluation logic.\n- GCP: Additive model — you can only grant permissions, not deny them (deny policies are a newer addition). Simpler but less granular.\n- Azure: RBAC with deny assignments (newer feature). Role assignments are additive.\n\n3. Service identities:\n- AWS: IAM Roles with instance profiles for EC2, execution roles for Lambda.\n- GCP: Service Accounts — first-class identities that can be assigned to resources.\n- Azure: Managed Identities — system-assigned or user-assigned, automatically managed credentials.\n\nDespite different terminology and architecture, all three providers implement the same core concepts: authentication, authorization, least privilege, and deny by default.",
    "difficulty": "hard",
    "topic_area": "Identity & Access Management (IAM)"
  },
  {
    "id": "q076",
    "concept_ids": ["c-firewall-001"],
    "scenario": "",
    "question_text": "Explain the concept of network firewalls in cloud computing. Compare AWS Security Groups (stateful) and Network ACLs (stateless) in detail: how each works, at what level they operate, their default behavior, rule evaluation order, and the implications of stateful vs stateless filtering. Why is using both considered defense in depth?",
    "model_answer": "Network Firewalls in Cloud Computing:\nA firewall is a security system that monitors and controls network traffic based on predefined rules. In cloud computing, firewalls are virtualized and operate at different levels of the network stack. They filter traffic based on: source/destination IP addresses, ports, protocols, and direction (inbound/outbound).\n\nAWS Security Groups (Stateful):\n\nLevel of operation: Instance level — attached to ENIs (Elastic Network Interfaces) on EC2 instances, RDS databases, Lambda functions, etc.\n\nStateful behavior:\n- If you allow an inbound request, the response is automatically allowed regardless of outbound rules.\n- If you allow an outbound request, the response is automatically allowed regardless of inbound rules.\n- The firewall tracks the state of connections — it knows that a response packet belongs to an allowed request.\n- Example: If you allow inbound TCP 443 (HTTPS), the server's response on the ephemeral port is automatically allowed outbound. You don't need an explicit outbound rule for the response.\n\nDefault behavior:\n- Inbound: DENY ALL — no traffic is allowed in unless you create a rule.\n- Outbound: ALLOW ALL — all outbound traffic is allowed by default.\n\nRule evaluation:\n- All rules are evaluated together. If ANY rule allows the traffic, it is allowed.\n- There is no rule ordering or priority — all rules are permissive (allow only, no deny rules).\n- To block specific traffic, you must NOT have a rule that allows it.\n\nKey features:\n- Can reference other security groups as sources/destinations (e.g., 'allow traffic from the web-server security group').\n- Changes take effect immediately.\n- An instance can have multiple security groups (rules are aggregated).\n\nNetwork ACLs (NACLs) — Stateless:\n\nLevel of operation: Subnet level — applies to ALL traffic entering and leaving the subnet.\n\nStateless behavior:\n- Each packet is evaluated independently — the NACL does not track connections.\n- If you allow inbound TCP 443, you MUST also explicitly allow the outbound response on ephemeral ports (1024-65535).\n- Both inbound and outbound rules must be configured for every allowed communication.\n- Example: To allow HTTPS, you need: Inbound rule allowing TCP 443 from 0.0.0.0/0 AND outbound rule allowing TCP 1024-65535 to 0.0.0.0/0 (for responses).\n\nDefault behavior:\n- Default NACL: ALLOWS all inbound and outbound traffic.\n- Custom NACL: DENIES all inbound and outbound traffic until rules are added.\n\nRule evaluation:\n- Rules are evaluated in order by rule number (lowest first).\n- The first matching rule is applied — subsequent rules are not evaluated.\n- Rules can be Allow OR Deny (unlike Security Groups which are allow-only).\n- A catch-all deny rule (*) at the end denies anything not explicitly allowed.\n- Rule ordering matters: a deny rule with a lower number overrides an allow rule with a higher number.\n\nStateful vs Stateless Implications:\n\nStateful (Security Groups):\n- Simpler to configure — only define rules for the initiating direction.\n- Fewer rules needed — responses are automatic.\n- Better for most use cases — less error-prone.\n- Slight performance overhead for connection tracking.\n\nStateless (NACLs):\n- More complex to configure — must define rules for both directions.\n- More rules needed — explicit rules for responses.\n- More granular control — can explicitly deny specific traffic.\n- No connection tracking overhead.\n- Can block specific IP addresses (deny rules) — something Security Groups cannot do.\n\nDefense in Depth:\nUsing both Security Groups AND NACLs provides layered security:\n\n1. NACLs (subnet level): First line of defense. Can block known malicious IP ranges, restrict traffic between subnets, provide broad subnet-level controls. Can explicitly DENY traffic (Security Groups cannot).\n\n2. Security Groups (instance level): Second line of defense. Fine-grained per-instance rules. Can reference other security groups for dynamic, identity-based rules.\n\nIf an attacker bypasses one layer (e.g., a misconfigured NACL), the other layer still provides protection. This is the principle of defense in depth — multiple independent security layers, each providing protection even if others fail.",
    "difficulty": "hard",
    "topic_area": "Network Firewall"
  },
  {
    "id": "q077",
    "concept_ids": ["c-vm-001"],
    "scenario": "",
    "question_text": "Explain virtualization technology in depth: what it is, why it was developed, and how it works. Describe the role of the hypervisor, the difference between Type 1 (bare-metal) and Type 2 (hosted) hypervisors, and why cloud providers exclusively use Type 1. How does virtualization improve resource efficiency compared to traditional physical servers?",
    "model_answer": "What is Virtualization:\nVirtualization is the technology of using software to create virtual versions of physical hardware — allowing multiple virtual machines (VMs) to run on a single physical server. Each VM operates as an independent computer with its own operating system, applications, and allocated resources (CPU, memory, storage, network), completely isolated from other VMs on the same host.\n\nWhy Virtualization Was Developed:\nTraditional physical servers had a fundamental inefficiency problem:\n- A typical physical server runs at only 10-15% CPU utilization.\n- Each application required its own dedicated server for isolation and reliability.\n- This meant organizations needed many physical servers, most of which sat mostly idle.\n- Hardware procurement took weeks to months.\n- Each server required physical space, power, cooling, and maintenance.\n\nVirtualization solves this by running multiple workloads on a single physical server, dramatically improving resource utilization (to 60-80% or higher).\n\nHow Virtualization Works:\nThe hypervisor is the key software layer that enables virtualization:\n\n1. The hypervisor runs on the physical hardware (host).\n2. It creates and manages virtual machines (guests).\n3. It allocates physical resources (CPU cores, RAM, disk, network) to each VM.\n4. It provides hardware abstraction — each VM thinks it has its own dedicated hardware.\n5. It enforces isolation — VMs cannot access each other's memory or resources.\n6. It handles scheduling — multiplexing VM workloads onto physical CPU cores.\n\nType 1 Hypervisor (Bare-Metal):\n- Runs directly on the physical hardware — there is no host operating system.\n- The hypervisor IS the operating system of the physical machine.\n- Examples: VMware ESXi, KVM (Kernel-based Virtual Machine), Xen, Microsoft Hyper-V (server), AWS Nitro.\n- Characteristics: Best performance (no host OS overhead), direct hardware access, designed for production workloads, supports many VMs per host.\n- Architecture: Hardware → Hypervisor → VMs\n\nType 2 Hypervisor (Hosted):\n- Runs as an application on top of a conventional operating system.\n- The host OS manages hardware; the hypervisor runs as a process within the host OS.\n- Examples: VirtualBox, VMware Workstation, VMware Fusion, Parallels.\n- Characteristics: Easier to set up (install like any application), lower performance (host OS overhead), suitable for development and testing, typically runs fewer VMs.\n- Architecture: Hardware → Host OS → Hypervisor → VMs\n\nWhy Cloud Providers Use Type 1 Exclusively:\n1. Performance: No host OS overhead. VMs get near-native performance.\n2. Efficiency: More resources available for VMs (no host OS consuming CPU/RAM).\n3. Security: Smaller attack surface — no full OS to exploit. The hypervisor is a minimal, hardened layer.\n4. Density: More VMs per physical server = better economics.\n5. Custom optimization: Cloud providers build custom hypervisors (AWS Nitro, GCP custom KVM) optimized for their hardware and workloads.\n6. Hardware-assisted virtualization: Modern CPUs (Intel VT-x, AMD-V) provide hardware support for Type 1 hypervisors, further improving performance.\n\nResource Efficiency Improvement:\n\nWithout virtualization:\n- 10 applications → 10 physical servers\n- Each server at 10-15% utilization\n- Total: 10 servers × $10,000 = $100,000 hardware cost\n- Effective utilization: 10-15%\n- Wasted capacity: 85-90%\n\nWith virtualization:\n- 10 applications → 10 VMs on 2-3 physical servers\n- Each server at 60-80% utilization\n- Total: 2-3 servers × $15,000 = $30,000-$45,000 hardware cost\n- Effective utilization: 60-80%\n- Wasted capacity: 20-40%\n\nAdditional efficiency gains: faster provisioning (minutes vs weeks), easier backup (VM snapshots), better disaster recovery (VM migration), dynamic resource allocation (live migration, auto-scaling).",
    "difficulty": "hard",
    "topic_area": "Virtual Machines"
  },
  {
    "id": "q078",
    "concept_ids": ["c-vm-002"],
    "scenario": "",
    "question_text": "Describe the different types of virtualization: full virtualization, paravirtualization, and hardware-assisted virtualization. Explain how each approach handles the challenge of running guest operating systems, their performance characteristics, and their trade-offs. What is the AWS Nitro System, and why did AWS develop a custom hypervisor?",
    "model_answer": "The Virtualization Challenge:\nThe fundamental challenge of virtualization is running multiple operating systems on hardware designed for one. Guest operating systems expect direct hardware access, but the hypervisor must intercept and mediate all hardware interactions to maintain isolation and share resources.\n\nFull Virtualization:\n- How it works: The hypervisor presents a complete emulation of the underlying hardware to each guest OS. The guest OS runs unmodified — it doesn't know it's virtualized.\n- Hardware access: Guest OS instructions that try to access hardware are trapped by the hypervisor, which emulates the hardware response. This is called 'trap and emulate.'\n- Performance: Moderate overhead due to instruction trapping and emulation. Every privileged instruction must be intercepted and handled by the hypervisor.\n- Trade-offs: No guest OS modification needed (any OS can run), but performance penalty from emulation overhead.\n- Examples: Early VMware products, QEMU (without KVM).\n\nParavirtualization:\n- How it works: The guest OS is modified to be aware it's running in a virtual environment. Instead of executing privileged hardware instructions (which would need to be trapped), the guest OS makes direct 'hypercalls' to the hypervisor.\n- Hardware access: Guest OS communicates directly with the hypervisor through a defined API, bypassing the trap-and-emulate overhead.\n- Performance: Better than full virtualization because hypercalls are more efficient than trapping and emulating instructions.\n- Trade-offs: Requires modifying the guest OS kernel — not all operating systems support this. The guest OS must be specifically adapted for the hypervisor.\n- Examples: Xen with paravirtualized guests, early AWS EC2 instances.\n\nHardware-Assisted Virtualization:\n- How it works: The CPU itself provides hardware support for virtualization. Intel VT-x and AMD-V add new CPU instructions and execution modes specifically for running hypervisors and guest OSes.\n- Hardware access: The CPU has a 'VMX root mode' for the hypervisor and 'VMX non-root mode' for guests. Hardware automatically handles the transitions between guest and hypervisor, eliminating the need for software-based trapping.\n- Performance: Near-native performance. Hardware handles the virtualization overhead that previously required software emulation.\n- Trade-offs: Requires modern CPUs with virtualization extensions (virtually all modern server CPUs have this). Best of both worlds — no guest OS modification needed AND high performance.\n- Examples: KVM, modern VMware ESXi, Hyper-V, all major cloud providers.\n\nAWS Nitro System:\nNitro is AWS's custom-built hypervisor and hardware platform, developed to address limitations of traditional virtualization:\n\nWhat it is:\n- A combination of custom hardware (Nitro Cards) and a lightweight hypervisor.\n- Nitro Cards are dedicated hardware that offloads networking, storage, and security functions from the main CPU.\n- The Nitro Hypervisor is a minimal, stripped-down KVM-based hypervisor.\n\nWhy AWS developed it:\n1. Performance: Traditional hypervisors consume CPU cycles for I/O processing (network packets, storage operations). Nitro Cards handle these in dedicated hardware, freeing all CPU cores for customer workloads. This enables 'bare-metal-like' performance.\n\n2. Security: The Nitro hypervisor is extremely minimal — less code means fewer vulnerabilities. Nitro Cards provide hardware-level isolation. Even AWS operators cannot access customer data on Nitro instances.\n\n3. Innovation: Nitro's modular architecture enables rapid development of new instance types. AWS can add new capabilities (faster networking, new storage options) by updating Nitro Cards without changing the hypervisor.\n\n4. Cost efficiency: By offloading I/O to hardware, more CPU is available for customers, improving the economics of each physical server.\n\n5. Bare metal instances: Nitro enables AWS to offer bare-metal instances (no hypervisor overhead) while still providing cloud management capabilities through Nitro Cards.\n\nEvolution: AWS moved from Xen (paravirtualization) → Xen with HVM (hardware-assisted) → Nitro (custom hardware + minimal hypervisor). Each step improved performance, security, and efficiency.",
    "difficulty": "hard",
    "topic_area": "Virtual Machines"
  },
  {
    "id": "q079",
    "concept_ids": ["c-vm-003"],
    "scenario": "",
    "question_text": "Compare EC2 Instance Store and EBS (Elastic Block Store) in depth. For each storage type, explain the underlying architecture, performance characteristics, data persistence behavior, and cost model. Under what workload conditions is each type optimal, and what are the critical risks engineers must understand for each?",
    "model_answer": "EC2 Instance Store:\n\nArchitecture:\n- Physically attached storage — SSDs or HDDs directly connected to the host server where the EC2 instance runs.\n- The storage is part of the physical host, not a separate networked device.\n- Not all instance types include Instance Store volumes — it depends on the instance family (e.g., i3, d2, c5d, m5d).\n\nPerformance:\n- Highest I/O performance available — no network latency since storage is physically attached.\n- Very high IOPS (Input/Output Operations Per Second) and throughput.\n- Consistent, low-latency access — no competition with other instances for network bandwidth.\n- NVMe-based Instance Store on newer instances provides millions of IOPS.\n\nData Persistence:\n- EPHEMERAL — data is lost when:\n  - The instance is stopped (not just rebooted — stopped).\n  - The instance is terminated.\n  - The underlying host hardware fails.\n  - The instance is moved to a different host (maintenance events).\n- Data DOES survive instance reboots.\n- There is NO way to recover lost Instance Store data — it is permanently gone.\n- Cannot be detached and reattached to another instance.\n- Cannot create snapshots of Instance Store volumes.\n\nCost:\n- Included in the instance price — no additional storage charges.\n- The cost is effectively 'free' if you're already using an instance type that includes it.\n\nOptimal workloads:\n- Temporary data: Batch processing intermediate results, scratch space, temporary caches.\n- High-performance buffers: Application-level caching (Redis/Memcached data that can be rebuilt).\n- Distributed systems with replication: Databases like Cassandra or HDFS where data is replicated across multiple nodes — loss of one node's storage is handled by the cluster.\n\nCritical risks:\n- Data loss on stop/terminate/failure — engineers MUST design for this.\n- No backup mechanism — cannot snapshot Instance Store.\n- Applications must handle data loss gracefully (rebuild from source, replicate from peers).\n\nEBS (Elastic Block Store):\n\nArchitecture:\n- Network-attached block storage — EBS volumes are separate devices connected to EC2 instances over the network.\n- EBS volumes exist independently of EC2 instances — they persist even when the instance is terminated (if configured).\n- Stored redundantly within a single Availability Zone.\n\nPerformance:\n- Lower than Instance Store due to network overhead.\n- Multiple volume types with different performance profiles:\n  - gp3/gp2 (General Purpose SSD): Balanced price/performance. Up to 16,000 IOPS.\n  - io2/io1 (Provisioned IOPS SSD): High-performance. Up to 64,000 IOPS. For databases.\n  - st1 (Throughput Optimized HDD): High throughput for sequential reads. For big data.\n  - sc1 (Cold HDD): Lowest cost for infrequent access.\n- Performance can be affected by network congestion (though EBS-optimized instances have dedicated bandwidth).\n\nData Persistence:\n- PERSISTENT — data survives instance stop, restart, and termination (if DeleteOnTermination is false).\n- Can be detached from one instance and attached to another (within the same AZ).\n- Supports snapshots — point-in-time backups stored in S3. Snapshots can be used to create new volumes, copy to other regions, or share with other accounts.\n- Replicated within the AZ for durability.\n\nCost:\n- Charged separately based on volume type, size (GB/month), and provisioned IOPS/throughput.\n- Snapshot storage is additional cost.\n- You pay for provisioned capacity, not just what you use.\n\nOptimal workloads:\n- Databases: Any data that must persist (MySQL, PostgreSQL, MongoDB).\n- Boot volumes: OS and application installation.\n- Any stateful application: Data that cannot be lost or rebuilt easily.\n- Compliance workloads: Data that must be backed up and retained.\n\nCritical risks:\n- Network dependency — if the network between instance and EBS has issues, I/O is affected.\n- AZ-scoped — EBS volumes exist in one AZ. If the AZ fails, volumes are unavailable (use snapshots for cross-AZ/region recovery).\n- Cost accumulation — unused EBS volumes still incur charges. Orphaned volumes after instance termination are a common cost waste.\n- Snapshot management — snapshots accumulate and incur storage costs if not managed with lifecycle policies.",
    "difficulty": "hard",
    "topic_area": "Virtual Machines"
  },
  {
    "id": "q080",
    "concept_ids": ["c-vm-004"],
    "scenario": "",
    "question_text": "Describe the different VM instance types (general purpose, compute optimized, memory optimized, storage optimized, accelerated computing) and the pricing models available in cloud computing (on-demand, reserved, spot, dedicated). For each pricing model, explain how it works, its cost savings potential, and the trade-offs. How should an organization choose the right combination of instance types and pricing models?",
    "model_answer": "VM Instance Types:\n\nInstance types are optimized for different workload characteristics:\n\n1. General Purpose (e.g., AWS t3, m5, m6i):\n- Balanced compute, memory, and networking.\n- Use for: Web servers, application servers, development environments, small databases.\n- Characteristics: Good all-around performance, cost-effective for diverse workloads.\n\n2. Compute Optimized (e.g., AWS c5, c6i):\n- High-performance processors, high CPU-to-memory ratio.\n- Use for: Batch processing, scientific modeling, gaming servers, machine learning inference, high-performance web servers.\n- Characteristics: Best price-per-compute performance.\n\n3. Memory Optimized (e.g., AWS r5, r6i, x1):\n- Large amounts of RAM, high memory-to-CPU ratio.\n- Use for: In-memory databases (Redis, Memcached, SAP HANA), real-time big data analytics, large caches.\n- Characteristics: Designed for workloads that process large datasets in memory.\n\n4. Storage Optimized (e.g., AWS i3, d2, h1):\n- High sequential read/write access to large datasets on local storage.\n- Use for: Data warehousing, distributed file systems (HDFS), log processing, Elasticsearch.\n- Characteristics: High I/O throughput, large Instance Store volumes.\n\n5. Accelerated Computing (e.g., AWS p4, g5, inf1):\n- Hardware accelerators (GPUs, FPGAs, custom chips).\n- Use for: Machine learning training, graphics rendering, video encoding, genomics.\n- Characteristics: Specialized hardware for parallel processing tasks.\n\nPricing Models:\n\n1. On-Demand:\n- How it works: Pay by the hour or second with no commitment. Start and stop instances at any time.\n- Cost: Full list price. Most expensive per-hour rate.\n- Savings: None — this is the baseline price.\n- Trade-offs: Maximum flexibility, no commitment, no risk. But highest cost.\n- Best for: Short-term, unpredictable workloads. Development and testing. Spiky traffic that can't be predicted. Applications being evaluated before committing.\n\n2. Reserved Instances (RIs) / Savings Plans:\n- How it works: Commit to using a specific instance type (or compute amount) for 1 or 3 years. Pay upfront (all, partial, or none) for a discounted hourly rate.\n- Cost: 30-72% discount compared to on-demand, depending on commitment length and payment option.\n- Savings: Significant for predictable, steady-state workloads.\n- Trade-offs: Commitment risk — if your needs change, you're still paying. Less flexibility. Requires capacity planning.\n- Best for: Production workloads with predictable, steady usage. Databases, core application servers, baseline capacity.\n\n3. Spot Instances:\n- How it works: Bid on unused cloud capacity at steep discounts. The provider can reclaim (interrupt) your instance with short notice (2 minutes on AWS) when demand for that capacity increases.\n- Cost: Up to 90% discount compared to on-demand.\n- Savings: Highest potential savings of any pricing model.\n- Trade-offs: Instances can be terminated at any time. Not suitable for workloads that can't handle interruption. Requires architecture that handles interruptions gracefully.\n- Best for: Fault-tolerant workloads (batch processing, data analysis, CI/CD builds), stateless web servers behind load balancers, big data processing (Spark, Hadoop), containerized workloads with orchestration.\n\n4. Dedicated Hosts / Dedicated Instances:\n- How it works: Physical server dedicated to your use. No other customer's workloads run on the same hardware.\n- Cost: Most expensive option. Pay for the entire physical server.\n- Savings: None — premium pricing for isolation.\n- Trade-offs: Highest cost, but provides physical isolation.\n- Best for: Regulatory compliance requiring physical isolation, software licensing tied to physical cores/sockets (Oracle, Windows Server), security requirements prohibiting multi-tenancy.\n\nChoosing the Right Combination:\n\n1. Analyze workload patterns: Identify steady-state baseline (Reserved), variable/predictable scaling (On-Demand), and fault-tolerant batch work (Spot).\n\n2. Layered approach:\n- Base layer: Reserved Instances for minimum always-on capacity (60-70% of steady state).\n- Scaling layer: On-Demand for predictable scaling above baseline.\n- Opportunistic layer: Spot Instances for fault-tolerant workloads and cost optimization.\n\n3. Right-sizing: Before choosing pricing, ensure you're using the right instance type and size. An oversized Reserved Instance wastes money for the entire commitment period.\n\n4. Regular review: Workloads change. Review instance utilization quarterly and adjust the mix of pricing models accordingly.",
    "difficulty": "hard",
    "topic_area": "Virtual Machines"
  },
  {
    "id": "q081",
    "concept_ids": ["c-vm-005"],
    "scenario": "",
    "question_text": "Explain the Meltdown and Spectre vulnerabilities: what speculative execution is, how these vulnerabilities exploit it, and why they are particularly dangerous in multi-tenant cloud environments. What mitigations exist, and how do cloud providers protect customers from these hardware-level vulnerabilities?",
    "model_answer": "Speculative Execution:\nModern CPUs optimize performance through speculative execution — the processor predicts which instructions will be needed next and executes them in advance, before knowing if they're actually needed. If the prediction is correct, the results are used immediately (saving time). If incorrect, the results are discarded. This optimization has been fundamental to CPU performance for decades.\n\nThe vulnerability: While speculative execution results are 'discarded' when predictions are wrong, they leave traces in the CPU cache. Attackers can measure cache access times to infer what data was speculatively accessed — even data they shouldn't have permission to read.\n\nMeltdown (CVE-2017-5754):\n- Affects: Primarily Intel processors.\n- How it works: Exploits a flaw in how CPUs handle permission checks during speculative execution. The CPU speculatively reads kernel memory (which should be inaccessible to user programs) before the permission check completes. Although the read is eventually blocked, the data briefly exists in the CPU cache. By measuring cache timing, an attacker can reconstruct the kernel memory contents.\n- Impact: A user-space program can read arbitrary kernel memory, including passwords, encryption keys, and data from other processes.\n- Severity: Relatively straightforward to exploit. Breaks the fundamental isolation between user space and kernel space.\n\nSpectre (CVE-2017-5753, CVE-2017-5715):\n- Affects: Nearly all modern processors (Intel, AMD, ARM).\n- How it works: Manipulates the branch prediction mechanism to trick the CPU into speculatively executing code paths that access sensitive data. Two main variants:\n  - Variant 1 (Bounds Check Bypass): Tricks the CPU into speculatively accessing array elements beyond their bounds.\n  - Variant 2 (Branch Target Injection): Poisons the branch predictor to redirect speculative execution to attacker-chosen code.\n- Impact: Can read memory from other processes or even other VMs on the same physical host.\n- Severity: Harder to exploit than Meltdown but also harder to fully mitigate. Affects a broader range of processors.\n\nWhy Particularly Dangerous in Multi-Tenant Cloud:\nCloud computing is fundamentally multi-tenant — multiple customers' VMs run on the same physical hardware, sharing the same CPU:\n\n1. Cross-VM attacks: An attacker's VM on the same physical host could potentially read memory from other customers' VMs — including encryption keys, passwords, and sensitive data.\n2. Shared CPU caches: The CPU cache is shared between VMs on the same physical core. Speculative execution side-channel attacks exploit this shared resource.\n3. Scale of impact: A single vulnerable physical server may host dozens of VMs from different customers.\n4. Trust boundary violation: The hypervisor is supposed to provide complete isolation between VMs. These vulnerabilities bypass that isolation at the hardware level.\n\nMitigations:\n\n1. Microcode updates: CPU manufacturers (Intel, AMD) released firmware updates that modify how speculative execution works. These reduce (but don't eliminate) the attack surface. Performance impact: 2-30% depending on workload.\n\n2. Kernel patches:\n- KPTI (Kernel Page Table Isolation) for Meltdown: Separates kernel and user page tables so kernel memory is not mapped in user space. Eliminates Meltdown but adds overhead for system calls.\n- Retpoline for Spectre Variant 2: Replaces indirect branches with a construct that prevents speculative execution of the target.\n\n3. Compiler mitigations: Updated compilers insert barriers that prevent speculative execution of sensitive code paths.\n\n4. Cloud provider mitigations:\n- AWS: Nitro hypervisor provides hardware-level isolation. Microcode updates applied to all hosts. Customers can use dedicated hosts for complete physical isolation.\n- GCP: Live-migrated VMs to patched hosts. Applied microcode and kernel patches.\n- Azure: Applied firmware and OS updates across the fleet.\n- All providers: Ongoing monitoring for new variants and rapid patching.\n\n5. Dedicated hosts: For the most sensitive workloads, customers can use dedicated hosts (no multi-tenancy) to eliminate the cross-VM attack vector entirely.\n\nOngoing concern: Meltdown and Spectre were the first in a family of speculative execution vulnerabilities. New variants continue to be discovered (L1TF, MDS, TAA), requiring ongoing vigilance and patching.",
    "difficulty": "hard",
    "topic_area": "Virtual Machines"
  },
  {
    "id": "q082",
    "concept_ids": ["c-image-001"],
    "scenario": "",
    "question_text": "Explain what machine images are, their role in cloud computing, and how they enable immutable infrastructure. Describe the components of an AWS AMI (Amazon Machine Image), the AMI lifecycle, and how machine images solve the 'snowflake server' problem. What is the relationship between machine images and the 'cattle not pets' philosophy?",
    "model_answer": "Machine Images Defined:\nA machine image is a pre-configured template that contains a complete snapshot of a virtual machine's state: the operating system, installed software, configurations, and optionally data. It serves as a blueprint from which identical VM instances can be launched.\n\nCloud provider implementations:\n- AWS: Amazon Machine Image (AMI)\n- GCP: Custom Image / Machine Image\n- Azure: VM Image / Managed Image\n\nRole in Cloud Computing:\n1. Standardization: Ensure every instance launched from the same image is identical.\n2. Speed: Launch pre-configured instances in minutes instead of spending hours installing and configuring software.\n3. Reproducibility: The same image produces the same server every time, regardless of who launches it or when.\n4. Scalability: Auto-scaling groups use images to launch new instances automatically — each one identical to the others.\n\nAWS AMI Components:\n1. Root volume: Contains the operating system, installed applications, and configurations. This is the boot volume.\n2. Launch permissions: Control which AWS accounts can use the AMI (public, specific accounts, or private).\n3. Block device mapping: Defines additional EBS volumes to attach when launching (size, type, encryption).\n4. Metadata: AMI ID, name, description, architecture (x86_64, arm64), virtualization type (hvm), creation date.\n\nAMI Lifecycle:\n1. Create: Build an AMI from a running instance (capture current state) or from a snapshot.\n2. Register: AMI is registered in the region and assigned a unique AMI ID.\n3. Copy: AMIs can be copied to other regions for multi-region deployment.\n4. Share: AMIs can be shared with specific AWS accounts or made public.\n5. Launch: Create EC2 instances from the AMI.\n6. Deprecate: Mark an AMI as deprecated (still usable but flagged as outdated).\n7. Disable: Prevent new launches from the AMI.\n8. Deregister: Remove the AMI registration (snapshots remain until manually deleted).\n\nImmutable Infrastructure:\nImmutable infrastructure is the practice of never modifying running servers. Instead of patching, updating, or configuring a running server:\n1. Build a new machine image with the desired changes.\n2. Launch new instances from the new image.\n3. Route traffic to the new instances.\n4. Terminate the old instances.\n\nBenefits of immutable infrastructure:\n- Predictability: Every instance is a known, tested state. No configuration drift.\n- Debugging: If something is wrong, compare the image to the previous working image.\n- Rollback: Deploy the previous image to instantly revert changes.\n- Security: No SSH access needed to production servers (nothing to change manually).\n\nSolving the Snowflake Server Problem:\nSnowflake servers are unique, manually configured servers that cannot be reproduced:\n- They accumulate manual changes over months/years.\n- No one knows the exact configuration.\n- If they fail, they cannot be recreated.\n- They are 'pets' — irreplaceable and feared.\n\nMachine images solve this by:\n- Capturing the complete server configuration as code/image.\n- Making servers reproducible — launch a new one from the image.\n- Eliminating manual configuration — everything is in the image.\n- Making servers disposable — if one fails, launch another.\n\nRelationship to 'Cattle not Pets':\nMachine images are the enabler of the cattle philosophy:\n- Pets: Manually configured, unique, irreplaceable → snowflake servers.\n- Cattle: Created from images, identical, disposable → machine image-based instances.\n\nWithout machine images, you can't have cattle — you'd need to manually configure each new server, making it unique. Machine images ensure every instance is identical and replaceable, which is the foundation of auto-scaling, self-healing, and immutable infrastructure.",
    "difficulty": "hard",
    "topic_area": "Custom Machine Images"
  },
  {
    "id": "q083",
    "concept_ids": ["c-image-002"],
    "scenario": "",
    "question_text": "Explain the concept of a Golden Image and its role in enterprise cloud environments. Describe the best practices for creating and managing golden images: security hardening, versioning, tagging, lifecycle policies, and CI/CD integration. Why is a well-managed golden image pipeline critical for both security and operational efficiency?",
    "model_answer": "Golden Image Defined:\nA golden image is a standardized, hardened, and approved base machine image that serves as the foundation for all instances in an organization. It represents the organization's baseline configuration — the 'gold standard' that all servers must meet.\n\nRole in Enterprise Cloud Environments:\n1. Security baseline: Every instance starts from a security-hardened image, ensuring consistent security posture.\n2. Compliance: Regulatory requirements (PCI-DSS, HIPAA, SOC2) often mandate standardized, auditable base configurations.\n3. Consistency: All teams use the same base, reducing 'works on my server' issues.\n4. Efficiency: Teams don't need to repeat base configuration — they build on top of the golden image.\n\nBest Practices:\n\n1. Security Hardening:\n- Remove unnecessary packages and services (reduce attack surface).\n- Apply all security patches and updates.\n- Configure secure defaults: disable root SSH login, enforce key-based authentication, configure firewall rules.\n- Implement CIS (Center for Internet Security) benchmarks.\n- Remove default credentials and sample applications.\n- Enable audit logging.\n- Configure automatic security updates (or plan for regular image rebuilds).\n- Run vulnerability scanners against the image before approval.\n\n2. Versioning:\n- Use semantic versioning for images (e.g., golden-ubuntu-22.04-v2.3.1).\n- Maintain a changelog documenting what changed in each version.\n- Keep previous versions available for rollback.\n- Tag images with version numbers, build dates, and source commit hashes.\n- Never modify an existing image version — create a new version instead (immutability).\n\n3. Tagging:\n- Apply consistent metadata tags to all images:\n  - Version: v2.3.1\n  - OS: ubuntu-22.04\n  - Build date: 2024-01-15\n  - Builder: CI pipeline / commit hash\n  - Status: approved / deprecated / testing\n  - Owner: platform-team\n  - Compliance: pci-dss, hipaa\n- Tags enable automated lifecycle management and auditing.\n\n4. Lifecycle Policies:\n- Define clear states: testing → approved → deprecated → deregistered.\n- Automatically deprecate images older than a defined period (e.g., 90 days).\n- Prevent launching instances from deprecated images.\n- Clean up old snapshots and deregistered images to control costs.\n- Maintain at least N previous versions for rollback capability.\n\n5. CI/CD Integration:\n- Automate image building in a CI/CD pipeline:\n  - Trigger: New security patches, scheduled (weekly/monthly), or on-demand.\n  - Build: Use Packer to create the image from a template.\n  - Test: Run automated tests (security scans, compliance checks, functional tests).\n  - Approve: Automated approval for passing images, manual approval for production.\n  - Publish: Register the image and update tags.\n  - Notify: Alert teams that a new golden image is available.\n- Store image templates in version control (Git) alongside application code.\n- Treat image building as a software development process.\n\nWhy a Golden Image Pipeline is Critical:\n\nSecurity:\n- Without it: Teams create images ad-hoc, with inconsistent security configurations. Some images have unpatched vulnerabilities, default credentials, or unnecessary services.\n- With it: Every image meets the security baseline. Vulnerabilities are patched systematically. Compliance is verifiable.\n\nOperational Efficiency:\n- Without it: Each team spends hours configuring base images. Inconsistencies cause debugging nightmares. No standardization.\n- With it: Teams start from a proven, tested base. Faster instance provisioning. Consistent behavior across all environments.\n\nIncident Response:\n- Without it: When a vulnerability is discovered, you don't know which servers are affected or how to patch them consistently.\n- With it: Rebuild the golden image with the patch, test it, and roll out new instances. Every server is patched identically.",
    "difficulty": "hard",
    "topic_area": "Custom Machine Images"
  },
  {
    "id": "q084",
    "concept_ids": ["c-image-003"],
    "scenario": "",
    "question_text": "Explain what HashiCorp Packer is and how it automates machine image creation. Describe Packer's architecture: builders, provisioners, and post-processors. How does Packer enable multi-cloud image building from a single template, and how does it integrate into a CI/CD pipeline for automated image management?",
    "model_answer": "What is Packer:\nPacker is an open-source tool by HashiCorp that automates the creation of machine images for multiple platforms from a single source configuration. Instead of manually creating images through cloud consoles, Packer codifies the image creation process, making it reproducible, version-controlled, and automatable.\n\nPacker Architecture — Three Core Components:\n\n1. Builders:\n- Purpose: Define WHERE and HOW to create the image — which cloud platform, what base image, what instance type to use during building.\n- Each builder targets a specific platform: amazon-ebs (AWS AMI), googlecompute (GCP Image), azure-arm (Azure Image), docker (Docker image), virtualbox-iso (VirtualBox).\n- The builder launches a temporary instance, runs provisioners on it, then creates an image from the result and terminates the temporary instance.\n- Multiple builders can be defined in one template — building images for AWS, GCP, and Azure simultaneously.\n\n2. Provisioners:\n- Purpose: Define WHAT to install and configure on the image — the actual content of the image.\n- Types: shell (run bash scripts), ansible (run Ansible playbooks), chef (run Chef recipes), file (upload files), powershell (Windows scripts).\n- Provisioners run in order on the temporary instance created by the builder.\n- Example sequence: upload configuration files → run shell script to install packages → run Ansible playbook for configuration → run security hardening script.\n\n3. Post-Processors:\n- Purpose: Define what to do AFTER the image is created — tagging, compression, uploading, notification.\n- Examples: tag the image with metadata, compress and upload to artifact storage, push Docker images to a registry, create a manifest file listing all built images.\n- Post-processors run after the builder creates the image.\n\nMulti-Cloud Image Building:\nPacker's key differentiator is building identical images across multiple clouds from one template:\n\nA single Packer template can define:\n- An AWS builder (creates an AMI)\n- A GCP builder (creates a GCP Image)\n- An Azure builder (creates an Azure Image)\n\nAll three builders use the SAME provisioners — the same shell scripts, Ansible playbooks, and configuration files. This ensures:\n- Consistency: The same software, configuration, and security hardening across all clouds.\n- Single source of truth: One template to maintain, not three separate processes.\n- Parallel building: Packer builds all images simultaneously, reducing total build time.\n- Version alignment: All cloud images are built from the same template version.\n\nCI/CD Integration:\nPacker integrates into CI/CD pipelines for automated image management:\n\nPipeline stages:\n1. Trigger: New commit to image template repository, scheduled build (weekly), or security patch notification.\n2. Validate: packer validate — check template syntax without building.\n3. Build: packer build — create images across all target platforms.\n4. Test: Run automated tests against instances launched from the new images (security scans, compliance checks, smoke tests).\n5. Approve: Automated approval for passing images; manual approval gate for production images.\n6. Publish: Tag images as 'approved,' update image references in Terraform configurations.\n7. Notify: Alert teams that new images are available.\n\nBenefits of CI/CD integration:\n- Automated security patching: When a CVE is published, update the template, and the pipeline automatically builds, tests, and publishes patched images.\n- Audit trail: Every image build is tracked in the CI/CD system with the exact template version, build logs, and test results.\n- Reproducibility: Any image can be rebuilt from its template at any time.\n- Speed: Automated pipeline builds images in minutes, not hours of manual work.\n\nPacker + Terraform workflow:\n1. Packer builds and publishes machine images.\n2. Terraform references the latest approved image ID.\n3. Terraform provisions infrastructure using the Packer-built images.\n4. Updates: New Packer image → Terraform apply → new instances with updated image.",
    "difficulty": "hard",
    "topic_area": "Building Custom Machine Images using Hashicorp Packer"
  },
  {
    "id": "q085",
    "concept_ids": ["c-cloudinit-001"],
    "scenario": "",
    "question_text": "Explain what cloud-init is, why it exists, and what gap it fills that machine images alone cannot address. Describe cloud-init's boot stages (Generator, Local, Network, Config, Final) and the types of initialization tasks it performs. How does cloud-init receive its configuration (user data), and what is the difference between cloud-init and configuration management tools like Ansible?",
    "model_answer": "What is cloud-init:\ncloud-init is the industry-standard tool for automating the initialization of cloud instances during their first boot. It is pre-installed on most cloud-provided Linux images (Ubuntu, Amazon Linux, CentOS, etc.) and runs automatically when an instance starts for the first time.\n\nWhy cloud-init Exists — The Gap Machine Images Cannot Fill:\nMachine images capture a static snapshot of a system. But many configuration elements are dynamic and instance-specific — they cannot be baked into an image:\n\n1. SSH keys: Each instance needs unique SSH authorized keys for the users who will access it.\n2. Hostname: Each instance needs a unique hostname.\n3. Network configuration: IP addresses, DNS settings vary per instance.\n4. Environment-specific configuration: Database hostnames, API endpoints, feature flags differ between dev/staging/production.\n5. Secrets and credentials: Should NEVER be baked into images (images may be shared, copied, or stored insecurely).\n6. Dynamic service discovery: Endpoints of other services that may change.\n7. Instance metadata: Instance ID, region, availability zone — only known at launch time.\n\ncloud-init bridges this gap: the machine image provides the static base (OS, packages, security hardening), and cloud-init applies the dynamic, instance-specific configuration at boot time.\n\ncloud-init Boot Stages:\ncloud-init runs in five stages during the boot process, each with a specific purpose:\n\n1. Generator Stage:\n- Determines if cloud-init should run at all.\n- Checks if the instance is in a cloud environment.\n- Runs before any other cloud-init stage.\n\n2. Local Stage (cloud-init local):\n- Runs very early in boot, before networking is configured.\n- Retrieves instance metadata from the cloud provider's metadata service.\n- Applies network configuration.\n- Purpose: Set up networking so subsequent stages can access network resources.\n\n3. Network Stage (cloud-init init):\n- Runs after networking is available.\n- Retrieves user data (cloud-config) from the metadata service.\n- Sets up disk devices, mount points.\n- Configures SSH keys and user accounts.\n\n4. Config Stage (cloud-init modules --mode=config):\n- Runs configuration modules: package installation, file writing, NTP configuration.\n- Processes the cloud-config YAML directives.\n\n5. Final Stage (cloud-init modules --mode=final):\n- Runs final configuration tasks: package installations, user scripts (runcmd), configuration management tool bootstrapping.\n- This is where most user-defined customization happens.\n\nInitialization Tasks cloud-init Performs:\n- User and group creation (with SSH key setup)\n- Package installation and updates\n- File creation and modification\n- Command execution (shell scripts)\n- Hostname configuration\n- Mount point setup\n- SSH configuration\n- Timezone and locale settings\n- Configuration management tool bootstrapping (Ansible, Puppet, Chef)\n\nHow cloud-init Receives Configuration:\ncloud-init receives its configuration through 'user data' — a script or cloud-config YAML document provided at instance launch time:\n- AWS: User Data field in EC2 launch configuration (up to 16KB).\n- GCP: Startup script metadata.\n- Azure: Custom data field.\n- The user data is stored in the cloud provider's metadata service and retrieved by cloud-init during boot.\n\ncloud-init vs Configuration Management Tools (Ansible, Puppet, Chef):\n\ncloud-init:\n- Runs ONCE at first boot (by default).\n- No agent required — built into the image.\n- Purpose: Initial instance setup and bootstrapping.\n- Scope: First-boot configuration only.\n- Complexity: Simple YAML configuration.\n\nConfiguration Management Tools:\n- Run continuously or on-demand (ongoing management).\n- May require an agent (Puppet, Chef) or agentless (Ansible via SSH).\n- Purpose: Ongoing configuration enforcement and drift correction.\n- Scope: Full lifecycle configuration management.\n- Complexity: More powerful but more complex.\n\nCommon pattern: cloud-init bootstraps the instance (installs the Ansible agent, sets up SSH keys, applies initial configuration), then Ansible takes over for ongoing configuration management.",
    "difficulty": "hard",
    "topic_area": "cloud-init"
  },
  {
    "id": "q086",
    "concept_ids": ["c-cloudinit-002"],
    "scenario": "",
    "question_text": "Explain cloud-init's cloud-config YAML format in detail. Describe the key modules (users, packages, write_files, runcmd, ssh_authorized_keys) with their syntax and purpose. What are the rules for writing valid cloud-config YAML, and what common mistakes should be avoided? Provide examples of how each module is used.",
    "model_answer": "cloud-config YAML Format:\ncloud-config is a YAML-based configuration format that cloud-init processes during boot. It provides a declarative way to configure instances without writing shell scripts.\n\nCritical rule: The file MUST start with #cloud-config on the first line. This tells cloud-init to interpret the file as cloud-config YAML rather than a shell script.\n\nKey Modules:\n\n1. users — Create User Accounts:\nPurpose: Create user accounts, set passwords, configure SSH keys, assign groups and sudo permissions.\n\nSyntax:\nusers:\n  - name: deploy\n    groups: [sudo, docker]\n    shell: /bin/bash\n    sudo: ['ALL=(ALL) NOPASSWD:ALL']\n    ssh_authorized_keys:\n      - ssh-rsa AAAA...key1\n      - ssh-rsa AAAA...key2\n  - name: appuser\n    system: true\n    shell: /usr/sbin/nologin\n\nNotes: The 'default' user (ec2-user, ubuntu) is created automatically. System users (system: true) are for running services, not human login.\n\n2. packages — Install Software Packages:\nPurpose: Install packages from the system package manager (apt, yum).\n\nSyntax:\npackage_update: true\npackage_upgrade: true\npackages:\n  - nginx\n  - docker.io\n  - python3-pip\n  - [git, 2.34.1]  # specific version\n\nNotes: package_update runs apt update / yum update before installing. package_upgrade upgrades all existing packages. Packages are installed in the Config stage.\n\n3. write_files — Create Configuration Files:\nPurpose: Create or modify files on the instance with specified content, permissions, and ownership.\n\nSyntax:\nwrite_files:\n  - path: /etc/nginx/conf.d/app.conf\n    content: |\n      server {\n        listen 80;\n        server_name example.com;\n        location / {\n          proxy_pass http://localhost:3000;\n        }\n      }\n    owner: root:root\n    permissions: '0644'\n  - path: /opt/app/config.env\n    content: |\n      DB_HOST=db.example.com\n      DB_PORT=5432\n    permissions: '0600'\n    owner: appuser:appuser\n\nNotes: Use the | (literal block scalar) for multi-line content. Permissions should be quoted strings ('0644'). Files are written in the Config stage (before runcmd).\n\n4. runcmd — Execute Commands:\nPurpose: Run arbitrary shell commands during the Final stage of boot.\n\nSyntax:\nruncmd:\n  - systemctl enable nginx\n  - systemctl start nginx\n  - ['sh', '-c', 'echo \"Boot complete\" >> /var/log/cloud-init-custom.log']\n  - curl -fsSL https://get.docker.com | sh\n  - usermod -aG docker deploy\n\nNotes: Commands run as root. Each list item is a separate command. Commands run in order. Use array syntax ['cmd', 'arg1'] for commands with special characters. runcmd runs ONCE at first boot, not on subsequent reboots.\n\n5. ssh_authorized_keys — Configure SSH Access:\nPurpose: Add SSH public keys to the default user's authorized_keys file.\n\nSyntax:\nssh_authorized_keys:\n  - ssh-rsa AAAA...key1 user@host\n  - ssh-ed25519 AAAA...key2 user@host\n\nNotes: These keys are added to the default user (ubuntu, ec2-user). For other users, use the ssh_authorized_keys field within the users module.\n\nCommon Mistakes to Avoid:\n\n1. Missing #cloud-config header: Without this first line, cloud-init treats the file as a shell script, not cloud-config YAML.\n\n2. YAML indentation errors: YAML is indentation-sensitive. Use spaces (not tabs). Inconsistent indentation causes parsing failures.\n\n3. Unquoted special characters: Values starting with special YAML characters ({, [, *, &) must be quoted.\n\n4. Permission format: Permissions must be quoted strings ('0644'), not integers (0644). YAML interprets unquoted 0644 as an octal number.\n\n5. Ordering assumptions: write_files runs before runcmd, but packages may not be installed yet when write_files runs. Plan accordingly.\n\n6. Baking secrets: Never put actual secrets (passwords, API keys) in cloud-config stored in version control. Use cloud provider secret management (AWS Secrets Manager, Parameter Store) and retrieve them in runcmd.\n\n7. Size limits: User data has size limits (16KB on AWS). For complex configurations, use cloud-init to download and execute larger scripts from S3 or other storage.\n\n8. Assuming runcmd runs on every boot: runcmd runs only on first boot by default. For commands that should run on every boot, use bootcmd instead.",
    "difficulty": "hard",
    "topic_area": "cloud-init"
  },
  {
    "id": "q087",
    "concept_ids": ["c-cloudinit-003"],
    "scenario": "",
    "question_text": "Explain the Golden Image + cloud-init pattern in depth. What is the decision framework for determining what should be 'baked' into the golden image versus what should be applied dynamically via cloud-init at boot time? Discuss the trade-offs between fully baked images, fully dynamic configuration, and the hybrid approach. Why should secrets never be baked into images?",
    "model_answer": "The Golden Image + cloud-init Pattern:\nThis pattern combines two complementary approaches to instance configuration:\n- Golden Image (bake time): Pre-configured machine image with static, shared configuration.\n- cloud-init (boot time): Dynamic, instance-specific configuration applied at first boot.\n\nDecision Framework — Bake vs Boot:\n\nBake into the Golden Image (static, shared across all instances):\n- Operating system and kernel configuration\n- Common packages and dependencies (nginx, Docker, monitoring agents)\n- Security hardening (CIS benchmarks, firewall rules, SSH configuration)\n- Base application binaries and runtime environments\n- Compliance configurations (audit logging, encryption settings)\n- Performance tuning (kernel parameters, sysctl settings)\n\nCriteria for baking: The configuration is the same across all instances of this type, changes infrequently, and takes significant time to install/configure.\n\nApply via cloud-init (dynamic, instance-specific):\n- SSH authorized keys (different per user/team)\n- Hostname and instance identity\n- Environment-specific configuration (database endpoints, API URLs, feature flags)\n- Secrets and credentials (API keys, database passwords, TLS certificates)\n- Service discovery endpoints (which other services to connect to)\n- Auto-scaling group membership and registration\n- Environment variables (dev vs staging vs production)\n\nCriteria for cloud-init: The configuration varies between instances or environments, changes frequently, or contains sensitive data.\n\nTrade-offs of Different Approaches:\n\n1. Fully Baked Images (everything in the image, no cloud-init):\n- Pros: Fastest boot time (nothing to install/configure at boot). Most predictable (what you tested is exactly what runs). No external dependencies at boot time.\n- Cons: Separate images needed for each environment (dev, staging, production). Secrets baked into images (security risk). Any configuration change requires rebuilding the entire image. Image proliferation (many variants).\n\n2. Fully Dynamic (minimal image, everything via cloud-init):\n- Pros: One image for all environments. Maximum flexibility. Easy to change configuration without rebuilding images.\n- Cons: Slowest boot time (installing packages, downloading software at boot). Boot depends on external services (package repositories, artifact stores). Less predictable (what if a package version changed since last boot?). More failure points during boot.\n\n3. Hybrid Approach (Golden Image + cloud-init) — RECOMMENDED:\n- Pros: Fast boot (most software pre-installed). Flexible (environment-specific config at boot). Secure (secrets never in images). One image per application type, configured per environment. Predictable base with dynamic customization.\n- Cons: Requires maintaining both image pipeline and cloud-init templates. Slightly more complex than either pure approach.\n\nWhy Secrets Must NEVER Be Baked into Images:\n\n1. Images are stored and shared: AMIs can be copied to other regions, shared with other accounts, or stored in registries. Anyone with access to the image has access to the secrets.\n\n2. Images are persistent: Even after deregistering an AMI, the underlying snapshots may persist. Secrets in images are difficult to fully purge.\n\n3. No rotation: Secrets baked into images cannot be rotated without rebuilding and redeploying all instances. This makes credential rotation impractical.\n\n4. Audit trail: It's difficult to track who accessed secrets embedded in images. Cloud secret management services (AWS Secrets Manager, Parameter Store) provide access logging.\n\n5. Blast radius: If an image is compromised, all secrets in it are exposed. With external secret management, compromising an instance only exposes the secrets that instance was authorized to access.\n\nBest practice for secrets:\n- Store secrets in a secret management service (AWS Secrets Manager, HashiCorp Vault, Parameter Store).\n- Use cloud-init to configure the instance to retrieve secrets at boot time.\n- Use IAM roles to authorize instances to access only the secrets they need.\n- Rotate secrets regularly without rebuilding images.",
    "difficulty": "hard",
    "topic_area": "cloud-init"
  },
  {
    "id": "q088",
    "concept_ids": ["c-systemd-001"],
    "scenario": "",
    "question_text": "Explain what systemd is and why it replaced older init systems (SysVinit, Upstart). Describe systemd's role as PID 1, its responsibilities in system boot and service management, and its key components (systemd, journald, logind, networkd). How does systemd's parallel startup improve boot performance compared to sequential init systems?",
    "model_answer": "What is systemd:\nsystemd is the init system and service manager for modern Linux distributions. It is the first process started by the kernel (PID 1) and is responsible for bootstrapping the entire user space — starting all other services and managing them throughout the system's lifetime.\n\nWhy systemd Replaced Older Init Systems:\n\nSysVinit (traditional):\n- Sequential startup: Services started one at a time, in a fixed order defined by numbered scripts (S01network, S02sshd, S03apache).\n- Shell script-based: Each service had a shell script in /etc/init.d/ with start/stop/restart functions.\n- Slow boot: Sequential execution meant boot time was the sum of all service startup times.\n- No dependency management: Order was determined by script numbering, not actual dependencies.\n- No automatic restart: If a service crashed, it stayed down until manually restarted.\n\nUpstart (transitional):\n- Event-based: Services started in response to events (network up, filesystem mounted).\n- Better parallelism than SysVinit.\n- But: Complex event system, limited adoption, eventually superseded by systemd.\n\nsystemd advantages:\n- Parallel startup: Services start simultaneously when their dependencies are met.\n- Dependency management: Explicit dependency declarations (After=, Requires=, Wants=).\n- Automatic restart: Services can be configured to restart automatically on failure.\n- Unified management: One tool (systemctl) for all service operations.\n- Logging integration: journald provides structured, indexed logging.\n- Socket activation: Services start on-demand when a connection arrives.\n- Cgroup integration: Resource limits (CPU, memory) per service.\n\nPID 1 — The First Process:\nThe kernel starts systemd as PID 1 — the ancestor of all other processes:\n- PID 1 is special: it cannot be killed (the system would crash). It adopts orphaned processes. It is responsible for system shutdown.\n- systemd as PID 1: Reads its configuration, builds a dependency tree of all services (units), and starts them in the correct order with maximum parallelism.\n\nResponsibilities:\n1. System boot: Initialize the system from kernel handoff to login prompt.\n2. Service lifecycle: Start, stop, restart, reload services. Monitor service health.\n3. Dependency ordering: Ensure services start after their dependencies are ready.\n4. Automatic recovery: Restart failed services based on configured policies.\n5. Logging: Collect and store logs from all services via journald.\n6. Resource management: Apply cgroup limits to services.\n7. System shutdown: Gracefully stop all services in reverse dependency order.\n\nKey Components:\n\n1. systemd (core): The init system and service manager. Manages units (services, sockets, timers, mounts, etc.).\n\n2. journald: The logging daemon. Collects logs from all services, the kernel, and boot messages. Stores logs in a structured, binary format (indexed for fast querying). Accessed via journalctl command. Replaces traditional syslog for many use cases.\n\n3. logind: Manages user logins and sessions. Tracks which users are logged in, manages seat/session concepts. Handles power management (suspend, hibernate, shutdown) permissions.\n\n4. networkd: Network configuration manager. Configures network interfaces, IP addresses, routes. Alternative to NetworkManager (more common on servers).\n\n5. resolved: DNS resolver daemon. Provides DNS resolution services. Manages /etc/resolv.conf.\n\n6. timesyncd: NTP client for time synchronization.\n\nParallel Startup Performance:\n\nSysVinit (sequential):\nService A (2s) → Service B (3s) → Service C (1s) → Service D (2s)\nTotal boot time: 2 + 3 + 1 + 2 = 8 seconds\n\nsystemd (parallel with dependencies):\nIf A and B have no dependencies on each other, and C depends on A, and D depends on B:\n- Second 0: Start A and B simultaneously\n- Second 2: A completes, start C\n- Second 3: B completes, start D, C completes\n- Second 5: D completes\nTotal boot time: 5 seconds (vs 8 sequential)\n\nWith socket activation, some services don't even need to start until they receive their first connection, further reducing effective boot time. This parallel approach is especially important for cloud instances where fast boot time directly impacts scaling responsiveness and deployment speed.",
    "difficulty": "hard",
    "topic_area": "systemd - System and Service Manager"
  },
  {
    "id": "q089",
    "concept_ids": ["c-systemd-002"],
    "scenario": "",
    "question_text": "Describe the structure of a systemd service unit file in detail, explaining each section ([Unit], [Service], [Install]) and their key directives. Then explain the essential systemctl and journalctl commands for managing services and viewing logs. How do you create a service that starts on boot, restarts on failure, and runs as a non-root user?",
    "model_answer": "systemd Service Unit File Structure:\n\nUnit files are stored in /etc/systemd/system/ (administrator-created) or /lib/systemd/system/ (package-installed). They use an INI-like format with sections denoted by [brackets].\n\n[Unit] Section — Metadata and Dependencies:\nDescribes the unit and defines its relationships with other units.\n\nKey directives:\n- Description=: Human-readable description of the service. Shown in systemctl status output.\n- Documentation=: URL to documentation.\n- After=: Start this unit AFTER the listed units. Defines ordering but not dependency. Example: After=network.target means 'start after networking is ready.'\n- Requires=: Hard dependency. If the required unit fails to start, this unit won't start either. If the required unit is stopped, this unit is also stopped.\n- Wants=: Soft dependency. Like Requires but this unit starts even if the wanted unit fails.\n- Before=: Start this unit BEFORE the listed units (reverse of After).\n- Conflicts=: Cannot run simultaneously with the listed units.\n\n[Service] Section — Service Behavior:\nDefines how the service runs.\n\nKey directives:\n- Type=: How systemd determines the service is 'started':\n  - simple (default): The process started by ExecStart IS the service. systemd considers it started immediately.\n  - forking: The process forks and the parent exits. systemd waits for the fork.\n  - oneshot: The process runs and exits. Used for initialization scripts.\n  - notify: The service sends a notification to systemd when it's ready.\n- ExecStart=: The command to start the service. Must be an absolute path.\n- ExecStop=: The command to stop the service (optional — default sends SIGTERM).\n- ExecReload=: The command to reload configuration without restarting.\n- User=: Run the service as this user (not root).\n- Group=: Run the service as this group.\n- WorkingDirectory=: Set the working directory for the service.\n- Environment=: Set environment variables (Environment=NODE_ENV=production).\n- EnvironmentFile=: Load environment variables from a file.\n- Restart=: When to restart the service:\n  - no: Never restart (default).\n  - always: Always restart, regardless of exit status.\n  - on-failure: Restart only if the process exits with a non-zero status or is killed by a signal.\n  - on-abnormal: Restart on signal, timeout, or watchdog.\n- RestartSec=: Time to wait before restarting (default 100ms). Example: RestartSec=10 waits 10 seconds.\n- StandardOutput=: Where to send stdout (journal, syslog, file).\n- StandardError=: Where to send stderr.\n\n[Install] Section — Boot Integration:\nDefines how the service is enabled (started at boot).\n\nKey directives:\n- WantedBy=: Which target should include this service. multi-user.target is the standard for services that should start at boot (equivalent to runlevel 3 — multi-user, no GUI).\n- RequiredBy=: Stronger version of WantedBy.\n- Also=: Additional units to enable/disable together with this one.\n\nComplete Example — Web Application Service:\n\n[Unit]\nDescription=Node.js Web Application\nDocumentation=https://docs.example.com\nAfter=network.target\nWants=postgresql.service\n\n[Service]\nType=simple\nUser=webapp\nGroup=webapp\nWorkingDirectory=/opt/webapp\nEnvironmentFile=/opt/webapp/.env\nExecStart=/usr/bin/node /opt/webapp/server.js\nExecReload=/bin/kill -HUP $MAINPID\nRestart=on-failure\nRestartSec=10\nStandardOutput=journal\nStandardError=journal\n\n[Install]\nWantedBy=multi-user.target\n\nThis service: runs as non-root user 'webapp', starts after networking, restarts on failure after 10 seconds, logs to journald, and starts on boot.\n\nEssential systemctl Commands:\n\n- systemctl daemon-reload: Reload unit file changes (MUST run after editing unit files).\n- systemctl start webapp: Start the service now.\n- systemctl stop webapp: Stop the service.\n- systemctl restart webapp: Stop then start.\n- systemctl reload webapp: Reload configuration without restarting (if supported).\n- systemctl enable webapp: Enable start on boot (creates symlink in multi-user.target.wants/).\n- systemctl disable webapp: Disable start on boot.\n- systemctl enable --now webapp: Enable AND start immediately.\n- systemctl status webapp: Show current status, PID, memory usage, recent logs.\n- systemctl is-active webapp: Check if running (returns 'active' or 'inactive').\n- systemctl is-enabled webapp: Check if enabled for boot.\n- systemctl list-units --type=service: List all loaded services.\n- systemctl list-unit-files --type=service: List all installed services and their enable status.\n\nEssential journalctl Commands:\n\n- journalctl -u webapp: View all logs for the webapp service.\n- journalctl -u webapp -f: Follow logs in real-time (like tail -f).\n- journalctl -u webapp --since '1 hour ago': Logs from the last hour.\n- journalctl -u webapp --since '2024-01-15' --until '2024-01-16': Date range.\n- journalctl -u webapp -p err: Only error-level messages.\n- journalctl -u webapp -n 50: Last 50 log lines.\n- journalctl -u webapp --no-pager: Output without paging (useful for scripts).\n- journalctl -b: All logs from current boot.\n- journalctl --disk-usage: Check journal storage usage.",
    "difficulty": "hard",
    "topic_area": "systemd - System and Service Manager"
  },
  {
    "id": "q090",
    "concept_ids": ["c-cloudproviders-001"],
    "scenario": "",
    "question_text": "Compare the three major public cloud providers — AWS, GCP, and Azure — across key dimensions: market position, core strengths, flagship services, target customers, and pricing models. What factors should an organization consider when choosing a cloud provider, and what are the arguments for and against a multi-cloud strategy?",
    "model_answer": "AWS (Amazon Web Services):\n\nMarket position: Largest cloud provider with approximately 31-33% market share. First mover (launched 2006). Most mature ecosystem.\n\nCore strengths:\n- Broadest service offering: 200+ services covering every conceivable cloud need.\n- Largest global infrastructure: Most regions and availability zones.\n- Deepest ecosystem: Largest partner network, most third-party integrations, most community resources.\n- Enterprise adoption: Dominant in enterprise cloud, extensive compliance certifications.\n\nFlagship services: EC2 (compute), S3 (storage), RDS (managed databases), Lambda (serverless), EKS (Kubernetes), CloudFormation (IaC).\n\nTarget customers: Enterprises of all sizes, startups (AWS Activate program), government (GovCloud).\n\nGCP (Google Cloud Platform):\n\nMarket position: Third largest provider with approximately 10-12% market share. Growing rapidly, especially in specific verticals.\n\nCore strengths:\n- Data analytics and ML/AI: Best-in-class data processing and machine learning services.\n- Kubernetes: Google created Kubernetes; GKE is considered the best managed Kubernetes service.\n- Network infrastructure: Google's global private network provides excellent performance.\n- Open source commitment: Strong support for open-source technologies.\n\nFlagship services: Compute Engine (compute), Cloud Storage, BigQuery (data warehouse), GKE (Kubernetes), Vertex AI (ML platform), Cloud Spanner (globally distributed database).\n\nTarget customers: Data-intensive organizations, AI/ML companies, Kubernetes-native teams, organizations valuing open-source.\n\nAzure (Microsoft Azure):\n\nMarket position: Second largest provider with approximately 22-25% market share. Fastest growing among the big three.\n\nCore strengths:\n- Enterprise integration: Seamless integration with Microsoft products (Active Directory, Office 365, Windows Server, SQL Server).\n- Hybrid cloud: Best hybrid cloud story with Azure Arc and Azure Stack.\n- Enterprise relationships: Existing Microsoft enterprise agreements extend to Azure.\n- Developer tools: Visual Studio, GitHub (Microsoft-owned), Azure DevOps.\n\nFlagship services: Virtual Machines (compute), Blob Storage, Azure SQL, Azure AD (identity), Azure DevOps, Azure Kubernetes Service.\n\nTarget customers: Microsoft-centric enterprises, organizations with existing Microsoft licensing, hybrid cloud needs.\n\nPricing Models (common across providers):\n- On-demand/pay-as-you-go: No commitment, highest per-unit cost.\n- Reserved/committed use: 1-3 year commitments for 30-72% discounts.\n- Spot/preemptible: Spare capacity at 60-90% discount, can be interrupted.\n- Free tiers: All three offer free tiers for learning and small workloads.\n- Sustained use discounts: GCP automatically discounts sustained usage (no commitment needed).\n\nFactors for Choosing a Provider:\n1. Existing technology stack: Microsoft shop → Azure. Data/ML focus → GCP. Broadest needs → AWS.\n2. Compliance requirements: Check which provider has the needed certifications in required regions.\n3. Team expertise: Choose the provider your team knows (or can learn most easily).\n4. Specific service needs: Some services are uniquely strong on specific providers (BigQuery on GCP, Active Directory on Azure).\n5. Pricing: Compare total cost for your specific workload pattern.\n6. Geographic presence: Ensure the provider has regions where you need them.\n7. Support and SLAs: Compare support tiers and service level agreements.\n\nMulti-Cloud Strategy:\n\nArguments for:\n- Avoid vendor lock-in: Not dependent on a single provider's pricing, availability, or decisions.\n- Best-of-breed: Use each provider's strongest services (GCP for ML, AWS for breadth).\n- Negotiating leverage: Multiple providers compete for your business.\n- Resilience: Provider-level outage doesn't take down everything.\n\nArguments against:\n- Complexity: Managing multiple providers requires expertise in each, different tools, different APIs.\n- Cost: Duplication of effort, training, and tooling. Data transfer between clouds is expensive.\n- Lowest common denominator: To be portable, you may avoid provider-specific features that provide the most value.\n- Operational overhead: More monitoring, more security configurations, more compliance audits.\n\nRecommendation: Most organizations benefit from a primary cloud provider with selective use of a second provider for specific needs, rather than a fully symmetric multi-cloud approach.",
    "difficulty": "hard",
    "topic_area": "Overview of Public Cloud Providers"
  },
  {
    "id": "q091",
    "concept_ids": ["c-terraform-network-001"],
    "scenario": "",
    "question_text": "Explain how Terraform is used to define and manage cloud networking infrastructure. Describe the key Terraform resource types for AWS networking (aws_vpc, aws_subnet, aws_route_table, aws_internet_gateway, aws_security_group) and how they relate to each other. How does defining networking as code differ from configuring it through the cloud console, and what are the benefits for team collaboration and disaster recovery?",
    "model_answer": "Terraform for Cloud Networking:\nTerraform enables defining the entire network infrastructure as code — VPCs, subnets, route tables, gateways, and security groups are all defined in .tf files, version-controlled, and managed through the standard Terraform workflow (init → plan → apply).\n\nKey AWS Networking Resource Types:\n\n1. aws_vpc — Virtual Private Cloud:\nThe foundation of cloud networking. Defines an isolated virtual network.\n\nresource \"aws_vpc\" \"main\" {\n  cidr_block           = \"10.0.0.0/16\"\n  enable_dns_support   = true\n  enable_dns_hostnames = true\n  tags = { Name = \"main-vpc\" }\n}\n\nKey attributes: cidr_block defines the IP address range. All subnets and resources within the VPC use addresses from this range.\n\n2. aws_subnet — Subnets:\nDivide the VPC into smaller network segments, each in a specific AZ.\n\nresource \"aws_subnet\" \"public_a\" {\n  vpc_id                  = aws_vpc.main.id\n  cidr_block              = \"10.0.1.0/24\"\n  availability_zone       = \"us-east-1a\"\n  map_public_ip_on_launch = true\n  tags = { Name = \"public-subnet-a\" }\n}\n\nresource \"aws_subnet\" \"private_a\" {\n  vpc_id            = aws_vpc.main.id\n  cidr_block        = \"10.0.10.0/24\"\n  availability_zone = \"us-east-1a\"\n  tags = { Name = \"private-subnet-a\" }\n}\n\nRelationship: Subnets belong to a VPC (vpc_id reference). Public subnets have map_public_ip_on_launch = true.\n\n3. aws_internet_gateway — Internet Access:\nEnables communication between the VPC and the internet.\n\nresource \"aws_internet_gateway\" \"main\" {\n  vpc_id = aws_vpc.main.id\n  tags = { Name = \"main-igw\" }\n}\n\nRelationship: Attached to a VPC. Route tables reference it to enable internet access.\n\n4. aws_route_table and aws_route — Traffic Routing:\nDefine how traffic flows within and outside the VPC.\n\nresource \"aws_route_table\" \"public\" {\n  vpc_id = aws_vpc.main.id\n  tags = { Name = \"public-rt\" }\n}\n\nresource \"aws_route\" \"public_internet\" {\n  route_table_id         = aws_route_table.public.id\n  destination_cidr_block = \"0.0.0.0/0\"\n  gateway_id             = aws_internet_gateway.main.id\n}\n\nresource \"aws_route_table_association\" \"public_a\" {\n  subnet_id      = aws_subnet.public_a.id\n  route_table_id = aws_route_table.public.id\n}\n\nRelationship: Route tables belong to a VPC. Routes within the table direct traffic to targets (IGW, NAT, peering). Route table associations link subnets to route tables.\n\n5. aws_security_group — Firewall Rules:\nDefine inbound and outbound traffic rules at the instance level.\n\nresource \"aws_security_group\" \"web\" {\n  name        = \"web-sg\"\n  description = \"Allow HTTPS inbound\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nRelationship: Security groups belong to a VPC. They are attached to instances, RDS databases, and other resources.\n\nResource Dependency Chain:\nVPC → Subnets → Route Tables → Routes → Internet Gateway\nVPC → Security Groups\nSubnets + Route Table Associations\n\nTerraform automatically understands these dependencies through resource references (aws_vpc.main.id) and creates resources in the correct order.\n\nCode vs Console — Key Differences:\n\nConsole (manual):\n- Click through multiple screens to create each resource.\n- Easy to forget a step (missing route table association, wrong CIDR block).\n- No record of what was configured or why.\n- Impossible to reproduce exactly for another environment.\n- Changes are immediate and unreviewed.\n\nTerraform (code):\n- All resources defined in files that can be read, reviewed, and understood.\n- terraform plan shows exactly what will change before any changes are made.\n- Version controlled in Git — complete history of every change.\n- Reproducible — same code creates identical networks in any environment.\n- Peer review through pull requests before applying changes.\n\nBenefits for Team Collaboration:\n1. Shared understanding: The code IS the documentation. Any team member can read the .tf files to understand the network architecture.\n2. Code review: Network changes go through pull requests. Security team can review security group changes before they're applied.\n3. Consistency: Dev, staging, and production networks are defined by the same code with different variables.\n4. Onboarding: New team members can understand the network by reading the code, not by clicking through the console.\n\nBenefits for Disaster Recovery:\n1. Rapid reconstruction: If a region fails, the entire network can be recreated in another region by running terraform apply with a different region variable.\n2. Known state: The code defines exactly what the network should look like. No guessing about configurations.\n3. Tested recovery: DR procedures can be tested by creating the network in a test region and verifying it works.\n4. No tribal knowledge: The network configuration is in code, not in someone's head. Recovery doesn't depend on specific people being available.\n\nGCP and Azure equivalents:\n- GCP: google_compute_network (VPC), google_compute_subnetwork, google_compute_firewall, google_compute_router.\n- Azure: azurerm_virtual_network, azurerm_subnet, azurerm_network_security_group, azurerm_route_table.\n\nThe same Terraform workflow applies regardless of cloud provider — the resource names change but the approach is identical.",
    "difficulty": "hard",
    "topic_area": "AWS, GCP & Azure Networking with Terraform"
  }
]